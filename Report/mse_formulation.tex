In the second method, we solve the \ac{JSFRA} problem by exploiting the relation between the \ac{MSE} and the achievable \ac{SINR} when the \ac{MMSE} receivers are used at the user terminals \cite{mse_duality,christensen2008weighted}. The \ac{MSE} \me{\epsilon_{l,k,n}}, for a data symbol \me{d_{l,k,n}} is given by
\iftoggle{single_column}{
\begin{equation} \label{mse-error}
	\mathbb{E} \big [ ( d_{l,k,n} - \hat{d}_{l,k,n} )^2 \big ] = \left | 1 - \mvec{w}{l,k,n}^\herm \mvec{H}{b_k,k,n} \mvec{m}{l,k,n} \right |^2 + \sum_{\mathclap{(j,i) \neq (l,k)}} \left | \mvec{w}{l,k,n}^\herm \mvec{H}{b_i,k,n} \mvec{m}{j,i,n} \right |^2 + \enoise = \epsilon_{l,k,n}
\end{equation}}{\allowdisplaybreaks
\begin{multline} \label{mse-error}
 \mathbb{E} \big [ ( d_{l,k,n} - \hat{d}_{l,k,n} )^2 \big ] = \left | 1 - \mvec{w}{l,k,n}^\herm \mvec{H}{b_k,k,n} \mvec{m}{l,k,n} \right |^2 \\
 + \sum_{\mathclap{(j,i) \neq (l,k)}} \left | \mvec{w}{l,k,n}^\herm \mvec{H}{b_i,k,n} \mvec{m}{j,i,n} \right |^2 + \enoise = \epsilon_{l,k,n}
\end{multline}}
where \me{\hat{d}_{l,k,n}} is the estimate of the transmitted symbol. Plugging the \ac{MMSE} receivers in \eqref{eqn-10} into the \ac{MSE} expression in \eqref{mse-error} and into the \ac{SINR} expression in \eqref{eq:SINR}, we arrive at the following relation between the \ac{MSE} and the \ac{SINR} as
\begin{equation} \label{eqn-mse-eq}
\epsilon_{l,k,n} = (1 + \gamma_{l,k,n})^{-1}.
\end{equation}
The above equivalence is valid only if the receivers are based on the \ac{MMSE} criterion. Using the equivalence in \eqref{eqn-mse-eq}, the \ac{WSRM} objective can be reformulated as the \ac{WMMSE} to obtain the precoders for the \acs{MU}-\acs{MIMO} scenario as discussed in \cite{christensen2008weighted,wmmse_shi,hong2012decomposition}. Note that the receive beamformers based on the \ac{MMSE} criterion are independent of the choice of the \me{\ell_q} norm used in the objective function to obtain the optimal transmit precoders \me{\mvec{m}{l,k,n}}.

Before proceeding futher, let \me{{v}^{\prime}_k = Q_k - \sum_{n=1}^N \sum_{l=1}^L t_{l,k,n}} denotes the queue deviation corresponding to user \me{k} and \me{\tilde{{v}}^{\prime}_k \triangleq a_k^{1/{q}}v^{\prime}_k} be the corresponding weighted objective.By using the relaxed \ac{MSE} expression in \eqref{mse-error}, we can reformulate \eqref{eqn-3} as
\iftoggle{single_column}{
\begin{IEEEeqnarray}{rCl}\label{eqn-mse-1} \neqsub \allowdisplaybreaks
\underset{\substack{t_{l,k,n},\mvec{m}{l,k,n},\\ \epsilon_{l,k,n},\mvec{w}{l,k,n}}} {\text{minimize}} & \quad & \|  \tilde{\mbf{v}}^{\prime}  \|_q \IEEEyessubnumber \label{eqn-mse-1.1} \\
\text{subject to} & \quad & t_{l,k,n} \leq -\log_2(\epsilon_{l,k,n}) \IEEEyessubnumber \label{eqn-mse-1.2} \\
 & \quad & \epsilon_{l,k,n} \geq  \left | 1 - \mvec{w}{l,k,n}^\herm \mvec{H}{b_k,k,n} \mvec{m}{l,k,n} \right |^2 + \sum_{\mathclap{(j,i) \neq (l,k)}} \left | \mvec{w}{l,k,n}^\herm \mvec{H}{b_i,k,n} \mvec{m}{j,i,n} \right |^2 + \enoise \eqspace \IEEEyessubnumber \label{eqn-mse-1.3} \\
& \quad & \sum_{n = 1}^N \sum_{k \in \mathcal{U}_b} \sum_{l=1}^L \trace \, (\mvec{m}{l,k,n} \mvec{m}{l,k,n}^\herm) \leq P_{{\max}} \; \fall b. \IEEEyessubnumber  \eqspace \label{eqn-mse-1.4}
\end{IEEEeqnarray}
}{\allowdisplaybreaks
\begin{IEEEeqnarray}{CL}\label{eqn-mse-1} \neqsub \allowdisplaybreaks
	\underset{\substack{t_{l,k,n},\mvec{m}{l,k,n},\\ \epsilon_{l,k,n},\mvec{w}{l,k,n}}} {\text{minimize}} & \quad \|  \tilde{\mbf{v}}^{\prime}  \|_q \IEEEyessubnumber \label{eqn-mse-1.1} \\
	\text{subject to} & t_{l,k,n} \leq -\log_2(\epsilon_{l,k,n}) \IEEEyessubnumber \label{eqn-mse-1.2} \\
	& \sum_{{(j,i) \neq (l,k)}} \left | \mvec{w}{l,k,n}^\herm \mvec{H}{b_i,k,n} \mvec{m}{j,i,n} \right |^2 + \enoise \nonumber \\	
	& \qquad + \left | 1 - \mvec{w}{l,k,n}^\herm \mvec{H}{b_k,k,n} \mvec{m}{l,k,n} \right |^2 \leq \epsilon_{l,k,n}\IEEEyessubnumber \label{eqn-mse-1.3} \\
	& \quad \sum_{n = 1}^N \sum_{k \in \mathcal{U}_b} \sum_{l=1}^L \trace \, (\mvec{m}{l,k,n} \mvec{m}{l,k,n}^\herm) \leq P_{{\max}} \; \fall b. \IEEEyessubnumber  \eqspace \label{eqn-mse-1.4}
\end{IEEEeqnarray}}

The alternative \ac{MSE} formulation given by \eqref{eqn-mse-1} is non-convex even for the fixed \me{\mvec{w}{l,k,n}} due to the constraint \eqref{eqn-mse-1.2}. Again we resort to the \ac{SCA} approach \cite{marks1978technical} by relaxing the constraint by a sequence of convex subsets using the first order Taylor approximation around a fixed \ac{MSE} point \me{\tilde{\epsilon}_{l,k,n}} as
\begin{IEEEeqnarray}{rCl}
- \log_2(\tilde{\epsilon}_{l,k,n}) - \frac{\left ( {\epsilon}_{l,k,n} - \tilde{\epsilon}_{l,k,n} \right ) }{\log(2) \, \tilde{\epsilon}_{l,k,n}} &\geq& t_{l,k,n}
\label{mse-lin}
\end{IEEEeqnarray}

Using the above approximation for the rate constraint, the problem defined in \eqref{eqn-mse-1} is solved for optimal transmit precoders \me{\mvec{m}{l,k,n}}, \acp{MSE} \me{\epsilon_{l,k,n}}, and the user rates over each sub-channel \me{t_{l,k,n}} for fixed receivers. The optimization subproblem to find transmit precoders for fixed \me{\mvec{w}{l,k,n}} is given by
\begin{IEEEeqnarray}{CCl}\label{eqn-mse-2} \neqsub
\underset{\substack{t_{l,k,n},\mvec{m}{l,k,n},\epsilon_{l,k,n}}}{\text{minimize}} &\quad& \|  \tilde{\mbf{v}}^{\prime}  \|_q \IEEEyessubnumber \label{eqn-mse-2.1} \\
\text{subject to} & \quad & \eqref{eqn-mse-1.3},\eqref{eqn-mse-1.4},\,\text{and} \:\eqref{mse-lin}. \IEEEyessubnumber \label{eqn-mse-2.2}
\end{IEEEeqnarray}
The optimal transmit precoders for fixed receivers are obtained by solving the subproblem \eqref{eqn-mse-2} iteratively and by updating the fixed \ac{MSE} point \me{\tilde{\epsilon}_{l,k,n}} with \me{{\epsilon}_{l,k,n}} from the previous iteration until termination as discussed in Section \ref{sec-3.2.1}. 

In all simulations, objective sequence generated by Algorithm \ref{algo-1} converges. \reviewF{However, for formal convergence analysis, following modifications are assumed. (i) Instead of \eqref{eqn-6.1} and \eqref{eqn-mse-2.1}, we consider a regularized objective in \eqref{mod_obj}, which exhibits strong convexity. (ii) Moreover, for some large \eqn{B_{\max} \gg P_{\max}}, we limit \eqn{\beta_{l,k,n}} as \eqn{\beta_{l,k,n} \leq B_{\max}} without altering the solution, to ensure the feasible sets of \eqref{eqn-9} and \eqref{eqn-9--1} bounded. The additional constraint on \eqn{\beta_{l,k,n}} is necessary, since \eqref{eqn-6.3} need not be tight when \eqn{\gamma_{l,k,n} \to 0}. Due to this, \eqn{\beta_{l,k,n}} becomes unbounded, since any value of \eqn{\beta_{l,k,n}} satisfying \eqref{eqn-6.3} with inequality, satisfies \eqref{eqn-6.3} with equality. Finally, with these changes, convergence of Algorithm \ref{algo-1} is dealt in Appendix \ref{sec-3.5}.}

\begin{comment}
\subsubsection*{Convergence}
Following similar approach as in Section \ref{sec-3.2.1}, at each iteration, the \ac{SCA} subproblems converge to a stationary point of the original nonconvex problem. The uniqueness of the precoders are justified if there is no phase ambiguity in the stationary solution. By reorganizing \eqref{eqn-mse-1.3} as
\iftoggle{single_column}{
\begin{equation}
\epsilon_{l,k,n} \geq  1 - 2 \Re{ \left \lbrace \mvec{w}{l,k,n}^\herm \mvec{H}{b_k,k,n} \mvec{m}{l,k,n} \right \rbrace} + \sum_{\mathclap{\forall (j,i)}} \left | \mvec{w}{l,k,n}^\herm \mvec{H}{b_i,k,n} \mvec{m}{j,i,n} \right |^2 + N_0 \, \|\mvec{w}{l,k,n}\|^2,
\end{equation}
}{
\begin{multline}
	\epsilon_{l,k,n} \geq  1 - 2 \Re{ \left \lbrace \mvec{w}{l,k,n}^\herm \mvec{H}{b_k,k,n} \mvec{m}{l,k,n} \right \rbrace} \\ 
	+ \sum_{\mathclap{\forall (j,i)}} \left | \mvec{w}{l,k,n}^\herm \mvec{H}{b_i,k,n} \mvec{m}{j,i,n} \right |^2 + N_0 \, \|\mvec{w}{l,k,n}\|^2,
\end{multline}
}
we can see that the ambiguity in the phase rotations for the transmit and the receive beamformers are eliminated by the presence of real component in the \ac{MSE} expression. 

At each \ac{SCA} update, the transmit precoders are obtained uniquely by minimizing \eqref{eqn-mse-1} due to the convex nature of the relaxed problem. For a fixed transmit precoders, the \ac{MMSE} receiver improves the objective value \cite{christensen2008weighted,wmmse_shi}, leading to the monotonic convergence of the objective function. At each \ac{SCA} step, the optimal value of the previous iteration is also included in the domain of the problem, and the objective value can either decrease or stays the same after each iteration. Note that the objective function improves at each iteration, whereas the sum rate need not follow the same behavior.

\end{comment}