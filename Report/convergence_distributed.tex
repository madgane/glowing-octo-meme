The convergence of the distributed algorithm in Algorithm \ref{algo-3} follows the same discussion as the one in Appendix \ref{sec-3.5}, if the subproblem in \eqref{eqn-decent-1} converges to the centralized solution. Note that \eqref{eqn-decent-1} satisfies the Slater's constraint qualification by having a non-empty interior and a compact set, which are required for the convergence. Since in primal decomposition, the master subproblem uses subgradient to update the coupling interference vectors in consensus with the objective function, the convergence of the subproblem can be guaranteed as the iteration index \me{i \rightarrow \infty} for a diminishing step size \cite{bertsekas1999nonlinear}. 

\review{To show the convergence of \ac{ADMM} approach, we use the discussions in \cite[Prop. 4.2]{bertsekas1989parallel} by writing the problem as
\begin{IEEEeqnarray}{rCl} \neqsub
	\underset{\mbf{x} \in \mc{C}_1,\mbf{z} \in \mc{C}_2}{\text{minimize}} &\quad& G(\mbf{x}) + H(\mbf{z}) \eqsub \\
	\text{subject to} &\quad& \mbf{A} \mbf{x} = \mbf{z} \eqsub \label{const-admm}
\end{IEEEeqnarray}
the following conditions are required for the convergence.
\begin{itemize} \allowdisplaybreaks
	\item \me{G,H} should be convex
	\item \me{\mc{C}_1,\mc{C}_2} should be a convex set and bounded
	\item \me{\mbf{A}^\herm \mbf{A}} should be invertible.
\end{itemize}
Note that the equality constraint in \eqref{const-admm} is identical to that in \eqref{const-admm1} used in the \ac{ADMM} subproblem \eqref{eqn-dual-3}. It is evident from the equality constraint \eqref{const-admm1} that \me{\mbf{A} = \mbf{I}}, which is an identity matrix and is invertible. The objective function \me{G,H} includes \me{\ell_q} norm with an additional quadratic term as in \eqref{mod_obj}, and therefore exhibits strong convexity. The set defined by the constraints of the problem \eqref{eqn-decent-1} is convex and has a nonempty interior. The feasibility of the interior point can be verified by having a non-zero precoder for only one user. Therefore, by following \cite[Prop. 4.2]{bertsekas1989parallel}, it can be seen that the \ac{ADMM} approach converges to the centralized solution as \me{i \rightarrow \infty}. The distributed algorithms also converges to a stationary point of the original nonconvex problem by following Appendix \ref{sec-3.5}, if the \ac{ADMM} is iterated until convergence in each \ac{SCA} step.}

\review{However, if the distributed algorithms are terminated well before the convergence or performed for limited number of iterations, convergence of the algorithm is not guaranteed. Since each update of \eqn{\mbf{x}^{(i)}} involves two \ac{SCA} updates until convergence with limited distributed iterations, the monotonicity of the objective and hence the convergence of the iterates is not guaranteed, unless the number of iterations is large enough to ensure strictly decreasing objective. It follows from the fact that in each primal or \ac{ADMM} update, the global objective is not guaranteed to be monotonically decreasing, despite reaching the centralized solution upon convergence.}
		
	%Since each update of \eqn{\mbf{x}^{(i)}} involves two \ac{SCA} updates until convergence with limited distributed iterations, the monotonicity and the convergence of the objective is not guaranteed unless the number of iterations is large enough to have strictly decreasing objective. Moreover, monotonicity of the global objective in each primal or \ac{ADMM} update is not ensured, despite reaching the centralized solution upon convergence.}

Unlike the primal or the \ac{ADMM} methods, the decomposition approach via \ac{KKT} conditions, presented in Section \ref{sec-4.3}, updates all the optimization variables at once, \textit{i.e}, the \ac{SCA} update of \me{\epsilon^{(i-1)}}, the \ac{AO} update of \me{\mvec{w}{l,k,n}} and the dual variable update of \me{\alpha} using subgradient method. Therefore, it is difficult to theoretically prove the convergence of the algorithm to a stationary point of the nonconvex problem in \eqref{eqn-6}.

The algorithm in \eqref{kkt-mse-4} is identical to \eqref{eqn-mse-2}, if the receivers \me{\mvec{w}{l,k,n}} and the \ac{MSE} operating point \me{\epsilon_{l,k,n}^{(i-1)}} are fixed to find the optimal transmit precoders \me{\mvec{m}{l,k,n}} and the dual variable \me{\alpha_{l,k,n}}. Note that it requires four nested loops to obtain the centralized solution, namely, the receive beamformer loop, \ac{MSE} operating point loop, the dual variable update loop and the bisection method to find the transmit precoders. 

However, to avoid the nested iterations, the proposed method performs group update of all the variables at once to obtain the transmit and the receive beamformers with the limited number of iterations, thus it achieves improved speed of convergence. Since the optimization variables are updated together, it is theoretically difficult to prove the monotonicity of the objective in each block update. Therefore, it is difficult to prove the convergence of the Algorithm \ref{algo-4} to a stationary point of the original nonconvex problem in \eqref{eqn-mse-1}. Even though the convergence is not theoretically guaranteed, it converges in all numerical experiments considered in Section \ref{sec-5.2}.

\begin{comment}
%Even though the optimization variables are updated together, the monotonicity of the objective still follows from the arguments presented in Appendix \ref{mcity}. Using the discussions in \cite{zangwill1969nonlinear,meyer1976sufficient}, the convergence to a stationary point of the nonconvex problem in \eqref{eqn-6} can be guaranteed for the proposed distributed algorithm with the \me{\ell_2} norm. It is also verified in all the numerical experiments considered in Section \ref{sec-5.2}. 

The convergence of the distributed algorithm in Algorithm \ref{algo-3} follows the same discussion as the one in Appendix \ref{sec-3.5} if the subproblem in \eqref{eqn-decent-1} converges to the centralized solution. Since the subproblem in \eqref{eqn-decent-1} is convex, each \ac{BS} specific slave subproblem is also convex for a fixed interference vector \me{\mbfa{\zeta}_{b_k}^{(i)}} \cite{palomar2006tutorial}. The master subproblem in the \acl{PD} uses subgradient to update the coupling interference vectors in consensus with the objective function, it is guaranteed to converge to the centralized solution as the iteration \me{i \rightarrow \infty} \cite{bertsekas1999nonlinear} for a diminishing step size. It can be seen that the subproblem \eqref{eqn-decent-1} satisfies the Slater's constraint qualification by having a non-empty interior and the set is compact.

To prove the convergence of the \ac{ADMM} approach, we use the discussions in \cite[Prop. 4.2]{bertsekas1989parallel} by writing the problem as
\begin{subeqnarray}
	\underset{\mbf{x},\mbf{z}}{\text{minimize}} &\quad& G(\mbf{x}) + H(\mbf{y}) \eqsub \\
	\text{subject to} &\quad& \mbf{A} \mbf{x} = \mbf{z} \eqsub \slabel{const-admm} \\
&\quad&	\mbf{x} \in \mc{C}_1, \mbf{z} \in \mc{C}_2 \eqsub
\end{subeqnarray}
the following conditions are required for the convergence.
\begin{itemize} \allowdisplaybreaks
	\item \me{G,H} should be convex
	\item \me{\mc{C}_1,\mc{C}_2} should be a convex set and bounded
	\item \me{\mbf{A}^\herm \mbf{A}} should be invertible.
\end{itemize}
Note that the equality constraint in \eqref{const-admm} is identical to that in \eqref{const-admm1} used in the \ac{ADMM} subproblem \eqref{eqn-dual-3}. It is evident from the equality constraint \eqref{const-admm1} that \me{\mbf{A} = \mbf{I}}, which is an identity matrix and is invertible. The objective function \me{G,H} are \me{\ell_q} norm in \eqref{eqn-decent-1} are convex and the set defined by the constraints of the problem \eqref{eqn-decent-1} is convex and has a nonempty interior. The feasibility of the interior point is verified by having a non-zero precoder for only one user. Therefore, by following \cite[Prop. 4.2]{bertsekas1989parallel}, it can be seen that the \ac{ADMM} approach converges to the centralized solution as \me{i \rightarrow \infty}. The distributed algorithms also converges to a stationary point of the original nonconvex problem as that of the centralized algorithms by following the discussions on Appendix \ref{sec-3.5}, if it is iterated until convergence.

\subsection{KKT based Decomposition} \label{sec-dist-conv-kkt}
Unlike \ac{PD} or \ac{ADMM} schemes, the decomposition via \ac{KKT} conditions for the \ac{MSE} reformulation discussed in Section \ref{sec-4.3}, all variables are updated at once, \textit{i.e}, the \ac{SCA} update of \me{\epsilon^{(i-1)}}, the \ac{AO} update of \me{\mvec{w}{l,k,n}} and the dual variable update of \me{\alpha} using subgradient. It is not guaranteed to obtain the same point as that of the centralized problem. 

The algorithm in \eqref{kkt-mse-4} is identical to \eqref{eqn-mse-2}, if the receivers \me{\mvec{w}{l,k,n}} and the \ac{MSE} operating point \me{\epsilon_{l,k,n}^{(i-1)}} are fixed to find the optimal transmit precoders \me{\mvec{m}{l,k,n}} and the dual variable \me{\alpha_{l,k,n}}. Note that it requires four nested loops to obtain the centralized solution, namely, the receive beamformer loop, \ac{MSE} operating point loop, the dual variable update loop and the bisection method to find the transmit precoders. 

To avoid this, the proposed method performs group update of all variables at once to obtain the transmit and the receive beamformers with the limited number of iterations, thus it achieves improved speed of convergence. Even though the convergence is not theoretically guaranteed, it converges in all numerical experiments considered in Section \ref{sec-5.2}. If the step size \me{\rho < 1}, the algorithm for \me{\ell_2} norm will converge by using the arguments on controlling overallocation and the nonincreasing objective function, since the earlier iterates are the operating point for the current iteration.

\end{comment}