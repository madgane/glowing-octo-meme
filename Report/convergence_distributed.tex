The convergence of the distributed algorithm in Algorithm \ref{algo-3} follows the same discussion as the one in Appendix \ref{sec-3.5}, if the subproblem in \eqref{eqn-decent-1} converges to the centralized solution. Note that \eqref{eqn-decent-1} satisfies the Slater's constraint qualification by having a non-empty interior and a compact set, which are required for the convergence. Since in primal decomposition, the master subproblem uses subgradient to update the coupling interference vectors in consensus with the objective function, the convergence of the subproblem is guaranteed as \me{i \rightarrow \infty} for a diminishing step size as shown in \cite{scutari_1},\cite[Prop 8.2.6]{bertsekas2003convex}. 

To show the convergence of the \ac{ADMM}, we use the discussion in \cite[Prop. 4.2]{bertsekas1989parallel} and \cite{boyd2011distributed} by writing \eqref{eqn-dual-3} as
\begin{IEEEeqnarray}{rCl} \neqsub
	\underset{\mbf{x} \in \mc{C}_1,\mbf{z} \in \mc{C}_2}{\text{minimize}} &\quad& G(\mbf{x}) + H(\mbf{z}) \eqsub \\
	\text{subject to} &\quad& \mbf{A} \mbf{x} = \mbf{z} \eqsub \label{const-admm}
\end{IEEEeqnarray}
where the constraint in \eqref{const-admm} is identical to that in \eqref{const-admm1} used in the \ac{ADMM} subproblem \eqref{eqn-dual-3}. 
To show the convergence, we rely on the following conditions. (i) \me{G,H} should be convex. (ii) \me{\mc{C}_1,\mc{C}_2} should be a convex set and bounded. (iii) \me{\mbf{A}^\herm \mbf{A}} should be invertible. It is evident from the equality constraints \eqref{const-admm} and \eqref{const-admm1} that \me{\mbf{A} = \mbf{I}}, and therefore it is invertible. Note that the objective functions \me{G,H} include \me{\ell_q} norm and an additional quadratic term as in \eqref{mod_obj}, thereby exhibiting strong convexity. Moreover, the feasible set defined by the constraints of \eqref{eqn-decent-1} is convex and has a nonempty interior. Now, by using \cite[Prop. 4.2]{bertsekas1989parallel}, we can show that the \ac{ADMM} converges to the centralized solution as \me{i \rightarrow \infty} by using diminishing step size in each \ac{ADMM} iteration as discussed in \cite{boyd2011distributed}. Therefore, if both the primal and \ac{ADMM} approaches are allowed to converge to the centralized solution in each \ac{SCA} step, \reviewF{then every limit point of the sequence generated by the distributed schemes is a stationary point of \eqref{con}, by following similar arguments as in Appendix \ref{sec-3.5}.}

%However, if the distributed algorithms are terminated well before the convergence or performed for few number of iterations, say, \eqn{J_{\max}}, then the convergence of the iterates \me{\{\ma^{(i)}\}} is not guaranteed. It follows from the fact that in each primal or \ac{ADMM} update, the global objective need not be decreasing monotonically, despite reaching the centralized solution upon convergence. However, if the distributed schemes for \eqref{con-m} are updated for sufficient number iterations to ensure strict monotonicity in each \ac{SCA} step, then the limit point of \me{\{\ma^{(i)}\}} is a stationary point of \eqref{con}, which can be different from the one obtained by solving \eqref{con-m} with \eqn{J_{\max} = \infty}.}
%\review{However, if the distributed algorithms are terminated well before the convergence or performed for limited number of iterations, say, \eqn{J_{\max}}, the convergence of the sequence \me{\{\ma^{(i)}\} \triangleq \{\iter{\mx}{\ast}{i},\iter{\my}{\ast}{i},\iter{\mz}{\ast|\mx}{i}\}} is not guaranteed. It follows from the fact that in each primal or \ac{ADMM} update, the global objective is not guaranteed to be monotonically decreasing, despite reaching the centralized solution upon convergence. If the distributed algorithm is iterated sufficient number of times to ensure strict monotonicity of the objective in each \ac{SCA} step with the modified function \eqref{mod_obj}, then the limit point of the convergent sequence is a stationary point of \eqref{con}, which can be different from the limit point obtained by solving the centralized solution.}
%If both primal and \ac{ADMM} approaches are allowed to converge in each \ac{SCA} step to the centralized solution, then the limit point of the sequence is a stationary point of \eqref{con}, using Appendix \ref{sec-3.5}. However, if the distributed algorithms are terminated well before the convergence or performed for few number of iterations, say, \eqn{J_{\max}}, then the convergence of the iterates \me{\{\ma^{(i)}\}} is not guaranteed. It follows from the fact that in each primal or \ac{ADMM} update, the global objective need not be decreasing monotonically, despite reaching the centralized solution upon convergence. However, if the distributed schemes for \eqref{con-m} are updated for sufficient number iterations to ensure strict monotonicity in each \ac{SCA} step, then the limit point of \me{\{\ma^{(i)}\}} is a stationary point of \eqref{con}, which can be different from the one obtained by solving \eqref{con-m} with \eqn{J_{\max} = \infty}.}
%Since each update of \eqn{\mbf{x}^{(i)}} involves two \ac{SCA} updates until convergence with limited distributed iterations, the monotonicity and the convergence of the objective is not guaranteed unless the number of iterations is large enough to have strictly decreasing objective. Moreover, monotonicity of the global objective in each primal or \ac{ADMM} update is not ensured, despite reaching the centralized solution upon convergence.}

Unlike the primal or the \ac{ADMM} methods, the decomposition approach via \ac{KKT} conditions, presented in Section \ref{sec-4.3}, updates all the optimization variables at once, \textit{i.e}, the \ac{SCA} update of \me{\epsilon^{(i-1)}}, the \ac{AO} update of \me{\mvec{w}{l,k,n}} and the dual variable update of \me{\alpha} using subgradient method. Therefore, it is difficult to theoretically prove the convergence of the algorithm to a stationary point of the nonconvex problem in \eqref{eqn-6}.

The algorithm in \eqref{kkt-mse-4} is identical to \eqref{eqn-mse-2}, if the receivers \me{\mvec{w}{l,k,n}} and the \ac{MSE} operating point \me{\epsilon_{l,k,n}^{(i-1)}} are fixed to find the optimal transmit precoders \me{\mvec{m}{l,k,n}} and the dual variable \me{\alpha_{l,k,n}}. Note that it requires four nested loops to obtain the centralized solution, namely, the receive beamformer loop, \ac{MSE} operating point loop, the dual variable update loop and the bisection method to find the transmit precoders. 

However, to avoid the nested iterations, the proposed method performs group update of all the variables at once to obtain the transmit and the receive beamformers with the limited number of iterations, thus, achieving improved speed of convergence. Since the optimization variables are updated together, it is theoretically difficult to prove the monotonicity of the objective in each block update. Therefore, the convergence of the sequence of iterates generated by the Algorithm \ref{algo-4} is not guaranteed. Moreover, the objective function used to obtain the iterative algorithm is not strongly convex, and therefore the uniqueness of the iterates is not guaranteed in each update. Thus, the iterative scheme outlined in Algorithm \ref{algo-4} cannot be guaranteed to find a fixed point to ensure the convergence of the beamformer iterates.

\begin{comment}
%Even though the optimization variables are updated together, the monotonicity of the objective still follows from the arguments presented in Appendix \ref{mcity}. Using the discussions in \cite{zangwill1969nonlinear,meyer1976sufficient}, the convergence to a stationary point of the nonconvex problem in \eqref{eqn-6} can be guaranteed for the proposed distributed algorithm with the \me{\ell_2} norm. It is also verified in all the numerical experiments considered in Section \ref{sec-5.2}. 

The convergence of the distributed algorithm in Algorithm \ref{algo-3} follows the same discussion as the one in Appendix \ref{sec-3.5} if the subproblem in \eqref{eqn-decent-1} converges to the centralized solution. Since the subproblem in \eqref{eqn-decent-1} is convex, each \ac{BS} specific slave subproblem is also convex for a fixed interference vector \me{\mbfa{\zeta}_{b_k}^{(i)}} \cite{palomar2006tutorial}. The master subproblem in the \acl{PD} uses subgradient to update the coupling interference vectors in consensus with the objective function, it is guaranteed to converge to the centralized solution as the iteration \me{i \rightarrow \infty} \cite{bertsekas1999nonlinear} for a diminishing step size. It can be seen that the subproblem \eqref{eqn-decent-1} satisfies the Slater's constraint qualification by having a non-empty interior and the set is compact.

To prove the convergence of the \ac{ADMM} approach, we use the discussions in \cite[Prop. 4.2]{bertsekas1989parallel} by writing the problem as
\begin{subeqnarray}
	\underset{\mbf{x},\mbf{z}}{\text{minimize}} &\quad& G(\mbf{x}) + H(\mbf{y}) \eqsub \\
	\text{subject to} &\quad& \mbf{A} \mbf{x} = \mbf{z} \eqsub \slabel{const-admm} \\
&\quad&	\mbf{x} \in \mc{C}_1, \mbf{z} \in \mc{C}_2 \eqsub
\end{subeqnarray}
the following conditions are required for the convergence.
\begin{itemize} \allowdisplaybreaks
	\item \me{G,H} should be convex
	\item \me{\mc{C}_1,\mc{C}_2} should be a convex set and bounded
	\item \me{\mbf{A}^\herm \mbf{A}} should be invertible.
\end{itemize}
Note that the equality constraint in \eqref{const-admm} is identical to that in \eqref{const-admm1} used in the \ac{ADMM} subproblem \eqref{eqn-dual-3}. It is evident from the equality constraint \eqref{const-admm1} that \me{\mbf{A} = \mbf{I}}, which is an identity matrix and is invertible. The objective function \me{G,H} are \me{\ell_q} norm in \eqref{eqn-decent-1} are convex and the set defined by the constraints of the problem \eqref{eqn-decent-1} is convex and has a nonempty interior. The feasibility of the interior point is verified by having a non-zero precoder for only one user. Therefore, by following \cite[Prop. 4.2]{bertsekas1989parallel}, it can be seen that the \ac{ADMM} approach converges to the centralized solution as \me{i \rightarrow \infty}. The distributed algorithms also converges to a stationary point of the original nonconvex problem as that of the centralized algorithms by following the discussions on Appendix \ref{sec-3.5}, if it is iterated until convergence.

\subsection{KKT based Decomposition} \label{sec-dist-conv-kkt}
Unlike \ac{PD} or \ac{ADMM} schemes, the decomposition via \ac{KKT} conditions for the \ac{MSE} reformulation discussed in Section \ref{sec-4.3}, all variables are updated at once, \textit{i.e}, the \ac{SCA} update of \me{\epsilon^{(i-1)}}, the \ac{AO} update of \me{\mvec{w}{l,k,n}} and the dual variable update of \me{\alpha} using subgradient. It is not guaranteed to obtain the same point as that of the centralized problem. 

The algorithm in \eqref{kkt-mse-4} is identical to \eqref{eqn-mse-2}, if the receivers \me{\mvec{w}{l,k,n}} and the \ac{MSE} operating point \me{\epsilon_{l,k,n}^{(i-1)}} are fixed to find the optimal transmit precoders \me{\mvec{m}{l,k,n}} and the dual variable \me{\alpha_{l,k,n}}. Note that it requires four nested loops to obtain the centralized solution, namely, the receive beamformer loop, \ac{MSE} operating point loop, the dual variable update loop and the bisection method to find the transmit precoders. 

To avoid this, the proposed method performs group update of all variables at once to obtain the transmit and the receive beamformers with the limited number of iterations, thus it achieves improved speed of convergence. Even though the convergence is not theoretically guaranteed, it converges in all numerical experiments considered in Section \ref{sec-5.2}. If the step size \me{\rho < 1}, the algorithm for \me{\ell_2} norm will converge by using the arguments on controlling overallocation and the nonincreasing objective function, since the earlier iterates are the operating point for the current iteration.

\end{comment}