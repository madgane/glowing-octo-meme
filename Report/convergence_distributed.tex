The convergence of the distributed algorithm outlined in Algorithm \ref{algo-3} follows the same discussion as the one in Appendix \ref{sec-3.5} if the subproblem in \eqref{eqn-decent-1} converges to the centralized solution. Since the subproblem in \eqref{eqn-decent-1} is convex, each \ac{BS} specific slave subproblem is also convex for a fixed interference vector \me{\mbfa{\zeta}_{b_k}^{(i)}} \cite{palomar2006tutorial}. The master subproblem in the \acl{PD} uses subgradient to update the coupling interference vectors in consensus with the objective function, it is guaranteed to converge to the centralized solution as the iteration \me{i \rightarrow \infty} \cite{bertsekas1999nonlinear} for a diminishing step size. It can be seen that the subproblem \eqref{eqn-decent-1} satisfies the Slater's constraint qualification by having a non-empty interior and bounded due to the total power constraint for the transmit precoders.

To prove the convergence of the \ac{ADMM} approach, we use the discussions in \cite[Prop. 4.2]{bertsekas1989parallel} by writing the problem as
\begin{subeqnarray}
	\underset{\mbf{x},\mbf{z}}{\text{minimize}} &\quad& G(\mbf{x}) + H(\mbf{y}) \eqsub \\
	\text{subject to} &\quad& \mbf{A} \mbf{x} = \mbf{z} \eqsub \slabel{const-admm} \\
&\quad&	\mbf{x} \in \mc{C}_1, \mbf{z} \in \mc{C}_2 \eqsub
\end{subeqnarray}
the following conditions are required for the convergence.
\begin{itemize}
	\item \me{G,H} should be convex
	\item \me{\mc{C}_1,\mc{C}_2} should be a convex set and bounded
	\item \me{\mbf{A}^\herm \mbf{A}} should be invertible.
\end{itemize}
Note that the equality constraint in \eqref{const-admm} is identical to that in \eqref{const-admm1} used in the \ac{ADMM} subproblem \eqref{eqn-dual-3}. It is evident from the equality constraint \eqref{const-admm1} that \me{\mbf{A} = \mbf{I}}, which is an identity matrix and is invertible. The objective function \me{G,H} are \me{\ell_q} norm in \eqref{eqn-decent-1} are convex and the set defined by the constraints of the problem \eqref{eqn-decent-1} is convex and has a nonempty interior. The feasibility of the interior point is verified by having a non-zero precoder for only one user. Therefore, by following \cite[Prop. 4.2]{bertsekas1989parallel}, it can be seen that the \ac{ADMM} approach converges to the centralized solution as \me{i \rightarrow \infty}. The distributed algorithms also converges to a stationary point of the original nonconvex problem as that of the centralized algorithms by following the discussions on Appendix \ref{sec-3.5}, if the primal or the \ac{ADMM} updates are iterated until convergence.

Unlike \ac{PD} or \ac{ADMM} schemes, the decomposition via \ac{KKT} conditions for the \ac{MSE} reformulation discussed in Section \ref{sec-4.3}, all variables are update at once, \textit{i.e}, the \ac{SCA} update of \me{\epsilon^{(i-1)}}, the \ac{AO} update of \me{\mvec{w}{l,k,n}} and the dual variable update of \me{\alpha} using subgradient. It is not guaranteed to obtain the same point as that of the centralized problem. 

The algorithm in \eqref{kkt-mse-4} is identical to \eqref{eqn-mse-2}, if the receivers \me{\mvec{w}{l,k,n}} and the \ac{MSE} operating point \me{\epsilon_{l,k,n}^{(i-1)}} are fixed to find the optimal transmit precoders \me{\mvec{m}{l,k,n}} and the dual variable \me{\alpha_{l,k,n}}. Note that it requires four nested loops to obtain the centralized solution, namely, the receive beamformer loop, \ac{MSE} operating point loop, the dual variable update loop and the bisection method to find the transmit precoders. To avoid this, the proposed method performs group update of all variables at once to obtain the transmit and the receive beamformers with the limited number of iterations, thus it achieves improved speed of convergence. Even though the convergence is not theoretically guaranteed, it converges in all numerical experiments considered in Section \ref{sec-5.2}. If the step size \me{\rho < 1}, the algorithm for \me{\ell_2} norm will converge by using the arguments on controlling overallocation and the nonincreasing objective function, since the earlier iterates are the operating point for the current iteration.




