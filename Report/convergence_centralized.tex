\newcommand{\eqn}[1]{\(#1\)}
\newcommand{\mx}{\mbf{x}}
\newcommand{\my}{\mbf{y}}
\newcommand{\mz}{\mbf{z}}
\newcommand{\mxb}{{\mbf{x}}}
\newcommand{\myb}{{\mbf{y}}}
\newcommand{\iterate}[2]{{#1}^{(#2)}}

Let us write the \ac{JSFRA} problem in \eqref{eqn-6} and \eqref{eqn-mse-1} as
\begin{IEEEeqnarray}{rCl} \label{con}
\underset{\mx,\my,\mz}{\text{minimize}} &\quad& f(\mz) \eqsub \label{con-obj} \\
\text{subject to} &\quad& h(\mz) - g_0(\mx,\my) \leq 0 \eqsub \label{con-dc} \\
&\quad& g_1(\mx,\my) \leq 0, \eqsub \label{con-cvx-blk} \\
&\quad& g_2(\mx) \leq 0, \eqsub \label{con-cvx}
\end{IEEEeqnarray}
wher \me{g_2,f} are convex functions and \me{h} is a linear function. Let \me{g_0,g_1} are convex functions only on \me{\mx} or \me{\my} as the variable but not on both. Note that the \eqref{con-dc} correspond to the constraints in \eqref{eqn-6.2} and \eqref{eqn-mse-1.2} and \eqref{con-cvx-blk} corresponds to the constraints in \eqref{eqn-6.3} and \eqref{eqn-mse-1.3}. Other convex constraints are addressed by the constraint \eqref{con-cvx}. With this, the feasible set of the problem \eqref{con} is given by 
\begin{IEEEeqnarray}{rl}
\mc{F} &= \{ \mx,\my,\mz | h(\mz) - g_0(\mx,\my) \leq 0, g_1(\mx,\my) \leq 0, g_2(\mx) \leq 0\} \nonumber
\end{IEEEeqnarray}

Before proceeding further, let us define the following notations. Let \me{\iterate{\mx}{k}} denote the \me{\ith{k}} \ac{BCDM} operating point and \me{\mx_k} represent the \me{\ith{k}} \ac{SCA} approximation point. Since the problem \eqref{con} is nonconvex, it is not guaranteed to find the optimal solution. In order to solve for a suboptimal solution, let us resort to the \ac{BCDM} by fixing a block of variables and optimize for others. Since \ac{BCDM} is an iterative method, let us initialize the variable \me{\my} as \me{\iterate{\myb}{0} \in \mc{F}}. Now the problem in \eqref{con} is still nonconvex due to the constraint \eqref{con-dc}, which is in \ac{DC} form. Following the approaches presented in \cite{lipp2014variations,lanckriet2009convergence}, the function \me{g_0(\mx,\iterate{\myb}{0})} is majorized around a feasible point \me{\mxb_0 \in \mc{F}} as
\begin{equation} \label{con-relax}
\hat{g}_o(\mx,\iterate{\myb}{0};\mxb_0) = {g}_0(\mxb_0,\iterate{\myb}{0}) + \nabla g_0(\mxb_0,\iterate{\myb}{0})^\mathrm{T} (\mx - \mx_0).
\end{equation}
Using the relaxation \eqref{con-relax}, the convex subproblem for the \me{\ith{i}} \ac{BCDM} iterate of \me{\myb} and the \me{\ith{k}} linear approximation of \me{\mxb} as
\begin{subeqnarray} \label{con-m}
	\underset{\mx,\my,\mz}{\text{minimize}} &\quad& f(\mz) \eqsub \label{con-obj-m} \\
	\text{subject to} &\quad& h(\mz) - \hat{g}_0(\mx,\iterate{\myb}{i};\mx_k) \leq 0 \eqsub \label{con-dc-m} \\
	&\quad& g_1(\mx,\iterate{\myb}{i}) \leq 0, \eqsub \label{con-cvx-blk-m} \\
	&\quad& g_2(\mx) \leq 0, \eqsub \label{con-cvx-m}
\end{subeqnarray}
Let the set defined by the problem in \eqref{con-relax} be represented as \me{\iterate{\mc{F}}{i}_k}. In order to prove the convergence of the convex subproblem \eqref{con-m} for a fixed \me{\iterate{\myb}{i}} and \me{\mx_k}, let us assume that \eqref{con-m} yields \me{\mx_{k+1}} and \me{\mz_{k+1}} as the solution at the \me{\ith{k}} iteration. To show that \me{\mx_{k+1}} and \me{\mz_{k+1}} minimizes the objective function ans also feasible, let us assume that the point \me{\mx_k} is feasible for \eqref{con-m}. Since the function \me{{g}_0(\mx,\iterate{\myb}{i})} is linearized around \me{\mx_k}, it satisfies
\begin{equation}
h(\mz) - {g}_0(\mx,\iterate{\myb}{i}) \leq h(\mz) - \hat{g}_0(\mx,\iterate{\myb}{i};\mxb_k) \leq 0,
\end{equation}
since \me{{g}_0(\mx,\iterate{\myb}{i}) \leq \hat{g}_0(\mx,\iterate{\myb}{i};\mxb_k), \forall \mx \in \iterate{\mc{F}}{i}_{k-1}}, where \me{\mc{F}_0 = \mc{F}}. Now, \me{\mx_{k+1}} is the solution and feasible for the \me{\ith{k}} subproblem, therefore, it satisfies
\begin{multline}
h(\mz_{k+1}) - {g}_0(\mx_{k+1},\iterate{\myb}{i}) \leq h(\mz_{k+1}) - \hat{g}_0(\mx_{k+1},\iterate{\myb}{i};\mxb_k) \\
\leq h(\mz_{k}) - \hat{g}_0(\mx_{k},\iterate{\myb}{i};\mxb_k) \leq 0. \label{con-sub-set}
\end{multline}
Using \eqref{con-sub-set}, we can prove that the solution \me{\mx_{k+1}} and \me{\mz_{k+1}} are feasible, since the initial point \me{\mx_{0}} was chosen to be feasible. In order to prove the convergence of the objective, using \eqref{con-sub-set}, we can see that \me{\iterate{\mc{F}}{i}_{0} \subseteq \dotsc \subseteq \iterate{\mc{F}}{i}_{k-1} \subseteq \iterate{\mc{F}_k}{i} \subseteq \mc{F}}. Since at \me{\ith{k}} iteration, the feasible set of the problem \eqref{con-m} includes the feasible sets of the earlier iteration, we can see that 
\begin{equation} \label{con-convergence}
f(\mz_0) \geq f(\mz_k) \geq f(\mz_{k+1}) \geq f(\mz_{\ast,y,i}). 
\end{equation}
Thus the sequence \me{f(\mz_k)} is nonincreasing and converges to a critical point. Let \me{\mx_{\ast}} and \me{\mz_{\ast,y,i}} be the optimal point of the problem \eqref{con-m} when the iterative algorithm is converged. The symbol \me{y,i} in \me{\mz_{\ast,y,i}} denotes the optimal \me{\mz} with fixed \me{\my} at the \eqn{\ith{i}} \ac{BCDM} iteration. It can be seen from \cite{marks1978technical} that the optimal point of the converged subproblems is in deed the stationary point of the original nonconvex problem \eqref{con}. It can be easily verified by the \ac{KKT} expressions and the feasibility of the constraints.

Once the optimal solution is found for the fixed \me{\my}, we fix the variable \me{\mx} as \me{\iterate{\mxb}{i} = \mx_{\ast}} and optimize \me{\my}. The problem is still nonconvex due to the \ac{DC} constraint. By following similar approach as we discussed for the variable \me{\mx}, we can obtain the optimal solution \me{\my_{k}} for the convex subproblem at the \me{\ith{k}} iteration. The convergence and the nonincreasing behavior of the problem follows similar explanations. Now, the optimal solution of the converged subproblem with \me{\my} as variable is \me{\my_{\ast}} and \me{\mz_{\ast,x,i}}. Following the arguments presented in \cite{marks1978technical}, the optimal point of the converged convex subproblems is a stationary point of the nonconvex problem in \eqref{con}.

Finally, to prove the global convergence of the objective, we need to show the nonincreasing behavior of the objective function between each \ac{BCDM} update, \textit{i.e}, \me{f_0(\mz_{\ast,y,i}) \geq f_0(\mz_{\ast,x,i}) \geq f_0(\mz_{\ast,y,i+1})}. Let us assume that for an initial feasible value of \eqn{\my} at \eqn{\iterate{\my}{0}}, we achieve a stationary point of \eqref{con} by solving sequence of convex subproblems. The solution is given by \eqn{\mz_{\ast,y,0}} and \eqn{\mx_{\ast}}. Now, by fixing \eqn{\iterate{\mx}{0} = \mx_{\ast}}, we find the optimal solution \eqn{\mz_{\ast,x,0}} and \eqn{\my_{\ast}} by solving the convex subproblems. It can be seen that 
\begin{equation}
f_0(\mz_{k,y,0}) \geq f_0(\mz_{\ast,y,0}) \geq f_0(\mz_{0,x,0}) \geq f_0(\mz_{\ast,x,0}),
\end{equation}
which follows from the convexity of the objective function \eqn{f_0} and the initial \eqn{\iterate{\my}{0}} is also a feasible point for the variable \eqn{\my} of the \eqn{\ith{k}} subproblem. Using the property of induction, we can show that the global problem converges to a stationary point of the nonconvex problem \eqref{con} in a nonincreasing manner.

Note that the uniqueness of the convex subproblems \eqref{con-m} is required for the convergence of the proof. We can prove the uniqueness of the convex subproblems defined by \eqref{eqn-9} and \eqref{eqn-mse-2} using the following constraints. For the problem \eqref{eqn-9}, the uniqueness of the transmit precoders is guaranteed by the constraint \eqref{eqn-8}, which can be written as
\begin{equation}
\gamma_{l,k,n} - \mvec{\tilde{m}}{l,k,n}^\herm \mvec{\tilde{H}}{b_k,k,n}^\herm \mvec{\tilde{H}}{b_k,k,n} (\mvec{m}{l,k,n} - \mvec{\tilde{m}}{l,k,n}) \leq 0,
\end{equation}
where \eqn{\mvec{\tilde{H}}{b_k,k,n} = \mvec{w}{l,k,n}^\herm \mvec{\tilde{H}}{b_k,k,n}} and for the convex subproblem \eqref{eqn-mse-2}, uniqueness of the transmit precoder is guaranteed by the constraint 
\begin{multline}
| 1 - \mvec{w}{l,k,n}^\herm \mvec{H}{b_k,k,n} \mvec{m}{l,k,n} |^2 + \sum_{\mathclap{(j,i) \neq (l,k)}} | \mvec{w}{l,k,n}^\herm \mvec{H}{b_i,k,n} \mvec{m}{j,i,n} |^2 \\ + \enoise \leq \epsilon_{l,k,n}.
\end{multline}
Using these arguments, we can claim that the proposed \ac{JSFRA} centralized solution achieves a stationary point of the original nonconvex problem with the nonincreasing objective value at each iteration. 










\begin{comment}
In order to prove the convergence of the proposed iterative algorithm, following conditions are to be satisfied \cite{scutari}
\begin{itemize}
	\item convergence of the \ac{SCA} subproblem	
	\item uniqueness of the transmit and the receive beamformers
	\item monotonic convergence of the objective function
\end{itemize}
In the proposed solution, we replaced \eqref{eqn-6.2} by a convex constraint using the first order approximation, which is majorized by the quadratic-over-linear function in \eqref{eqn-6.2} from below around a fixed point \me{\tilde{\mbf{u}}^{(i)}_{l,k,n}}. Since the \ac{SCA} method is adopted in the proposed algorithm, the constraint approximation satisfies the following conditions as in \cite{marks1978technical}
\begin{subeqnarray} \label{sca-req}
	f(\tilde{\mbf{u}}_{l,k,n}) &\leq& \bar{f}(\tilde{\mbf{u}}_{l,k,n},\tilde{\mbf{u}}^{(i)}_{l,k,n}) \\
	f(\tilde{\mbf{u}}^{(i)}_{l,k,n}) &=& \bar{f}(\tilde{\mbf{u}}^{(i)}_{l,k,n},\tilde{\mbf{u}}^{(i)}_{l,k,n}) \\
	\nabla f(\tilde{\mbf{u}}^{(i)}_{l,k,n}) &=& \nabla \bar{f}(\tilde{\mbf{u}}^{(i)}_{l,k,n},\tilde{\mbf{u}}^{(i)}_{l,k,n}),
\end{subeqnarray}
where \me{\bar{f}(\mbf{x},\mbf{x}^{(i)})} is the approximate function of \me{f(\mbf{x})} around the point \me{\mbf{x}^{\ast(i)}}. The stationary point of the relaxed convex problem satisfies the \ac{KKT} conditions of the original nonconvex problem, which can be obtained by using conditions in \eqref{sca-req}. It can be seen that the  \ac{SCA} relaxed formulation converges to a local stationary point at each iteration.

The uniqueness of the transmit and the receive beamformers can be justified by forcing one antenna to be real valued to exclude the phase ambiguity arising from the complex precoders. The monotonic convergence of the objective function can be justified by the following arguments. At each \ac{SCA} iteration, the relaxed subproblem is solved for the locally optimal transmit precoders to minimize the objective function. Since the \ac{SCA} subproblem is relaxed around the \me{\ith{i-1}} optimal point, \textit{i.e},  \me{\mbf{x}^{\ast(i-1)}} for the \me{\ith{i}} iteration, the domain of the problem in the \me{\ith{i}} step includes optimal point from the  \me{\ith{i-1}} iteration as well. Therefore, at each \ac{SCA} step, the objective function can either be equal to or smaller than the previous value, thereby leading to the monotonic convergence of the objective function.

Once the problem is converged to a stationary transmit precoders, the receive beamformers are updated based on the receivers in \eqref{opt-rx} or \eqref{eqn-10}. The monotonic nature of the objective function is preserved by the receive beamformer update, since the receiver minimizes the objective value for the fixed transmit precoders, and hence the proposed \ac{JSFRA} scheme is guaranteed to converge to a stationary point of the original nonconvex problem.


Following similar approach as in Section \ref{sec-3.2.1}, at each iteration, the \ac{SCA} subproblems converge to a stationary point of the original nonconvex problem. The uniqueness of the precoders are justified if there is no phase ambiguity in the stationary solution. By reorganizing \eqref{eqn-mse-1.3} as
\iftoggle{single_column}{
	\begin{equation}
		\epsilon_{l,k,n} \geq  1 - 2 \Re{ \left \lbrace \mvec{w}{l,k,n}^\herm \mvec{H}{b_k,k,n} \mvec{m}{l,k,n} \right \rbrace} + \sum_{\mathclap{\forall (j,i)}} \left | \mvec{w}{l,k,n}^\herm \mvec{H}{b_i,k,n} \mvec{m}{j,i,n} \right |^2 + N_0 \, \|\mvec{w}{l,k,n}\|^2,
	\end{equation}
}{
\begin{multline}
\epsilon_{l,k,n} \geq  1 - 2 \Re{ \left \lbrace \mvec{w}{l,k,n}^\herm \mvec{H}{b_k,k,n} \mvec{m}{l,k,n} \right \rbrace} \\ 
+ \sum_{\mathclap{\forall (j,i)}} \left | \mvec{w}{l,k,n}^\herm \mvec{H}{b_i,k,n} \mvec{m}{j,i,n} \right |^2 + N_0 \, \|\mvec{w}{l,k,n}\|^2,
\end{multline}
}
we can see that the ambiguity in the phase rotations for the transmit and the receive beamformers are eliminated by the presence of real component in the \ac{MSE} expression. 

At each \ac{SCA} update, the transmit precoders are obtained uniquely by minimizing \eqref{eqn-mse-1} due to the convex nature of the relaxed problem. For a fixed transmit precoders, the \ac{MMSE} receiver improves the objective value \cite{christensen2008weighted,wmmse_shi}, leading to the monotonic convergence of the objective function. At each \ac{SCA} step, the optimal value of the previous iteration is also included in the domain of the problem, and the objective value can either decrease or stays the same after each iteration. Note that the objective function improves at each iteration, whereas the sum rate need not follow the same behavior.
\end{comment}