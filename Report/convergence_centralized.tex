\newcommand{\eqn}[1]{\(#1\)}
\newcommand{\mx}{\mbf{x}}
\newcommand{\my}{\mbf{y}}
\newcommand{\mz}{\mbf{z}}
\newcommand{\mxb}{{{\mbf{x}}}}
\newcommand{\myb}{{{\mbf{y}}}}
\newcommand{\iterate}[2]{{#1}^{(#2)}}
\newcommand{\iter}[3]{{#1}_{#2}^{(#3)}}

\subsubsection{Centralized Problem}

Let us express the \ac{JSFRA} problem in \eqref{eqn-6} and \eqref{eqn-mse-1} as
\begin{IEEEeqnarray}{rCl} \label{con}
\underset{\mx,\my,\mz}{\text{minimize}} &\quad& f(\mz) \eqsub \label{con-obj} \\
\text{subject to} &\quad& h(\mz) - g_0(\mx,\my) \leq 0 \eqsub \label{con-dc} \\
&\quad& g_1(\mx,\my) \leq 0, \eqsub \label{con-cvx-blk} \\
&\quad& g_2(\mx) \leq 0, \eqsub \label{con-cvx}
\end{IEEEeqnarray}
wher \me{g_2,f} are convex functions and \me{h} is a linear function. Let \me{g_0,g_1} are convex functions only on \me{\mx} or \me{\my} as the variable but not on both. Note that the \eqref{con-dc} correspond to the constraints in \eqref{eqn-6.2} and \eqref{eqn-mse-1.2} and \eqref{con-cvx-blk} corresponds to the constraints in \eqref{eqn-6.3} and \eqref{eqn-mse-1.3}. Other convex constraints are addressed by the constraint \eqref{con-cvx}. With this, the feasible set of the problem \eqref{con} is given by 
\begin{IEEEeqnarray}{rl}
\mc{F} &= \{ \mx,\my,\mz | h(\mz) - g_0(\mx,\my) \leq 0, g_1(\mx,\my) \leq 0, g_2(\mx) \leq 0\} \nonumber
\end{IEEEeqnarray}

In order to solve the problem, we resort to \ac{AO} by fixing a block of varibles and optimize for others. In the problem \eqref{con}, even after fixing the variable \me{\my}, the problem is nonconvex due to the \ac{DC} constraint in \eqref{con-dc}. In order to solve the problem after fixing the variable \me{\my}, we adopt the \ac{SCA} approach presented in \cite{lipp2014variations,lanckriet2009convergence,scutari_1} by relaxing the nonconvex set by a sequence of convex subsets. Since the proposed method involves two level of iterations, we denote the \ac{AO} iteration index by a superscript \me{(i)} and the \ac{DC} constraint relaxations by a subscript \me{k}. Let \me{\iter{\mc{X}}{k}{i}} be a feasible set for the \eqn{\ith{i}} \ac{AO} iteration and the \me{\ith{k}} \ac{SCA} point for a fixed \me{\my} and for a fixed \me{\mx}, the set is represented as \me{\iter{\mc{Y}}{k}{i}}. Since the \ac{SCA} iterations are performed until convergence, let \me{\iter{\mx}{\ast}{i}} denotes the converged point of \me{\mx} in the \me{\ith{i}} \ac{AO} iteration.

Without affecting the convergence proof, let us consider the variable \me{\my} is fixed in the iteration \me{i} for \ac{AO} and \me{k} for the \ac{SCA} problem to solve for the unique minimizer for \me{\mx \in \iter{\mc{X}}{k}{i}}. 


In order to solve for a suboptimal solution, let us resort to the \ac{AO} by fixing a block of variables and optimize for others. Before proceeding further, let us define the following notations. Let \me{\iter{\mx}{k}{i}} denote the \me{\ith{i}} \ac{AO} operating point and \me{k} represent the \me{\ith{k}} \ac{SCA} point. Let us define a set \eqn{\iter{\mc{X}}{k}{i}} as set of feasible values of the subproblem in the \me{\ith{i}} \ac{AO} iteration and the \me{\ith{k}} \ac{SCA} optimization point for a fixed variable \me{\my}. Since \ac{AO} is an iterative method, let us begin by fixing \me{\my} with the previously found optimal value \me{\iter{\myb}{\ast}{i-1} \in \iter{\mc{Y}}{\ast}{i-1} \subset \mc{F}}, where \me{\ast} in subscript denotes the optimal value over some set \me{\mc{Y}}. Now the problem in \eqref{con} is still nonconvex due to the constraint \eqref{con-dc}, which is in \ac{DC} form. Following the approaches presented in \cite{lipp2014variations,lanckriet2009convergence,scutari_1}, the function \me{g_0(\mx,\iter{\myb}{\ast}{i-1})} is linearized around a fixed feasible point \me{\iter{\mxb}{\ast}{i} \in \iter{\mc{X}}{\ast}{i-1} \subset \mc{F}} by
\begin{equation} \label{con-relax}
\hat{g}_o(\mx,\iter{\myb}{\ast}{i-1});\iter{\mx}{k}{i})) = {g}_0(\iterate{\mxb}{0},\iterate{\myb}{0}) + \nabla g_0(\iterate{\mxb}{0},\iterate{\myb}{0})^\mathrm{T} (\mx - \iterate{\mxb}{0}).
\end{equation}
Using the relaxation \eqref{con-relax}, the convex subproblem for the \me{\ith{i}} \ac{AO} iterate of \me{\myb} and the \me{\ith{k}} linear approximation of \me{\mxb} as
\begin{subeqnarray} \label{con-m}
	\underset{\mx,\my,\mz}{\text{minimize}} &\quad& f(\mz) \eqsub \label{con-obj-m} \\
	\text{subject to} &\quad& h(\mz) - \hat{g}_0(\mx,\iterate{\myb}{i};\iterate{\mxb}{k}) \leq 0 \eqsub \label{con-dc-m} \\
	&\quad& g_1(\mx,\iterate{\myb}{i}) \leq 0, \eqsub \label{con-cvx-blk-m} \\
	&\quad& g_2(\mx) \leq 0, \eqsub \label{con-cvx-m}
\end{subeqnarray}
Let the set defined by the problem in \eqref{con-relax} be represented as \me{\iterate{\mc{X}}{i}_{k} \subset \mc{F}}, where \me{\mc{X}} denotes the subset in which \me{\my} is fixed. In order to prove the convergence of the convex subproblem \eqref{con-m} for a fixed \me{\iterate{\myb}{i}} and \me{\iter{\mxb}{k}{i}}, let us assume that \eqref{con-m} yields \me{\mx_{k+1}} and \me{\mz_{k+1}} as the solution at the \me{\ith{k}} iteration. To show that \me{\mx_{k+1}} and \me{\mz_{k+1}} minimizes the objective function ans also feasible, let us assume that the point \me{\iterate{\mxb}{k}} is feasible for \eqref{con-m}. Since the function \me{{g}_0(\mx,\iterate{\myb}{i})} is linearized around \me{\iterate{\mxb}{k}}, it satisfies
\begin{equation}
h(\mz) - {g}_0(\mx,\iterate{\myb}{i}) \leq h(\mz) - \hat{g}_0(\mx,\iterate{\myb}{i};\mxb_k) \leq 0,
\end{equation}
since \me{{g}_0(\mx,\iterate{\myb}{i}) \leq \hat{g}_0(\mx,\iterate{\myb}{i};\mxb_k), \forall \mx \in \iterate{\mc{X}}{i}_{k-1}}. Now, \me{\mx_{k+1}} is the solution and feasible for the \me{\ith{k}} subproblem, therefore, it satisfies
\begin{multline}
h(\mz_{k+1}) - {g}_0(\mx_{k+1},\iterate{\myb}{i}) \leq h(\mz_{k+1}) - \hat{g}_0(\mx_{k+1},\iterate{\myb}{i};\mxb_k) \\
\leq h(\mz_{k}) - \hat{g}_0(\mx_{k},\iterate{\myb}{i};\mxb_k) \leq 0. \label{con-sub-set}
\end{multline}
Using \eqref{con-sub-set}, we can prove that the solution \me{\mx_{k+1}} and \me{\mz_{k+1}} are feasible, since the initial point \me{\mx_{0}} was chosen to be feasible. In order to prove the convergence of the objective, using \eqref{con-sub-set}, we can see that \me{\iterate{\mc{X}}{i}_{0} \subseteq \dots \subseteq \iterate{\mc{X}}{i}_{k-1} \subseteq \iterate{\mc{X}_k}{i} \subset \mc{F}}. Since at \me{\ith{k}} iteration, the feasible set of the problem \eqref{con-m} includes the feasible sets of the earlier iteration, we can see that 
\begin{equation} \label{con-convergence}
f(\mz_0) \geq f(\mz_k) \geq f(\mz_{k+1}) \geq f(\mz_{\ast,x,i}). 
\end{equation}
Thus the sequence \me{f(\mz_k)} is nonincreasing and converges to a critical point. Let \me{\mx_{\ast}} and \me{\mz_{\ast,x,i}} be the optimal point of the problem \eqref{con-m} when the iterative algorithm is converged. The symbol \me{x,i} in \me{\mz_{\ast,x,i}} denotes the optimal \me{\mz} is found over the variable \me{\mx} at the \eqn{\ith{i}} \ac{AO} iteration. Note that this point need not be a stationary point of the problem \eqref{con}, since it is the minimizer only in the feasible set \me{\iterate{\mc{X}_{\ast}}{i} \subset \mc{F}}, which depend on \me{\mx} only.

Once the solution is found for a fixed \me{\my}, we fix \me{\mx} as \me{\iterate{\mxb}{i} = \mx_{\ast}} and optimize for \me{\my}. The problem is still nonconvex due to the \ac{DC} constraint. By following similar approach, we can obtain the minimizer \me{\my_{k}} for the convex subproblem at each iteration \me{{k}}. The convergence and the nonincreasing behavior of the problem follows similar arguments as above. Now, the optimal solution of the converged subproblem with \me{\my} as variable is \me{\my_{\ast}} and \me{\mz_{\ast,y,i}}. Note that the solution \me{\mz_{\ast,y,i}} is the minimizer in the subset \me{\iterate{\mc{Y}_{\ast}}{i}}.

Finally, to prove the global convergence of the objective, we need to show the nonincreasing behavior of the objective function between each \ac{AO} update, \textit{i.e}, \me{f_0(\mz_{\ast,x,i}) \geq f_0(\mz_{\ast,y,i}) \geq f_0(\mz_{\ast,x,i+1})}. Let us consider the \ac{AO} iteration \me{i} in which the optimal value for \eqn{\mx} and \eqn{\mz} is obtained as \eqn{\mx^{(i)}_{\ast}} and \eqn{\mz_{\ast,x,i}} using a fixed \eqn{\my} as \eqn{\my_{\ast}^{(i-1)}}. In order to find \eqn{\iter{\my}{\ast}{i}}, we fix \eqn{\mx} as \eqn{\iter{\mx}{\ast}{i}} and optimize for \me{\my}. Since the problem \eqref{con} is nonconvex even after fixing \me{\mx}, we have to linearize the convex function in the \ac{DC} constraint \eqref{con-dc} around some fixed operating point of \me{\my}. Since the linearization is performed with the earlier optimal point \eqn{\iter{\my}{\ast}{i-1}}, the feasible set now includes \eqn{\{\iter{\my}{\ast}{i-1},\iter{\mx}{\ast}{i}\} \in \mc{Y}^{(i)}_{0}}. Now the optimization is performed to find the optimal \me{\my} using the relaxed problem \eqref{con-m}, the optimal solution \me{\mz_{0,y,i}} for the initial iteration after \ac{AO} update follows \me{f_0(\mz_{0,y,i}) \leq f_0(\mz_{\ast,x,i})}, since the feasible set includes the earlier optimal points \eqn{\{\iter{\my}{\ast}{i-1},\iter{\mx}{\ast}{i}\}}. Note that it is not possible to say \me{\iter{\mc{X}}{\ast}{i} \subseteq \iter{\mc{Y}}{0}{i}} since the set is nonconvex on \me{\mx,\my}, but \eqn{\{\iter{\my}{\ast}{i-1},\iter{\mx}{\ast}{i}\} \in \{ \iter{\mc{X}}{\ast}{i} \cap \iter{\mc{Y}}{0}{i} \}}. Now by using induction, we can show that the global problem converges to a feasible point of the nonconvex problem \eqref{con} in a nonincreasing manner.

To show the converged point of the iterative algorithm is in fact the stationary point of the nonconvex problem \eqref{con}, it must satisfy the \ac{KKT} conditions of the nonconvex problem. Since the converged point is the minimizer for the iterative algorithm such that 
\begin{equation} \label{con-opt}
f_0(\mz_{\ast,x,i}) = f_0(\mz_{\ast,y,i}) = f_0(\mz_{\ast,x,i+1}),
\end{equation}
the solution is inside the feasible set \me{\mc{F}} and \me{\mz_{\ast,x,i+1}} is the minimizer of the objective function \me{f_0} in the feasible set \me{\iterate{\mc{X}}{i+1}_{\ast} \subset \mc{F}}, which includes all optimal points of \me{\mz} identified so far. Using the discussions in \cite{marks1978technical}, we can easily show that the feasible point \me{\mz_{\ast,x,i+1}} is a stationary point of the non convex problem in \eqref{con} satisfying the constraint qualifications and the \ac{KKT} expressions for the set \me{\iterate{\mc{X}}{i+1}_{\ast} \subset \mc{F}}. The non differentiability of the objective function in \eqref{eqn-6} and \eqref{eqn-mse-1} requires the subdifferential set of the objective function to include \me{0 \in \partial f_0(\mz_{\ast})} to satisfy the \ac{KKT} conditions.

Note that the uniqueness of the convex subproblems \eqref{con-m} is required for the convergence of \ac{AO}. We can prove the uniqueness of the convex subproblems defined by \eqref{eqn-9} and \eqref{eqn-mse-2} using the following constraints. For the problem \eqref{eqn-9}, the uniqueness of the transmit precoders is guaranteed by the constraint \eqref{eqn-8}, which can be written as
\begin{multline}
\gamma_{l,k,n} + \tilde{\beta}_{l,k,n}^{-2} |\mvec{\tilde{H}}{b_k,k,n} \mvec{\tilde{m}}{l,k,n}|^2 (\beta_{l,k,n} - \tilde{\beta}_{l,k,n}) \\
- \tilde{\beta}_{l,k,n}^{-1} \mvec{\tilde{m}}{l,k,n}^\herm \mvec{\tilde{H}}{b_k,k,n}^\herm \mvec{\tilde{H}}{b_k,k,n} (\mvec{m}{l,k,n} - \mvec{\tilde{m}}{l,k,n}) \leq 0,
\end{multline}
where \eqn{\mvec{\tilde{H}}{b_k,k,n} = \mvec{w}{l,k,n}^\herm \mvec{\tilde{H}}{b_k,k,n}} and for the convex subproblem \eqref{eqn-mse-2}, uniqueness of the transmit precoder is guaranteed by the constraint 
\begin{multline}
| 1 - \mvec{w}{l,k,n}^\herm \mvec{H}{b_k,k,n} \mvec{m}{l,k,n} |^2 \\ + \sum_{\mathclap{(j,i) \neq (l,k)}} | \mvec{w}{l,k,n}^\herm \mvec{H}{b_i,k,n} \mvec{m}{j,i,n} |^2 + \enoise \leq \epsilon_{l,k,n}.
\end{multline}
Using these arguments, we can claim that the proposed \ac{JSFRA} centralized solution achieves a stationary point of the original nonconvex problem with the nonincreasing objective value at each iteration. 










\begin{comment}
In order to prove the convergence of the proposed iterative algorithm, following conditions are to be satisfied \cite{scutari}
\begin{itemize}
	\item convergence of the \ac{SCA} subproblem	
	\item uniqueness of the transmit and the receive beamformers
	\item monotonic convergence of the objective function
\end{itemize}
In the proposed solution, we replaced \eqref{eqn-6.2} by a convex constraint using the first order approximation, which is majorized by the quadratic-over-linear function in \eqref{eqn-6.2} from below around a fixed point \me{\tilde{\mbf{u}}^{(i)}_{l,k,n}}. Since the \ac{SCA} method is adopted in the proposed algorithm, the constraint approximation satisfies the following conditions as in \cite{marks1978technical}
\begin{subeqnarray} \label{sca-req}
	f(\tilde{\mbf{u}}_{l,k,n}) &\leq& \bar{f}(\tilde{\mbf{u}}_{l,k,n},\tilde{\mbf{u}}^{(i)}_{l,k,n}) \\
	f(\tilde{\mbf{u}}^{(i)}_{l,k,n}) &=& \bar{f}(\tilde{\mbf{u}}^{(i)}_{l,k,n},\tilde{\mbf{u}}^{(i)}_{l,k,n}) \\
	\nabla f(\tilde{\mbf{u}}^{(i)}_{l,k,n}) &=& \nabla \bar{f}(\tilde{\mbf{u}}^{(i)}_{l,k,n},\tilde{\mbf{u}}^{(i)}_{l,k,n}),
\end{subeqnarray}
where \me{\bar{f}(\mbf{x},\mbf{x}^{(i)})} is the approximate function of \me{f(\mbf{x})} around the point \me{\mbf{x}^{\ast(i)}}. The stationary point of the relaxed convex problem satisfies the \ac{KKT} conditions of the original nonconvex problem, which can be obtained by using conditions in \eqref{sca-req}. It can be seen that the  \ac{SCA} relaxed formulation converges to a local stationary point at each iteration.

The uniqueness of the transmit and the receive beamformers can be justified by forcing one antenna to be real valued to exclude the phase ambiguity arising from the complex precoders. The monotonic convergence of the objective function can be justified by the following arguments. At each \ac{SCA} iteration, the relaxed subproblem is solved for the locally optimal transmit precoders to minimize the objective function. Since the \ac{SCA} subproblem is relaxed around the \me{\ith{i-1}} optimal point, \textit{i.e},  \me{\mbf{x}^{\ast(i-1)}} for the \me{\ith{i}} iteration, the domain of the problem in the \me{\ith{i}} step includes optimal point from the  \me{\ith{i-1}} iteration as well. Therefore, at each \ac{SCA} step, the objective function can either be equal to or smaller than the previous value, thereby leading to the monotonic convergence of the objective function.

Once the problem is converged to a stationary transmit precoders, the receive beamformers are updated based on the receivers in \eqref{opt-rx} or \eqref{eqn-10}. The monotonic nature of the objective function is preserved by the receive beamformer update, since the receiver minimizes the objective value for the fixed transmit precoders, and hence the proposed \ac{JSFRA} scheme is guaranteed to converge to a stationary point of the original nonconvex problem.


Following similar approach as in Section \ref{sec-3.2.1}, at each iteration, the \ac{SCA} subproblems converge to a stationary point of the original nonconvex problem. The uniqueness of the precoders are justified if there is no phase ambiguity in the stationary solution. By reorganizing \eqref{eqn-mse-1.3} as
\iftoggle{single_column}{
	\begin{equation}
		\epsilon_{l,k,n} \geq  1 - 2 \Re{ \left \lbrace \mvec{w}{l,k,n}^\herm \mvec{H}{b_k,k,n} \mvec{m}{l,k,n} \right \rbrace} + \sum_{\mathclap{\forall (j,i)}} \left | \mvec{w}{l,k,n}^\herm \mvec{H}{b_i,k,n} \mvec{m}{j,i,n} \right |^2 + N_0 \, \|\mvec{w}{l,k,n}\|^2,
	\end{equation}
}{
\begin{multline}
\epsilon_{l,k,n} \geq  1 - 2 \Re{ \left \lbrace \mvec{w}{l,k,n}^\herm \mvec{H}{b_k,k,n} \mvec{m}{l,k,n} \right \rbrace} \\ 
+ \sum_{\mathclap{\forall (j,i)}} \left | \mvec{w}{l,k,n}^\herm \mvec{H}{b_i,k,n} \mvec{m}{j,i,n} \right |^2 + N_0 \, \|\mvec{w}{l,k,n}\|^2,
\end{multline}
}
we can see that the ambiguity in the phase rotations for the transmit and the receive beamformers are eliminated by the presence of real component in the \ac{MSE} expression. 

At each \ac{SCA} update, the transmit precoders are obtained uniquely by minimizing \eqref{eqn-mse-1} due to the convex nature of the relaxed problem. For a fixed transmit precoders, the \ac{MMSE} receiver improves the objective value \cite{christensen2008weighted,wmmse_shi}, leading to the monotonic convergence of the objective function. At each \ac{SCA} step, the optimal value of the previous iteration is also included in the domain of the problem, and the objective value can either decrease or stays the same after each iteration. Note that the objective function improves at each iteration, whereas the sum rate need not follow the same behavior.
\end{comment}