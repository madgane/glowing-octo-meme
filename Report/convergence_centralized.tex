\newcommand{\eqn}[1]{\(#1\)}
\newcommand{\mx}{\mbf{m}}
\newcommand{\my}{\mbf{w}}
\newcommand{\mz}{\mbfa{\gamma}}
\newcommand{\mxb}{{{\mbf{m}}}}
\newcommand{\myb}{{{\mbf{w}}}}
\newcommand{\iterate}[2]{{#1}^{(#2)}}
\newcommand{\iter}[3]{{#1}_{#2}^{(#3)}}
\newcommand{\ma}{\mbf{x}}

To prove the convergence of the transmit precoders, receive beamformers, and the objective values of the centralized algorithms in \eqref{eqn-6} and \eqref{eqn-mse-1}, we need to show that the following conditions are satisfied by the centralized formulations.
\begin{itemize}
	\item[(a)] The function should be coercive and bounded below
	\item[(b)] The feasible set should be a compact set
	\item[(c)] The sequence of the objective values should be strictly decreasing in each iteration
	\item[(d)] Uniqueness of the minimizer, \textit{i.e}, the transmit and the receive beamformers should be unique in each iteration.
\end{itemize}
Using \cite[Prop. A.8]{bertsekas1999nonlinear}, the existence of a global minimizer in the feasible set can be guaranteed if the conditions (a) and (b) are satisfied. Since the feasible set is not fixed in each iteration, we require additional conditions (c) and (d) to prove the global convergence of the objective function and the corresponding arguments namely, the transmit and the receive precoders. Assuming the conditions (a), (b) and (c) are satisfied, using \cite[Th. 3.14]{rudin1964principles}, we can show that the bounded monotonically decreasing objective value sequence has a unique minimum. 

\subsection{Boundedness of the Objective and Compactness of the Set}

The feasible set of the problem \eqref{eqn-6} and \eqref{eqn-mse-1} are bounded and closed, which can be verified by the total power constraint on the transmit precoders \eqref{eqn-6.4}, and therefore, a compact set. 

The minimum value of the norm operator in the objective is zero, \textit{i.e}, the limiting point is \me{< -\infty}, therefore it is bounded below. The objective function is Lipschitz continuous over the feasible set, and therefore, it is bounded from above as well, since the feasible set is bounded. The objective function in \eqref{eqn-6} and \eqref{eqn-mse-1} is continuous and approaches \me{\infty} as \me{\mbf{t}_k \rightarrow \infty}, therefore it is coercive. Note that the norm operator is not differentiable at the minimum point for \me{\ell_1} and \me{\ell_infty}. Using conditions (a) and (b), we can guarantee the existence of a minimizer to the nonconvex problem in \eqref{eqn-6} and \eqref{eqn-mse-1}.

\subsection{Monotonicity}

Let us express the centralized problem in \eqref{eqn-6} and \eqref{eqn-mse-1} as
\begin{IEEEeqnarray}{rCl} \label{con}
	\underset{\mx,\my,\mz}{\text{minimize}} &\quad& f(\mx,\my,\mz) \eqsub \label{con-obj} \\
	\text{subject to} &\quad& h(\mz) - g_0(\mx,\my) \leq 0 \eqsub \label{con-dc} \\
	&\quad& g_1(\mx,\my) \leq 0, \eqsub \label{con-cvx-blk} \\
	&\quad& g_2(\mx) \leq 0, \eqsub \label{con-cvx}
\end{IEEEeqnarray}
where \me{g_2,f} are convex and \me{h} is a linear function. Let \me{g_0,g_1} be convex in either \me{\mx} or \me{\my} but not on both. The constraint in \eqref{con-dc} corresponds to \eqref{eqn-6.2} or \eqref{eqn-mse-1.2} and the constraint \eqref{con-cvx-blk} correspond to \eqref{eqn-6.3} or \eqref{eqn-mse-1.3}. Other convex constraints are addressed by \eqref{con-cvx} and the feasible set of \eqref{con} is given by 
\begin{IEEEeqnarray}{rl}
\mc{F} = \{ \; \mx,\my,\mz \; \big | \; & h(\mz) - g_0(\mx,\my) \leq 0, \nonumber \\
								& g_1(\mx,\my) \leq 0, g_2(\mx) \leq 0 \; \} \nonumber
\end{IEEEeqnarray}

To solve \eqref{con}, we adopt \ac{AO} by fixing a block of varibles and optimize for others \cite{bezdek2002some}. In \eqref{con}, even after fixing the variable \me{\my}, the problem is nonconvex due to the \ac{DC} constraint \eqref{con-dc}. We adopt \ac{SCA} presented in \cite{lipp2014variations,lanckriet2009convergence,scutari_1} by relaxing the nonconvex set by a sequence of convex subsets. Since it involves two nested iterations, we denote the \ac{AO} iteration index by a superscript \me{(i)} and the \ac{DC} constraint relaxations by a subscript \me{k}. Let \me{\iter{\mc{X}}{k}{i}} be the feasible set for the \eqn{\ith{i}} \ac{AO} iteration and the \me{\ith{k}} \ac{SCA} point for a fixed \me{\my} and \me{\iter{\mc{Y}}{k}{i}} denotes the feasible set for a fixed \me{\mx}. Since the \ac{SCA} iterations are performed until convergence, let \me{\iter{\mx}{\ast}{i}} denote the converged point of \me{\mx} in the \me{\ith{i}} \ac{AO} iteration. Let \me{\iter{\mz}{\ast|y}{i}} be the optimal value of \me{\mz} obtained in the \me{\ith{i}} \ac{AO} iterate for a fixed \me{\my}.

To begin with, let us consider the variable \me{\my} is fixed for the \ac{AO} \me{i} with the optimal value achieved from the previous iteration \me{i-1} as \me{\iter{\my}{\ast}{i-1}}. In order to solve for \me{\mx} in the \ac{SCA} iteration \me{k}, we linearize the nonconvex function \me{g_0} using previous \ac{SCA} iterate of \me{\mx} as
\begin{multline} \label{con-relax}
\hat{g}_o(\mx,\iter{\my}{\ast}{i-1};\iter{\mx}{k}{i}) = {g}_0(\iter{\mx}{k}{i},\iter{\my}{\ast}{i-1}) \\ + \nabla g_0(\iter{\mx}{k}{i},\iter{\my}{\ast}{i-1})^{\mathrm{T}} (\mx - \iter{\mx}{k}{i}).
\end{multline}
Using \eqref{con-relax}, the convex subproblem for \me{\ith{i}} \ac{AO} iteration and \me{\ith{k}} \ac{SCA} point for the variable \me{\mx} and \me{\mz} is given by
\begin{subeqnarray} \label{con-m}
	\underset{\mx,\mz}{\text{minimize}} &\quad& f(\mx,\iter{\my}{\ast}{i-1},\mz) \eqsub \label{con-obj-m} \\
	\text{subject to} &\quad& h(\mz) - \hat{g}_0(\mx,\iter{\my}{\ast}{i-1};\iter{\mx}{k}{i}) \leq 0 \eqsub \label{con-dc-m} \\
	&\quad& g_1(\mx,\iter{\my}{\ast}{i-1}) \leq 0, \eqsub \label{con-cvx-blk-m} \\
	&\quad& g_2(\mx) \leq 0, \eqsub \label{con-cvx-m}
\end{subeqnarray}
Let the feasible set defined by the problem in \eqref{con-m} be represented as \me{\iter{\mc{X}}{k}{i} \subset \mc{F}}. In order to prove the convergence of the convex subproblem \eqref{con-m} for a fixed \me{\my = \iter{\my}{\ast}{i-1}} operating at \me{\iter{\mx}{k}{i}}, let us consider that \eqref{con-m} yields \me{\iter{\mx}{k+1}{i}} and \me{\iter{\mz}{k+1}{i}} as the solution for the \me{\ith{k}} iteration. Note that the point \me{\iter{\mx}{k+1}{i}} and \me{\iter{\mz}{k+1}{i}}, which minimizes the objective function, is also feasible for \eqref{con-m} using the following inequality
\allowdisplaybreaks{\begin{multline}\label{con-sub-set}
h(\iter{\mz}{k+1}{i}) - {g}_0(\iter{\mx}{k+1}{i},\iter{\my}{\ast}{i-1}) \leq - \hat{g}_0(\iter{\mx}{k+1}{i},\iter{\my}{\ast}{i-1};\iter{\mx}{k}{i}) \\ + h(\iter{\mz}{k+1}{i}) \leq h(\iter{\mz}{k}{i}) - \hat{g}_0(\iter{\mx}{k}{i},\iter{\my}{\ast}{i-1};\iter{\mx}{k}{i}) \leq 0. 
\end{multline}}
Using \eqref{con-sub-set}, we can prove that the solution \me{\iter{\mx}{k+1}{i}} and \me{\iter{\mz}{k+1}{i}} are feasible, since the initial point of \me{\mx = \iter{\mx}{\ast}{i-1}} was chosen to be feasible from the earlier \ac{AO} iteration \me{i-1}. At each \ac{SCA} iteration, the feasible set includes the optimal point from the previous iteration as \me{\{\iter{\mx}{k+1}{i},\iter{\my}{\ast}{i-1},\iter{\mz}{k+1}{i}\} \in \iter{\mc{X}}{k+1}{i} \subset \mc{F}}, thereby, leading to the monotonic decrease in the objective values \cite{lanckriet2009convergence,scutari_1,quoc2011sequential} as
\begin{multline} \label{con-convergence}
f(\iter{\mx}{0}{i},\iter{\my}{\ast}{i-1},\iter{\mz}{0}{i}) \geq f(\iter{\mx}{k}{i},\iter{\my}{\ast}{i-1},\iter{\mz}{k}{i}) \\ \geq f(\iter{\mx}{k+1}{i},\iter{\my}{\ast}{i-1},\iter{\mz}{k+1}{i}) \geq f(\iter{\mx}{\ast}{i},\iter{\my}{\ast}{i-1},\iter{\mz}{\ast|y}{i}). 
\end{multline}
Thus the sequence \me{f(\iter{\mx}{k}{i},\iter{\my}{\ast}{i-1},\iter{\mz}{k}{i})} is nonincreasing and approaches limiting point as \me{k \rightarrow \infty}. Note that feasible point \me{(\iter{\mx}{\ast}{i}, \iter{\my}{\ast}{i-1},\iter{\mz}{\ast|y}{i})} need not be a stationary point of \eqref{con}, since it is the minimizer over the set \me{\iter{\mc{X}}{\ast}{i} \subset \mc{F}}, for a fixed \me{\my}.

Once the solution is found for a fixed \me{\my}, we fix \me{\mx} as \me{\iter{\mx}{\ast}{i}} and optimize for \me{\my}. Even after treating \me{\mx} as a constant, the problem is still nonconvex due to the \ac{DC} constraint. Following similar approach, we can find the minimizer \me{\iter{\my}{k}{i}} and \me{\iter{\mz}{k}{i}} for a similar convex subproblem \eqref{con-m} at each iteration \me{{k}}. Note that \me{\iter{\mz}{k}{i}} is reused since the variable \me{\mx} is fixed for the \me{\ith{i}} \ac{AO} iteration. The convergence and the nonincreasing behavior of the problem follows similar arguments as above\footnote{Note that we can also use the \ac{MMSE} receiver in \eqref{eqn-10} instead of performing the \ac{SCA} updates until convergence for the optimal receiver}. Now, the optimal solution of the converged subproblems with \me{\my} as variable are \me{\iter{\my}{\ast}{i}} and \me{\iter{\mz}{\ast}{i}}. Note that the limiting point \me{(\iter{\mx}{\ast}{i}, \iter{\my}{\ast}{i},\iter{\mz}{\ast|x}{i})} is the unique minimizer in the set \me{\iter{\mc{Y}}{\ast}{i}}.

Finally, to prove the global convergence of the objective, we need to show the nonincreasing behavior of the objective function between each \ac{AO} update, \textit{i.e}, 
\allowdisplaybreaks{\begin{multline}
f(\iter{\mx}{\ast}{i},\iter{\my}{\ast}{i},\iter{\mz}{\ast|x}{i}) \leq f(\iter{\mx}{\ast}{i},\iter{\my}{0}{i},\iter{\mz}{0}{i}) \\
\leq f(\iter{\mx}{\ast}{i},\iter{\my}{\ast}{i-1},\iter{\mz}{\ast|y}{i}).
\end{multline}}
Let us consider an \ac{AO} iteration \me{i} in which the optimal value for \eqn{\mx} and \eqn{\mz} are obtained as \me{\iter{\mx}{\ast}{i}} and \me{\iter{\mz}{\ast|y}{i}} using fixed \eqn{\my = \iter{\my}{\ast}{i-1}}. To find \me{\iter{\my}{0}{i}}, we fix \eqn{\mx} as \me{\iter{\mx}{\ast}{i}} and optimize for \me{\my}. Since we linearize the convex function in \eqref{con-dc}, the fixed operating point is also included in the feasible set \eqn{\{\iter{\my}{\ast}{i-1},\iter{\mx}{\ast}{i},\iter{\mz}{\ast|y}{i}\} \in \iter{\mc{Y}}{0}{i}} using \eqref{con-sub-set}. Using this, we can show the monotonicity of the objective value sequence as
\begin{equation*}
f(\iter{\mx}{\ast}{i},\iter{\my}{0}{i},\iter{\mz}{0}{i}) \leq f(\iter{\mx}{\ast}{i},\iter{\my}{\ast}{i-1},\iter{\mz}{\ast|x}{i}).
\end{equation*}
The \ac{AO} update follows \eqn{\{\iter{\my}{\ast}{i-1},\iter{\mx}{\ast}{i},\iter{\mz}{\ast|y}{i}\} \in \{ \iter{\mc{X}}{\ast}{i} \cap \iter{\mc{Y}}{0}{i} \}}. %Using the conditions (a),(b) and (c), we can show that the objective values of the iterative algorithm converges to a limiting point or a local optimal point of the original nonconvex problem \eqref{con}.

\subsection{Uniqueness}

The uniqueness of the transmit precoders and the receive beamformers for the convex subproblem can be guaranteed in general by the constraint \eqref{eqn-8} for the problem \eqref{eqn-9} and \eqref{eqn-mse-1.3} for the \ac{MSE} reformulation in \eqref{eqn-mse-2}. Note that the receive beamformer \me{\mvec{w}{l,k,n}} in \eqref{eqn-10} is also unique for the given set of transmit precoders, since the convex subproblems in \eqref{eqn-9} and \eqref{eqn-mse-2} are susceptible to the transmit precoders phase rotations and thereby having a unique minimizer. 

When the objective function is zero, the uniqueness of the transmit precoders are not guaranteed by using \eqref{eqn-8} and \eqref{eqn-mse-1.3} constraints, since they are not active at the optimal solution of the subproblems. To obtain an unique set of transmit precoders when the objective is zero even for a single \ac{BS}, we can regularize the objective with the transmit power as discussed in Appendix \ref{a-3} without affecting the optimal value.
%we can regularize the objective with the total transmit power expression without affecting the optimal solution as 
%\begin{equation*}
%\| \tilde{\mbf{v}} \|_q + \varphi \sum_{k \in \mc{U}} \sum_{n = 1}^{N} \sum_{l=1}^{L} \mathrm{tr} \left ( \mvec{m}{l,k,n} \mbf{m}^\herm_{l,k,n} \right ),
%\end{equation*}
%for some arbitrarily small value of the scaling factor \me{\varphi \approx 0}. 

\subsection{Convergence of the beamformer iterates}

Let \me{\mbf{\ma} \triangleq [\mx,\my,\mz]} be the stacked vector of the optimization variables and let \me{\mc{A}} be a point-to-set mapping from \me{\mc{F}} into the nonempty subsets of \me{\mc{F}}. The objective function \me{f} in \eqref{con-cvx} is continuous and the iterative algorithm yields monotonically decreasing objective values \me{f(\ma^{(n)}) \leq f(\ma^{(n-1)})} in each iteration. Therefore, the operator \me{\mc{A}} is monotonic with respect to a continuous objective function \me{f} yielding a sequence of beamforming iterates \me{\{\ma^{(n)}\}}. The following conditions are satisfied by the iterative algorithm \cite{meyer1976sufficient}.
\begin{itemize}

\end{itemize}

 

. Let \me{\{\ma^{(n)}\}} be the sequence of beamforming iterates generated by the function \me{f} in \eqref{con-cvx} and the mapping operator \me{\mc{A}}. To prove the convergence of the beamformer iterates, following conditions are required \cite{meyer1976sufficient}.
\begin{itemize}
\item[(e)] The mapping operator \
\end{itemize}



approaches a limiting or accumulation point \me{\ma^{\ast} \in \mc{F}^\ast} as \me{n \rightarrow \infty}, where \me{\mc{F}^{\ast}} represents the set of accumulation points for the

\subsection{Stationary Point}

To show the limiting point of the iterative algorithm is the stationary point of \eqref{con}, it must satisfy the \ac{KKT} conditions of the nonconvex problem. As \me{i \rightarrow \infty} asymptotically, it satisfies
\allowdisplaybreaks{\begin{multline} \label{con-opt}
f(\iter{\mx}{\ast}{i},\iter{\my}{\ast}{i},\iter{\mz}{\ast|x}{i}) = f(\iter{\mx}{\ast}{i+1},\iter{\my}{\ast}{i},\iter{\mz}{\ast|y}{i+1}) \\ = 
f(\iter{\mx}{\ast}{i+1},\iter{\my}{\ast}{i+1},\iter{\mz}{\ast|x}{i+1}),
\end{multline}}
where the solution point is inside the feasible set \me{\mc{F}} and \me{(\iter{\mx}{\ast}{i+1},\iter{\my}{\ast}{i+1},\iter{\mz}{\ast|x}{i+1})} is the minimizer of the objective function \me{f_0} in the feasible set \me{\iterate{\mc{X}}{i+1}_{\ast} \subset \mc{F}}. Using the discussions in \cite{marks1978technical}, we can easily show that the feasible point \me{\mc{F}} and \me{(\iter{\mx}{\ast}{i+1},\iter{\my}{\ast}{i+1},\iter{\mz}{\ast|x}{i+1})}, which is the minimizer in the local neighborhood, is a stationary point of the non convex problem in \eqref{con} satisfying the constraint qualifications and the \ac{KKT} expressions for the set \me{\iterate{\mc{X}}{i+1}_{\ast} \subset \mc{F}}. The non differentiability of the objective function in \eqref{eqn-6} and \eqref{eqn-mse-1} requires the subdifferential set of the objective function to include \me{0 \in \partial f_0(\mz_{\ast})} to satisfy the \ac{KKT} conditions. Using the above arguments, we can show that the \ac{JSFRA} schemes attains a stationary point of the original nonconvex problem. 
