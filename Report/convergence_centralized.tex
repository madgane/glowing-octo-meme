%\begin{comment}
%
%The following conditions are required to show the convergence of Algorithm \ref{algo-1}, which is used to solve problems \eqref{eqn-6} and \eqref{eqn-mse-1} in an iterative manner.
%\begin{itemize}
%	\item[(a)] Objective function should be bounded below
%	\item[(b)] Feasible set should be compact
%	\item[(c)] Sequence of objective values should be strictly decreasing
%	\item[(d)] Uniqueness of the minimizer in each step.
%\end{itemize}
%\review{The conditions (a), (b), and (c) are required to prove the convergence of objective sequence generated by Algorithm \ref{algo-1}. However, to ensure strict monotonicity of the objective sequence, we require condition (d). Therefore, by assuming (a), (b), and (c) are satisfied by Algorithm \ref{algo-1}, the convergence of the objective sequence can be shown by using \cite[Th. 3.14]{rudin1964principles}. Finally, by using the discussions in \cite{marks1978technical,lanckriet2009convergence,scutari_1}, we show that \reviewF{every limit point of the sequence of iterates generated by Algorithm \ref{algo-1} is a stationary point of nonconvex problems \eqref{eqn-6} and \eqref{eqn-mse-1}}. The term iterates denotes the stacked vector of solution variables obtained in each iteration.}
%
%Before discussing the convergence analysis, let us consider a generalized formulation for the problems in \eqref{eqn-6} and \eqref{eqn-mse-1} as
%\begin{IEEEeqnarray}{RcL} \label{con} \neqsub
%	\underset{\mx,\my,\mz}{\text{minimize}} &\quad& \hat{f}(\mx,\my,\mz) \eqsub \label{con-obj} \\
%	\text{subject to} &\quad& h(\mz) - g_0(\mx,\my) \leq 0 \eqsub \label{con-dc} \\
%	&\quad& g_1(\mx,\my) \leq 0 \eqsub \label{con-cvx-blk} \\
%	&\quad& g_2(\mx) \leq 0 \eqsub \label{con-cvx}
%\end{IEEEeqnarray}
%where the functions \me{g_2} and \me{\hat{f}} are convex, and the function \me{h} is linear. Let functions \me{g_0} and \me{g_1} be convex w.r.t either \me{\mx} or \me{\my}, but not jointly convex on both variables. The constraint \eqref{con-dc} corresponds to \eqref{eqn-6.2} or \eqref{eqn-mse-1.2} and the constraint \eqref{con-cvx-blk} corresponds to \eqref{eqn-6.3} or \eqref{eqn-mse-1.3} respectively. Other convex constraints are handled by \eqref{con-cvx} and the feasible set of \eqref{con} is given by 
%\iftoggle{single_column}{
%	\begin{equation}
%		\mc{F} = \{ \; \mx,\my,\mz \; \big | h(\mz) - g_0(\mx,\my) \leq 0, g_1(\mx,\my) \leq 0, g_2(\mx) \leq 0 \}.
%	\end{equation}
%}{\begin{align}
%\mc{F} = \{ \; \mx,\my,\mz \; \big | & \quad h(\mz) - g_0(\mx,\my) \leq 0, \nonumber \\
%& \quad g_1(\mx,\my) \leq 0, g_2(\mx) \leq 0 \}.
%\end{align}}
%
%\review{We use the following notations to discuss the convergence proof. Since Algorithm \ref{algo-1} involves two nested iterations, \textit{i.e.}, one for \ac{SCA} and another one for \ac{AO}, we denote \ac{AO} step by a superscript \me{(i)} and \ac{SCA} iteration by a subscript \me{k}. Let \me{\mx}, \me{\my}, and \me{\mz} be the stacked vector all transmit precoders, receive beamformers, and other optimization variables present in the formulations \eqref{eqn-6} and \eqref{eqn-mse-1} respectively. Since Algorithm \ref{algo-1} includes two \ac{SCA} iterations performed until convergence in each \ac{AO} iteration, let us now consider \ac{AO} step \eqn{i} for fixed \eqn{\my}. The solution obtained at the \eqn{\ith{k}} \ac{SCA} step is represented as \eqn{\mx=\iter{\mx}{k}{i}} and \eqn{\mz=\iter{\mz}{k}{i}}. As \eqn{k \to \infty}, the objective sequence converges, and the corresponding solution obtained is denoted as \me{\iter{\mx}{\lpointx}{i}} and \me{\iter{\mz}{\lpointx|\my}{i}}. Similarly, for fixed \eqn{\mx} and as \eqn{k \to \infty}, the corresponding solution obtained is represented as \me{\my = \iter{\my}{\lpointx}{i}} and \me{\mz = \iter{\mz}{\lpointx|\mx}{i}} respectively.}
%
%%We denote \me{\iter{\mx}{\lpointx}{i}} and \me{\iter{\mz}{|\my}{i}} as the solution obtained for \me{\mx} and \me{\mz} when the sequence of objective values converges in \eqn{\ith{i}} \ac{AO} step for fixed \eqn{\my}. Similarly, \me{\iter{\my}{\lpointx}{i}} and \me{\iter{\mz}{|\mx}{i}} correspond to the solutions of \eqn{\my} and \eqn{\mz} in \eqn{\ith{i}} \ac{AO} update for fixed \eqn{\mx}.
%
%\subsection{Bounded Objective Function and Compact Feasible Set} \label{b_obj}
%The feasible sets of the problems \eqref{eqn-6} and \eqref{eqn-mse-1} are bounded and closed, which is due to the total power constraint on the transmit precoders \eqref{eqn-6.4}, therefore, the sets are compact. 
%
%The minimum of the norm in the objective is \me{\|x\|_q > -\infty}, therefore, it is bounded below. The objective function is Lipschitz continuous over the feasible set. The objective function is bounded from above as well, since the feasible set is bounded. %The objective function in \eqref{eqn-6} and \eqref{eqn-mse-1} is continuous and approaches \me{\infty} as \me{\mbf{t}_k \to \infty}, and therefore it is coercive in the feasible set. %Using conditions (a) and (b), we can guarantee the existence of a minimizer to the problem in \eqref{eqn-6} and \eqref{eqn-mse-1}.
%
%\subsection{Uniqueness of the Iterates and Strong Convexity} \label{c-a}
%%The uniqueness of the iterates \eqn{\{\iter{\mx}{k}{i},\iter{\my}{\lpointx}{i-1},\iter{\mz}{k|\my}{i}\}} can be ensured for the \ac{MSE} reformulated problem in \eqref{eqn-mse-2} when all the constraints are active. However when \eqn{\hat{f}(\iter{\mx}{k}{i},\iter{\my}{\lpointx}{i-1},\iter{\mz}{k|\my}{i}) = 0} after some iteration, say \me{k} and \me{i}, the uniqueness cannot be guaranteed as there can be multiple solutions with the same objective value in the feasible set. Similarly, when the objective is non-zero for \eqref{eqn-9}, the minimizer is unique only if the receivers are matched to the transmit beamformers. %, \textit{i.e.}, \me{q_{l,k,n} = 0} in \eqref{eqn-wsrm-expr}, while initializing the feasible point \eqn{\iter{\mx}{0}{0}} and \eqn{\iter{\my}{0}{0}}.
%
%To ensure the uniqueness of the transmit and the receive beamformers, the objective function of the convex subproblems in \eqref{eqn-9} and \eqref{eqn-mse-2} are regularized by a strongly convex function in each \ac{SCA} iteration \eqn{k} and \ac{AO} update step \eqn{i} as 
%\begin{IEEEeqnarray}{rCl} \neqsub
%	\hat{f}(\mbf{z}) &=& \| \tilde{\mbf{v}} \|_q \eqsub \label{orig_obj} \\ 
%	f(\mbf{z}) &=& \hat{f}(\mbf{z}) + {\tau}_k^{(i)} \, \| \mbf{z} - \mbf{z}^{(i)}_{k} \|^2_2 \eqsub \label{mod_obj} 
%\end{IEEEeqnarray}
%where \eqn{\mbf{z}} is a vector stacking all optimization variables, which can be either \eqn{[\mx,\iter{\my}{\lpoint}{i-1},\mz]} or \eqn{[\iter{\mx}{\lpoint}{i},\my,\mz]} depending on the problem and \eqn{\mbf{z}^{(i)}_{k} \triangleq [\mx_{k}^{(i)},\iter{\my}{\lpoint}{i-1},\mz_{k}^{(i)}]} is the solution obtained in the previous \eqn{\ith{(k-1)}} \ac{SCA} step. The positive constant \me{{\tau}_{k}^{(i)} > 0} ensures the strong convexity of \eqn{f} in each step and \eqn{c > 0} be the constant of strong convexity, defined as \eqn{c \triangleq \min_{k} \{{\tau}_{k}^{(i)}\}}, as shown in \cite{scutari-1}. Thus, due to the strong convexity of \eqn{f} in \eqref{mod_obj}, uniqueness of the minimizer is guaranteed in each \ac{SCA} step as discussed in \cite{yang_yang,scutari-1}. 
%
%%in each step and let \eqn{c > 0} be the constant of strong convexity of \eqn{f} with \eqn{c \triangleq \min_{k} \{{\tau}_{k}^{(i)}\}} as in \cite{scutari-1}.
%%The uniqueness of the transmit precoders and the receive beamformers for the \ac{MSE} reformulation in \eqref{eqn-mse-2} can be guaranteed in each iteration by the \ac{MSE} constraint in \eqref{eqn-mse-1.3}. The uniqueness of the iterates for the problem in \eqref{eqn-9} can be guaranteed in each convex subproblem if the receivers are matched to the equivalent downlink channel with the initially chosen feasible transmit precoders, \textit{i.e.}, \me{q_{l,k,n} = 0 \: \forall \, l,k,n} in \eqref{eqn-wsrm-expr} while beginning the \ac{SCA} iteration.
%%of the objective
%%When the objective is zero, the uniqueness of  the transmit and the receive beamformers are not guaranteed in both problem formulations, since they are the under-estimators and need not be active at the minimum. To obtain a unique set of transmit precoders and the receive beamformers in all scenarios, we can regularize the objective with a strongly convex function as
%%\begin{equation}
%%\| \tilde{\mbf{v}} \|_q + c \, \| \mbf{x} - \mbf{x}^{(i)} \|^2
%%\end{equation}
%%where \me{\mbf{x}} is the vector containing all the optimization variables and \me{c} is a positive constant. The vector \me{ \mbf{x}^{(i)}} is the value of \me{\mbf{x}} in the \me{\ith{i}} iteration. %Note that the uniqueness of the transmit precoders and the receive beamformers for each convex subproblem can be shown by using the constraint \eqref{eqn-8} for the problem \eqref{eqn-9} and \eqref{eqn-mse-1.3} for the \ac{MSE} reformulation in \eqref{eqn-mse-2}. The receive beamformers \me{\mvec{w}{l,k,n}} in \eqref{eqn-10} are also unique for the given set of transmit precoders, since the convex subproblems in \eqref{eqn-9} and \eqref{eqn-mse-2} are susceptible to the transmit precoder phase rotations, \textit{i.e.}, unitary transformations. When the objective function is zero, the uniqueness of the transmit precoders are not guaranteed by using \eqref{eqn-8} and \eqref{eqn-mse-1.3} constraints, since they are not active at the optimal solution of the subproblems. To obtain unique set of transmit precoders when the objective is zero even for a single \ac{BS}, we can regularize the objective with the transmit power as discussed in Appendix \ref{a-3} without affecting the optimal value.
%
%\subsection{Strict Monotonicity of the Objective Sequence} \label{mcity}
%In the following discussions, we consider the modified objective \eqn{f} in \eqref{mod_obj} instead of \eqn{\hat{f}} due to the uniqueness of the minimizer and upon convergence of the objective, \eqn{f} is equal to \eqn{\hat{f}}. Therefore, the discussions are valid for the \ac{JSFRA} problem in \eqref{eqn-6} by regularizing the objective \eqref{eqn-6.1} as \eqref{mod_obj}.
%
%To begin with, let us consider the variable \me{\my} is fixed for the \me{\ith{i}} \ac{AO} with the optimal value found in previous iteration \me{i-1} as \me{\iter{\my}{\lpoint}{i-1}}. In order to solve for \me{\mx} in \ac{SCA} iteration \me{k}, we linearize the nonconvex function \me{g_0} using previous \ac{SCA} iterate \me{\iter{\mx}{k}{i}} of \me{\mx} as
%\iftoggle{single_column}{
%\begin{equation}\label{con-relax}
%\hat{g}_o(\mx,\iter{\my}{\lpoint}{i-1};\iter{\mx}{k}{i}) = {g}_0(\iter{\mx}{k}{i},\iter{\my}{\lpoint}{i-1}) + \nabla g_0(\iter{\mx}{k}{i},\iter{\my}{\lpoint}{i-1})^{\mathrm{T}} (\mx - \iter{\mx}{k}{i}).
%\end{equation}}{
%\begin{multline} \label{con-relax}
%\hat{g}_o(\mx,\iter{\my}{\lpoint}{i-1};\iter{\mx}{k}{i}) = {g}_0(\iter{\mx}{k}{i},\iter{\my}{\lpoint}{i-1}) \\ + \nabla g_0(\iter{\mx}{k}{i},\iter{\my}{\lpoint}{i-1})^{\mathrm{T}} (\mx - \iter{\mx}{k}{i}).
%\end{multline}}
%Let \me{\iter{\mc{X}}{k}{i}} be the feasible set for the \eqn{\ith{i}} \ac{AO} iteration and the \me{\ith{k}} \ac{SCA} point for a fixed \me{\iter{\my}{\lpoint}{i-1}} and \me{\iter{\mx}{k}{i}}. Similarly, \me{\iter{\mc{Y}}{k}{i}} denotes the feasible set for a fixed \me{\iter{\mx}{\lpoint}{i}} and \me{\iter{\my}{k}{i}}. Using \eqref{con-relax}, the convex subproblem for the \me{\ith{i}} \ac{AO} iteration and the \me{\ith{k}} \ac{SCA} point for variables \me{\mx} and \me{\mz} is given by
%\begin{IEEEeqnarray}{rCl} \label{con-m} \neqsub
%	\underset{\mx,\mz}{\text{minimize}} &\quad& f(\mx,\iter{\my}{\lpoint}{i-1},\mz) \eqsub \label{con-obj-m} \\
%	\text{subject to} &\quad& h(\mz) - \hat{g}_0(\mx,\iter{\my}{\lpoint}{i-1};\iter{\mx}{k}{i}) \leq 0 \eqsub \label{con-dc-m} \\
%	&\quad& g_1(\mx,\iter{\my}{\lpoint}{i-1}) \leq 0, \quad g_2(\mx) \leq 0 \eqsub \label{con-cvx-blk-m}
%\end{IEEEeqnarray}
%The feasible set defined by the problem \eqref{con-m} is denoted by \me{\iter{\mc{X}}{k}{i} \subset \mc{F}}. To prove the convergence of the \ac{SCA} updates in the \me{\ith{i}} \ac{AO} iteration, let us consider that \eqref{con-m} yields 
%\me{\iter{\mx}{k+1}{i}} and \me{\iter{\mz}{k+1}{i}} as the solution in the \me{\ith{k}} iteration. The point \me{\iter{\mx}{k+1}{i}} and \me{\iter{\mz}{k+1}{i}}, which minimizes the objective function satisfies
%\iftoggle{single_column}{
%\begin{equation}\label{con-sub-set}\allowdisplaybreaks
%h(\iter{\mz}{k+1}{i}) - {g}_0(\iter{\mx}{k+1}{i},\iter{\my}{\lpoint}{i-1}) \leq h(\iter{\mz}{k+1}{i}) - \hat{g}_0(\iter{\mx}{k+1}{i},\iter{\my}{\lpoint}{i-1};\iter{\mx}{k}{i}) \leq 0. 
%\end{equation}}{
%\begin{multline}\label{con-sub-set}\allowdisplaybreaks
%h(\iter{\mz}{k+1}{i}) - {g}_0(\iter{\mx}{k+1}{i},\iter{\my}{\lpoint}{i-1}) \leq \\
%h(\iter{\mz}{k+1}{i}) - \hat{g}_0(\iter{\mx}{k+1}{i},\iter{\my}{\lpoint}{i-1};\iter{\mx}{k}{i}) \leq 0. 
%\end{multline}}
%Using \eqref{con-sub-set}, we can show that \me{\{\iter{\mx}{k+1}{i},\iter{\my}{\lpoint}{i-1}, \iter{\mz}{k+1}{i}\}} is feasible, since the initial \ac{SCA} operating point \me{\iter{\mx}{\lpoint}{i-1}} was chosen to be feasible from the \me{\ith{(i-1)}} \ac{AO} iteration. In each \ac{SCA} step, the feasible set includes the solution from the previous iteration as \me{\{\iter{\mx}{k+1}{i},\iter{\my}{\lpoint}{i-1},\iter{\mz}{k+1}{i}\} \in \iter{\mc{X}}{k+1}{i} \subset \mc{F}}, therefore, it decreases the objective as \cite{lanckriet2009convergence,scutari_1,amir}
%\iftoggle{single_column}{
%\begin{equation} \label{con-convergence}
%f(\iter{\mx}{0}{i},\iter{\my}{\lpoint}{i-1},\iter{\mz}{0}{i}) \geq f(\iter{\mx}{k}{i},\iter{\my}{\lpoint}{i-1},\iter{\mz}{k}{i}) \geq f(\iter{\mx}{k+1}{i},\iter{\my}{\lpoint}{i-1},\iter{\mz}{k+1}{i}) \geq f(\iter{\mx}{\lpoint}{i},\iter{\my}{\lpoint}{i-1},\iter{\mz}{\lpoint|\my}{i}). 
%\end{equation}}{
%\begin{multline} \label{con-convergence} \allowdisplaybreaks
%f(\iter{\mx}{0}{i},\iter{\my}{\lpoint}{i-1},\iter{\mz}{0}{i}) \geq f(\iter{\mx}{k}{i},\iter{\my}{\lpoint}{i-1},\iter{\mz}{k}{i}) \\ \geq f(\iter{\mx}{k+1}{i},\iter{\my}{\lpoint}{i-1},\iter{\mz}{k+1}{i}) \geq f(\iter{\mx}{\lpoint}{i},\iter{\my}{\lpoint}{i-1},\iter{\mz}{\lpoint|\my}{i}). 
%\end{multline}}
%Thus, the sequence \me{\{f(\iter{\mx}{k}{i},\iter{\my}{\lpoint}{i-1},\iter{\mz}{k}{i})\}} is nonincreasing. %\review{Since the objective is strongly convex due to the introduced quadratic term in \eqref{mod_obj}, the minimizer \eqn{\{\iter{\mx}{k}{i},\iter{\my}{\lpoint}{i-1},\iter{\mz}{k}{i}\}} in each \ac{SCA} step is unique. Now, by using the fact that the objective sequence is nonincreasing and the uniqueness of the minimizer in each convex subproblem \eqref{con-m}, we can ensure that the sequence of iterates generated by the iterative \ac{SCA} approach converges to a unique limit point in each \ac{AO} step \eqn{{i}}}. 
%However, to ensure strict monotonicity of the objective sequence, we rely on the modified objective in \eqref{con-obj-m}, which is strongly convex with some parameter \eqn{c > 0}. Now, to show strict monotonicity, let us consider \eqn{\mbf{z}_{k}^{(i)} \triangleq [\mx_{k}^{(i)},\iter{\my}{\lpoint}{i-1},\mz_{k}^{(i)}]} as the minimizer for \eqref{con-m} in the \eqn{\ith{(k-1)}} \ac{SCA} step and let \eqn{\iter{\mbf{z}}{k+1}{i}} be the minimizer in the \eqn{\ith{k}} step. At the \eqn{\ith{k}} \ac{SCA} iteration, \eqn{\forall \mbf{z} \in \mc{X}_{k}^{(i)}}, it follows
%\begin{IEEEeqnarray}{rCl} \neqsub \label{strict_monotonicity}
%\nabla f(\iter{\mbf{z}}{k+1}{i})^\tran (\mbf{z} - \iter{\mbf{z}}{k+1}{i}) &\geq& 0 \eqsub \\
%f(\mbf{z}) - f(\iter{\mbf{z}}{k+1}{i}) &\geq& c \, \|\mbf{z} - \iter{\mbf{z}}{k+1}{i}\|^2 \eqsub
%\end{IEEEeqnarray}
%since \eqn{\iter{\mbf{z}}{k+1}{i}} is the solution. Using \eqref{strict_monotonicity} and \eqn{\iter{\mbf{z}}{k}{i} \in \iter{\mc{X}}{k}{i}}, we can show that \eqn{f(\iter{\mbf{z}}{k}{i}) > f(\iter{\mbf{z}}{k+1}{i})} holds in each \ac{SCA} step unless \eqn{\mbf{z}_{k}^{(i)} \to \mbf{z}_{\lpoint}^{(i)}} as \eqn{k \to \infty}. Now using the facts that (i) \eqn{\{f(\mbf{z}^{(i)}_k)\}} is monotonic, and (ii) the uniqueness of the minimizer (see \eqref{strict_monotonicity}), strict monotonicity of \eqn{\{f(\mbf{z}^{(i)}_k)\}} is ensured \cite{scutari2010convex}. Now, by using the fact that the objective sequence is strictly nonincreasing and bounded, we can ensure the convergence of objective sequence in each \ac{AO} step \eqn{{i}} and \reviewF{\eqn{\mbf{z}_{\lpoint}^{(i)} \triangleq \{\iter{\mx}{\lpoint}{i},\iter{\my}{\lpoint}{i-1},\iter{\mz}{\lpoint|\my}{i}\}} is the solution obtained upon convergence of the objective sequence \eqn{\{f(\mbf{z}_k^{(i)})\}} in the \eqn{\ith{i}} \ac{AO} iteration.}
%
%Once \me{\iter{\mx}{\lpoint}{i}} is obtained for fixed \me{\my}, then \eqref{con} is solved for \me{\my} with fixed \me{\mx}. However, after fixing \me{\mx} as \me{\iter{\mx}{\lpoint}{i}} in \eqref{con}, the problem is still nonconvex due to \eqref{con-dc}. Following similar approach as above, the minimizer \me{ \{\iter{\mx}{\lpoint}{i},\iter{\my}{k+1}{i},\iter{\mz}{k+1}{i}\}} can be found in each \ac{SCA} step \me{k} by solving \eqref{con-m} iteratively. Note that \me{\iter{\mz}{k+1}{i}} is reused since the variable \me{\mx} is fixed in the \me{\ith{i}} \ac{AO} iteration. The convergence and the nonincreasing behavior of the objective follow similar arguments as above. \footnote{Note that the \ac{MMSE} receiver in \eqref{eqn-10} can also be used instead of performing the \ac{SCA} updates until convergence for the optimal receiver.} The solution obtained by solving \eqref{con-m} iteratively until convergence of the objective value is given as \me{\{\iter{\mx}{\lpoint}{i}, \iter{\my}{\lpoint}{i},\iter{\mz}{\lpoint|\mx}{i}\} \in \iter{\mc{Y}}{\lpoint}{i} \subset \mc{F}}. 
%
%Finally, to prove the global convergence of the \reviewF{objective sequence}, we need to show that the \ac{AO} updates also produce a nonincreasing sequence of objectives, \textit{i.e.}, 
%\iftoggle{single_column}{
%\begin{equation}
%f(\iter{\mx}{\lpoint}{i},\iter{\my}{0}{i},\iter{\mz}{0}{i}) \leq f(\iter{\mx}{\lpoint}{i},\iter{\my}{\lpoint}{i-1},\iter{\mz}{\lpoint|\my}{i}).
%\end{equation}}{
%\begin{equation}
%f(\iter{\mx}{\lpoint}{i},\iter{\my}{0}{i},\iter{\mz}{0}{i}) \leq f(\iter{\mx}{\lpoint}{i},\iter{\my}{\lpoint}{i-1},\iter{\mz}{\lpoint|\my}{i}).
%\end{equation}}
%Let \me{\iter{\mx}{\lpoint}{i}} and \me{\iter{\mz}{\lpoint|\my}{i}} be the solution obtained by solving \eqref{con-m} iteratively until \ac{SCA} convergence in the \me{\ith{i}} \ac{AO} iteration for \eqn{\mx} and \eqn{\mz} with fixed \eqn{\my = \iter{\my}{\lpoint}{i-1}}. To find \me{\iter{\my}{0}{i}}, we fix \eqn{\mx} as \me{\iter{\mx}{\lpoint}{i}} and optimize for \me{\my}. Since the convex function is linearized in \eqref{con-dc}, the fixed operating point is also included in the feasible set \eqn{\{\iter{\mx}{\lpoint}{i},\iter{\my}{\lpoint}{i-1},\iter{\mz}{\lpoint|\my}{i}\} \in \iter{\mc{Y}}{0}{i}} by the equality in \eqref{con-sub-set}. Using this, we can show the monotonicity of the objective as
%\begin{equation} \label{fixed_point}
%f(\iter{\mx}{\lpoint}{i},\iter{\my}{0}{i},\iter{\mz}{0}{i}) \leq f(\iter{\mx}{\lpoint}{i},\iter{\my}{\lpoint}{i-1},\iter{\mz}{\lpoint|\mx}{i}).
%\end{equation}
%The \ac{AO} update follows \eqn{\{\iter{\mx}{\lpoint}{i},\iter{\my}{\lpoint}{i-1},\iter{\mz}{\lpoint|\my}{i}\} \in \{ \iter{\mc{X}}{\lpoint}{i} \cap \iter{\mc{Y}}{0}{i} \}}.
%
%Using \eqref{strict_monotonicity}, strict monotonicity of the objective \eqref{mod_obj} while alternating the optimization variables from \eqn{\mx,\mz} to \eqn{\my,\mz} in the \eqn{\ith{i}} \ac{AO} update can also be ensured. It follows from that fact that \eqn{\iter{\mbf{z}}{\lpoint}{i}} is included in the feasible set \eqn{\mc{Y}_{0}^{i}}, and therefore the objective function follows \eqref{fixed_point} strictly in each \ac{AO} update.
%
%
%\begin{comment}
%\begin{equation} \label{strict_monotonicity}
%f(\iter{\mbf{z}}{k}{i}) - f(\iter{\mbf{z}}{k+1}{i}) \geq \nabla f(\iter{\mbf{z}}{k+1}{i})^\tran (\mbf{z}_{k}^{(i)} - \iter{\mbf{z}}{k+1}{i}) + \frac{m}{2} \, \|\mbf{z}_{k+1}^{(i)} - \iter{\mbf{z}}{k}{i}\|^2  \eqspace
%\end{equation}
%
%Note that the \ac{AO} and \ac{SCA} procedures generate a nonincreasing sequence of objectives for the original objective in \eqref{orig_obj} using the above discussions. However, by using the regularized objective in \eqref{mod_obj}, we can show that the sequence of objectives provided by \ac{AO} and \ac{SCA} steps is strictly decreasing. Let \eqn{\mbf{z}_{k}^{(i)} \triangleq [\mx_{k}^{(i)},\iter{\my}{\lpoint}{i-1},\mz_{k}^{(i)}]} be the solution of \eqref{con-m} in the \eqn{\ith{(k-1)}} \ac{SCA} step and let \eqn{\iter{\mbf{z}}{k+1}{i}} be the minimizer in the \eqn{\ith{k}} \ac{SCA} step. Note that the objective \eqref{con-obj-m} is strongly convex with some parameter \eqn{m > 0}, and therefore \eqn{\forall \mbf{z} \in \mc{X}_{k}^{(i)}}
%\begin{IEEEeqnarray}{l}
%f(\mbf{z}) \geq f(\iter{\mbf{z}}{k+1}{i}) + \nabla f(\iter{\mbf{z}}{k+1}{i})^\tran (\mbf{z} - \iter{\mbf{z}}{k+1}{i}) + \tfrac{m}{2} \|\mbf{z} - \iter{\mbf{z}}{k+1}{i}\|^2. \eqspace 
%\end{IEEEeqnarray}
%Since \eqn{\iter{\mbf{z}}{k+1}{i}} is the minimizer for \eqref{con-m} in the \eqn{\ith{k}} \ac{SCA} step, 
%\begin{IEEEeqnarray}{rCl} \neqsub \label{s_ineq_g}
%\nabla f(\iter{\mbf{z}}{k+1}{i})^\tran (\mbf{z}_{k}^{(i)} - \iter{\mbf{z}}{k+1}{i}) &\geq& 0, \quad \iter{\mbf{z}}{k}{i} \in \iter{\mc{X}}{k}{i} \eqsub \\
%f(\iter{\mbf{z}}{k}{i}) - f(\iter{\mbf{z}}{k+1}{i}) &\geq& \tfrac{m}{2} \, \|\mbf{z}_{k}^{(i)} - \iter{\mbf{z}}{k+1}{i}\|^2. \eqsub \label{s_ineq}
%\end{IEEEeqnarray}
%Note that \eqref{s_ineq} holds with strict inequality in each update as \eqn{f(\iter{\mbf{z}}{k}{i}) > f(\iter{\mbf{z}}{k+1}{i})}, unless \eqn{\mbf{z}_{k}^{(i)} = \mbf{z}_{k+1}^{(i)}}, which occurs at \eqn{\mbf{z}_{k}^{(i)} \to \mbf{z}_{\lpoint}^{(i)}} as \eqn{k \to \infty}. Using the facts that (i) \eqn{\{f(\mbf{z}^{(i)}_k)\}} is monotonic \cite{marks1978technical}, and (ii) the uniqueness of the minimizer (see \eqref{s_ineq_g}), strict monotonicity of \eqn{\{f(\mbf{z}^{(i)}_k)\}} is ensured. However, if \eqn{\hat{f}} is used as the objective in \eqref{con-m}, then strict monotonicity of \eqn{\{\hat{f}(\mbf{z}^{(i)}_k)\}} cannot be guaranteed due to the existence of multiple solutions.
%
%	In order to show the strict monotonicity of the sequence \eqn{\{f(\mbf{z}^{(i)}_k)\}} generated by the \ac{SCA} method, we rely on the strong convexity of \eqref{mod_obj} with parameter \eqn{m > 0} as %We know that the objective decreases in each \ac{SCA} step, since the \eqn{\ith{(k-1)}} solution \eqn{\mbf{z}^{(i)}_{k}} is a feasible point of \eqn{\mc{X}_{k}^{(i)}} in the \eqn{\ith{k}} \ac{SCA} step \cite{marks1978technical}. 
%	%It follows by the strong convexity of \eqref{mod_obj} with \eqn{m > 0} in the \eqn{\ith{k}} update
%	\begin{equation} \label{strict_monotonicity}
%	f(\iter{\mbf{z}}{k}{i}) - f(\iter{\mbf{z}}{k+1}{i}) \geq \underbrace{\nabla f(\iter{\mbf{z}}{k+1}{i})^\tran (\mbf{z}_{k}^{(i)} - \iter{\mbf{z}}{k+1}{i})}_{= 0} + \frac{m}{2} \, \|\mbf{z}_{k+1}^{(i)} - \iter{\mbf{z}}{k}{i}\|^2  \eqspace
%	\end{equation}
%	since \eqn{\iter{\mbf{z}}{k+1}{i}} is the minimizer of \eqref{con-m}. Note that \eqref{strict_monotonicity} holds with strict inequality in each update as \eqn{f(\iter{\mbf{z}}{k}{i}) > f(\iter{\mbf{z}}{k+1}{i})}, unless \eqn{\mbf{z}_{k}^{(i)} = \mbf{z}_{k+1}^{(i)}}, which occurs at \eqn{\mbf{z}_{k}^{(i)} \to \mbf{z}_{\lpoint}^{(i)}} as \eqn{k \to \infty}. Using the facts that (i) the sequence \eqn{\{f(\mbf{z}^{(i)}_k)\}} generated by the \ac{SCA} approach is monotonic \cite{marks1978technical}, and  (ii) the uniqueness of the minimizer in each \ac{SCA} step (see \eqref{strict_monotonicity}), we can ensure the strict monotonicity of \eqn{\{f(\mbf{z}^{(i)}_k)\}}. However, if \eqn{\hat{f}} is used as the objective in \eqref{con-m}, then strict monotonicity of \eqn{\{\hat{f}(\mbf{z}^{(i)}_k)\}} cannot be guaranteed due to the existence of multiple solutions.
%
%
%%Even though the original convex function \eqn{\hat{f}(\mbf{z})} can have multiple limit points upon the \ac{SCA} convergence, due to the strong convexity of \eqn{f(\mbf{z})} in \eqref{mod_obj}, the limit point is unique, and therefore the objective decreases monotonically in a strict sense. It can be proved, if \eqn{f(\mbf{z}^{(i)}_k)} is the unique minimizer in two consecutive steps even if the other limit points are included in the feasible set \eqn{\iter{\mc{X}}{k+1}{i}}, with the same \eqn{\hat{f}(\mbf{z}^{(i)}_{k})}.
%%Let \eqn{\mbf{z}^{i}_{k}} be an unique minimizer and also a limit point of the \eqn{\ith{k}} \ac{SCA} step. Let \eqn{\mbf{z}^{i}_{k+1}} be another limit point with the same objective \eqn{\hat{f}(\mbf{z}^{i}_{k}) = \hat{f}(\mbf{z}^{i}_{k+1})} in the \eqn{\ith{k+1}} feasible set \eqn{\iter{\mc{X}}{k+1}{i}}. Due to the convexity of \eqn{\hat{f}(\mbf{z})}, strict monotonicity cannot be ensured since the selection of \eqn{\mbf{z}^{i}_{k}} and \eqn{\mbf{z}^{i}_{k+1}} are equally likely. However, due to the additional quadratic term \eqn{c \, \| \mbf{z} - \mbf{z}^{(i)}_{k} \|^2_2} in \eqn{f(\mbf{z})} for the \eqn{\ith{k+1}} iteration, the update cannot choose \eqn{\mbf{z}^{i}_{k+1}} due to the positive quadratic residual, therefore, it selects the fixed point \eqn{\mbf{z}^{i}_{k}} in the \eqn{\ith{k+1}} iteration. Similarly, using the above argument, we can also prove the strict monotonicity of the objective in each \ac{AO} update \eqn{i}, and thereby proving strict monotonicity of the overall objective sequence.
%
%
%{Let \eqn{\mbf{z}_{k}^{(i)} \triangleq [\mx_{k}^{(i)},\iter{\my}{\lpoint}{i-1},\mz_{k}^{(i)}]} be the solution of \eqref{con-m} in the \eqn{\ith{k-1}} \ac{SCA} step. In order to show the strict monotonicity of the sequence \eqn{\{f(\mbf{z}^{(i)}_k)\}} generated by the \ac{SCA} method, we rely on the strong convexity of \eqref{mod_obj}. We know that the objective decreases in each \ac{SCA} step, since the \eqn{\ith{k}} solution \eqn{\mbf{z}^{(i)}_k} is a feasible point in \eqn{\mc{X}_{k+1}^{(i)}} of the \eqn{\ith{k+1}} iteration \cite{marks1978technical}. However, due to the strong convexity of \eqref{mod_obj}, it follows that \eqn{\forall \mbf{z} \in \mc{X}^{(i)}_{k}}
%	\begin{equation} \label{strict_monotonicity}
%	f(\mbf{z}) \geq f(\mbf{z}_{k}^{(i)}) + \nabla f(\mbf{z}_{k}^{(i)})^\tran (\mbf{z} - \mbf{z}_{k}^{(i)}) + c \, \|\mbf{z} - \mbf{z}_{k}^{(i)}\|^2
%	\end{equation}
%	where the equality is valid when \eqn{\mbf{z}_{k}^{(i)} = \mbf{z}_{k+1}^{(i)}}, \textit{i.e.}, as \eqn{k\to\infty}, \eqn{\mbf{z}_{k}^{(i)} \to \mbf{z}_{\lpoint}^{(i)}}. Using the facts that (i) the sequence \eqn{\{f(\mbf{z}^{(i)}_k)\}} generated by the \ac{SCA} approach is monotonic \cite{marks1978technical}, and  (ii) the uniqueness of the minimizer in each \ac{SCA} step (see \eqref{strict_monotonicity}), we can ensure the strict monotonicity of \eqn{\{f(\mbf{z}^{(i)}_k)\}}. However, if \eqn{\hat{f}} is used in the objective of \eqref{con-m}, then the limit point is not unique, \textit{i.e.}, \eqn{\{\exists \mbf{z} \in \mc{X}_{\lpoint}^{(i)} \vert \hat{f}(\mbf{z}) = \hat{f}(\mbf{z}_{\lpoint}^{(i)})\}}, and therefore the sequence generated by \eqref{con-m} with \eqn{\hat{f}} is not strictly monotonic.
%	
%	Similarly, we can ensure strict monotonicity of the objective \eqref{mod_obj} while alternating the optimization variables from \eqn{\mx,\mz} to \eqn{\my,\mz} in the \eqn{\ith{i}} \ac{AO} iteration. Since the limit point \eqn{\{\iter{\mx}{\lpoint}{i},\iter{\my}{\lpoint}{i-1},\iter{\mz}{\lpoint|\mx}{i}\}} is also included in the feasible set \eqn{\mc{Y}_{0}^{i}}, the objective function follows \eqref{fixed_point} in strict sense unless the overall sequence \eqn{\{\mbf{z}_{\lpoint}^{i} \}} converges to a limit point as \eqn{i \to \infty}.}
%
%
%%Due to the convexity in the original objective \eqn{\hat{f}(\mbf{z})}, it can have multiple limit points for the \ac{SCA} update, and therefore it follows \eqn{f(\mbf{z}^{(i)}_{k+1}) \leq f(\mbf{z}^{(i)}_k), \forall k}. However, due to the strong convexity of \eqn{f(\mbf{z})} in \eqref{mod_obj}, the limit point of the \ac{SCA} updates is also unique, and therefore has strict monotonicity in \eqn{f(\mbf{z}^{(i)}_k)} after each update. It can be justified, if \eqn{f(\mbf{z}^{(i)}_k)} is the unique minimizer in two consecutive steps even by including other limit points in the feasible set \eqn{\iter{\mc{X}}{k+1}{i}} with the same original objective value as \eqn{\hat{f}(\mbf{z}^{(i)}_{k})}.
%%In spite of choosing \eqn{\mbf{z}_{k}^{(i)}} in both \eqn{k} and \eqn{k+1} iteration, \eqn{f(\mbf{z})} decreases strictly due to the vanishing quadratic term in the \eqn{\ith{k+1}} step, therefore, it is a fixed point. Similarly, by using the above argument, we can also prove the strict monotonicity of the objective in each \ac{AO} update, and thereby proving strict monotonicity of the overall sequence.
%%In spite of the additional quadratic term \eqn{\| \mbf{z} - \mbf{z}^{(i)}_{k} \|^2_2} to ensure unique minimizer, strict monotonicity of the objective in each \ac{SCA} update cannot be ensured. However, strict monotonicity can be guaranteed in each update \eqn{i} of the algorithm \me{\mc{A}} in \eqref{g-algo} for the objective. Using (a),(b),(c) along with the above discussion, we can ensure strict monotonicity and the convergence of the objective.}
%%Due to the additional quadratic term \eqn{\| \mbf{z} - \mbf{z}^{(i)}_{k} \|^2_2} from the earlier iteration and the unique minimizer from the previous step is also included, the objective decreases strictly in each \ac{SCA} update for an arbitrary value of \eqn{c} in \eqref{mod_obj}. Therefore, in each update \eqn{i} of the algorithm \me{\mc{A}} in \ref{g-algo}, the objective decreases monotonically in strict sense. Using (a),(b),(c) along with the above discussion, we can ensure strict monotonicity and the convergence of the objective.}
%\end{comment}
%
%\subsection{Stationarity of Limit Points} \label{c-b}
%
%In this section, we discuss the convergence of the sequence of iterates generated by the iterative algorithm. In order to do so, let us use single global index \eqn{t} to represent both \ac{SCA} indexing \eqn{k} and \ac{AO} index \eqn{i}. Using this, we can represent \eqn{\ma^{t} \triangleq [\iter{\mx}{k}{i},\iter{\my}{\lpoint}{i-1},\iter{\mz}{k|\my}{i}]} as a stacked vector of solution obtained in the \eqn{\ith{k}} \ac{SCA} step of the \eqn{\ith{i}} \ac{AO} update with fixed \eqn{\my}. The index \eqn{t} is used to represent both \ac{SCA} updates involving transmit and receive beamformers within each \ac{AO} step. 
%
%%Additionally, we can also ensure the convergence of \eqn{\{\hat{f}(\ma^t)\}}, since the regularization term in \eqref{mod_obj} vanishes as \eqn{t \to \infty}, \eqn{\|\ma - \ma^t\|^2 \to 0}.
%Using the discussions in Appendices \ref{b_obj} and \ref{mcity}, we can show that the sequence of objective values \eqn{\{f(\ma^t)\}} converges.  Note that similar claim cannot be made on the convergence of \eqn{\{\ma^t\}}, since the sequence need not be monotonic. However, due to the compactness of the feasible set, the sequence of iterates \eqn{\{\ma^t\}} is bounded. Therefore, there exists at least one subsequence that converges to a limit point in the feasible set \cite[Th. 3.6]{rudin1964principles}. Since there can be possibly two subsequences of \eqn{\{\ma^t\}} that converges to two different limit points, the sequence of iterates \eqn{\{\ma^t\}} need not be convergent sequence.
%
%Therefore, we can only claim that every limit point of the sequence \eqn{\{\ma^t\}} is a stationary point, \textit{i.e.,} the limit point of every convergent subsequence is a stationary point. To prove that, let us consider a subsequence \eqn{\{\ma^{t_j} \, | \, j = 0,1,\dotsc\}} of \eqn{\{\ma^t\}} that converges to a limit point \eqn{\bar{\ma}}, where \eqn{\bar{\ma} = \lim_{j \to \infty} \ma^{t_j}}. It follows from \eqref{con-convergence}, \eqref{strict_monotonicity}, and \eqref{fixed_point} that
%\begin{equation} \label{seq_mon}
%f(\ma^{t_{j+1}}) < f(\ma^{t_{j}}) < f(\ma^{t_{j-1}}).
%\end{equation}
%Eq. \eqref{seq_mon} holds strictly while alternating the optimization variables as shown in \eqref{fixed_point}. Now, by taking the limit as \eqn{j \to \infty} and by using the continuity of objective function \eqn{f}, we obtain
%\begin{equation} \label{limpoint}
%\lim_{j \to \infty} f(\ma^{t_j}) = f(\lim_{j \to \infty} \ma^{t_{j}}) = f(\bar{\ma}).
%\end{equation}
%Using strict monotonicity in \eqn{\{f(\ma^{t_j})\}} and the fact that as \eqn{j \to \infty}, \eqn{|f(\ma^{t_{j}}) - f(\ma^{t_{j+1}})| \to 0}, we can show that limit point \eqn{\bar{\ma}} satisfies the optimality condition \cite[Prop. 2.1.2]{bertsekas1999nonlinear} as 
%\begin{equation} \label{seq_mon1}
%\nabla f (\bar{\ma}) (\ma - \bar{\ma}) \geq 0, \; \fall \ma \in \mc{Q} \subset \mc{F} \eqsub
%\end{equation}
%where \eqn{\mc{Q}} is a convex subset defined by \eqref{con-dc-m} and \eqref{con-cvx-blk-m} as \eqn{k \to \infty, i \to \infty}. Hence, the limit point \eqn{\bar{\ma}} is a stationary point of \eqref{con-m} as \eqn{k \to \infty, i \to \infty}.
%
%%The above discussion also considers \ac{AO} step, since in between each \ac{AO} update, the objective sequence is strictly monotonic \eqref{fixed_point} and it follows \cite[Prop. 2.7.1]{bertsekas1999nonlinear}. 
%
%Now to show that \eqn{\bar{\ma}} is a stationary point of original nonconvex problem \eqref{con}, we show the equivalence between the gradient of the subproblem \eqref{con-m} and \eqref{con} at stationary point \eqn{\bar{\ma}}. In order to show that \eqn{\bar{\ma}} is a \ac{KKT} point for \eqref{con}, it must satisfy the gradient condition of \eqref{con}, \textit{i.e.},
%\begin{equation} \label{kkt_expr}
%	\textstyle	\nabla \hat{f}(\bar{\ma}) + \mu_0 \big [ \nabla h(\bar{\ma}) - \nabla {g}_0(\bar{\ma}) \big ] + \sum_{i=1}^2 \mu_i \nabla g_i(\bar{\ma}) = 0
%\end{equation}
%for some Lagrange multipliers \eqn{\mu_i \geq 0} and feasible as \eqn{\bar{\ma} \in \mc{F}}.
%
%To prove the feasibility of the limit point \me{\bar{\ma}}, note that in each \ac{SCA} step, the feasible sets \eqn{\mc{X}_{k}^{(i)},\mc{Y}_{k}^{(i)} \subset \mc{F}}. Therefore, the limit point \eqn{\bar{\ma} \in \mc{F}}. Since \eqn{\bar{\ma}} satisfies Slater's constraint qualifications, there exist Lagrange multipliers \eqn{\mu_i} such that
%\begin{equation} \label{kkt_mod_expr}
%	\textstyle \nabla f(\bar{\ma}) + \mu_0 \big [ \nabla h(\bar{\ma}) - \nabla\hat{g}_0(\bar{\ma};\bar{\ma}) \big ] + \sum_{i=1}^2 \mu_i \nabla g_i(\bar{\ma}) = 0.
%\end{equation}
%The relation between \eqref{kkt_expr} and \eqref{kkt_mod_expr} is evident by the following facts: (i) The quadratic term \eqn{\|\ma - \ma^{t_j}\|^2} in \eqref{mod_obj} vanishes as \eqn{j \to \infty, \ma^{t_j} \to \bar{\ma}}, since we assume that \eqn{\bar{\ma}} is the limit point of convergent subsequence \eqn{\{\ma^{t_j}\}}. Therefore, the gradient evaluated at \eqn{\bar{\ma}} satisfies \eqn{\nabla {f}(\bar{\ma}) = \nabla \hat{f}(\bar{\ma})}. (ii) Additionally, by using the relation in \eqref{con-relax}, the gradient of the constraint \eqref{con-dc} evaluated at \eqn{\bar{\ma}} is given as
%\begin{equation} \label{rhs_equiv}
%	\nabla h(\bar{\ma}) - \nabla \hat{g}_o(\bar{\ma};\bar{\ma}) = \nabla h(\bar{\ma}) - \nabla g_o(\bar{\ma}).
%\end{equation}
%Now, by applying the relation \eqref{rhs_equiv} and \eqn{\nabla {f}(\bar{\ma}) = \nabla \hat{f}(\bar{\ma})} in \eqref{kkt_mod_expr}, we can show that the limit point \eqn{\bar{\ma}} satisfies \eqref{kkt_expr}. By using \cite[Thms. 2 and 11]{scutari_1} and \cite[Prop. 3.2]{amir}, we can show that every limit point of the sequence of iterates \eqn{\{\ma^t\}} generated by the algorithm is a stationary point of problem \eqref{con}.}
%
%\begin{comment}
%However, it is not the case with the sequence of iterates \eqn{\{\iter{\ma}{k}{i}\}}, since there can be two different convergent subsequences that has the, say, \eqn{\{\iter{\ma}{k}{i}\}_{k,i \in \mc{Q}_1}} and \eqn{\{\iter{\ma}{k}{i}\}_{k,i \in \mc{Q}_2}} such that
%
%Let \me{\ma^{(i)} \triangleq [\iter{\mx}{\lpoint}{i},\iter{\my}{\lpoint}{i},\iter{\mz}{\lpoint|\mx}{i}]} be the stacked vector of the solution point from the \eqn{\ith{i}} \ac{AO} iteration and let \me{\mc{A}} be a point-to-set mapping algorithm defined as
%\begin{equation} \label{g-algo}
%\iterate{\ma}{i+1} \in \mc{A}(\iterate{\ma}{i}) \triangleq
%\begin{cases}
%\underset{\mx,\mz}{\mathrm{argmin}} \; f(\mx,\mz;\iter{\my}{\lpoint}{i-1})& \forall \mx,\mz \in \iter{\mc{X}}{\lpoint}{i} \\
%\underset{\my,\mz}{\mathrm{argmin}} \; f(\my,\mz;\iter{\mx}{\lpoint}{i})& \forall \my,\mz \in \iter{\mc{Y}}{\lpoint}{i} 
%\end{cases} 
%\end{equation}
%Note that in each iteration \me{i}, the algorithm \eqn{\mc{A}} in \eqref{g-algo} includes two \ac{SCA} updates performed until convergence, \textit{i.e.}, one for variable \me{\mx} and \me{\mz} by keeping \me{\my} fixed and another for \me{\my} and \me{\mz} by fixing \me{\mx} as constant. Since the minimizer is unique, the mapping can be defined as \me{\iterate{\ma}{i+1} = \mc{A}(\iterate{\ma}{i})}. In order to prove the convergence of the beamformer iterates, we define \me{\mc{F}^\lpoint \subset \mc{F}} as the set of fixed points, \textit{i.e.}, \me{\forall {\ma}^{\lpoint} \in \mc{F}^\lpoint, \, {\ma}^{\lpoint} = \mc{A}({\ma}^{\lpoint})}, identified by the algorithm \me{\mc{A}} for different initialization points. 
%
%Using \cite[Th. 3.1]{meyer1976sufficient}, convergence of the iterates to a fixed point can be shown, if the following conditions are satisfied.
%\begin{itemize}
%\item Objective function \me{f} should be bounded and continuous.
%\item Feasible set \me{\iter{\mc{X}}{\lpoint}{i},\iter{\mc{Y}}{\lpoint}{i}} in each step should be compact.
%\item The sequence \me{\{\iterate{\ma}{i}\}} generated by \me{\mc{A}} should be strictly monotonic with respect to the objective \me{f}, \textit{i.e.}, \me{\ma^\prime = \mc{A}(\ma)} implies \me{f(\ma^\prime) < f(\ma)} whenever \me{\ma} is not a fixed point.
%\end{itemize}
%\review{Note that the strict monotonicity of the objective is guaranteed using the strong convexity of \eqn{f} in Section \ref{mcity}. Therefore, all the above requirements are satisfied by algorithm \eqref{g-algo}}. Using \cite{zangwill1969nonlinear} and \cite[Th. 3.1]{meyer1976sufficient}, we can show that (i) all limit points will be a fixed point, (ii) \eqn{f(\iterate{\ma}{i}) \to f(\ma^\lpoint)}, where \eqn{\ma^\lpoint} is a fixed point, and (iii) \eqn{\ma^\lpoint} is a regular point, \textit{i.e.}, \eqn{\|\iterate{\ma}{i} - \mc{A}(\iterate{\ma}{i}) \| \to 0}. Even though the fixed points in the set \me{\mc{F}^\lpoint} can be achieved by the algorithm \me{\mc{A}} for different initial points, the algorithm \me{\mc{A}} achieves a unique limit point, \textit{i.e.}, \me{\mc{F}^\lpoint = \{\ma^\lpoint\}}, once a feasible point is chosen to be the operating point while initializing \me{\mc{A}}.
%
%\begin{comment}
%	Note that the above conditions are satisfied by the algorithm \me{\mc{A}} in \eqref{g-algo} and the strict monotonicity of the objective sequence is guaranteed in Section \ref{mcity} using strong convexity of \eqn{f}}.
%and \me{\mc{A}} be a point-to-set mapping algorithm from \me{\mc{F}} into the nonempty subsets of \me{\mc{F}}, \textit{i.e.}, \eqn{\iterate{\ma}{i+1} \in \mc{A}(\iterate{\ma}{i})}. The set of accumulation or the limit points in the compact set \me{\mc{F}} be denoted by \me{\mc{F}^\lpoint} and the objective function be \me{f} is closed and continuous. Let \me{\{\iterate{\ma}{i}\}} be the sequence of iterates generated by the algorithm \me{\mc{A}} and the objective function \me{f}. If the mapping is strictly monotonic and uniformly compact, then the following conditions hold \cite{zangwill1969nonlinear} and \cite[Theorem 3.1]{meyer1976sufficient}.
%\begin{itemize}
%\item[(i)] all accumulation points will be a fixed point,
%\item[(ii)] \eqn{f(\iterate{\ma}{i}) \to f(\ma^\lpoint)}, where \eqn{\ma^\lpoint} is a fixed point,
%\item[(iii)] \eqn{\ma^\lpoint} is a regular point, \textit{i.e.} \eqn{\|\iterate{\ma}{i} - \iterate{\ma}{i+1} \| \to 0}.
%\end{itemize}
%
%To show that the proposed iterative method satisfies the above conditions,  the mapping \eqn{\mc{A}} for the iterative algorithm is given by
%\begin{equation}
%\iterate{\ma}{i+1} \in \mc{A}(\iterate{\ma}{i}) : \underset{\ma}{\mathrm{argmin}} \; f(\ma;\iterate{\ma}{i}) \; \forall \ma \in \iterate{\mc{X}}{i} \subset \mc{F}
%\end{equation}
%where \me{\mc{A}(\iterate{\ma}{i})} denotes the set of iterates in the subset \me{\iterate{\mc{X}}{i}}, which has the same objective value under the mapping \me{f} in the \me{\ith{i}} iteration The monotonicity of the proposed algorithm is guaranteed based on the arguments presented in Appendix \ref{mcity}. The mapping is strictly monotonic on \me{\mc{F}} with respect to the objective function as \me{\ma^\prime \in \mc{A}(\ma)} implies \me{f(\ma^\prime) < f(\ma)} whenever \me{\ma} is not a fixed point, \textit{i.e.}, \me{\ma \notin \mc{F}^\lpoint}, as in \eqref{fixed_point}. Using the above discussions, we can show that the sequence \me{\{\iterate{\ma}{i}\}} converges to a fixed point in the set \me{\mc{F}^\lpoint} for any feasible starting point \me{\iterate{\ma}{0} \in \mc{F}}
%
%The sequence \me{\{\iterate{\ma}{i}\}} converges to the set of fixed points \me{\mc{F}^\lpoint} for any feasible starting point \me{\iterate{\ma}{0} \in \mc{F}}, therefore the mapping is uniformly compact.
%
%To show the set of fixed points \me{\mc{F}^\lpoint} are the minimizers for the objective function \me{f}, we rely on the strict monotonicity of the objective function for each iteration. Since \me{f(\mc{A}(\iterate{\ma}{i})) < f(\iterate{\ma}{i})}, the objective decreases monotonically for each iteration and the equality is achieved when \me{\ma^\lpoint \in \mc{A}(\ma^\lpoint)}, which is a generalized fixed point. Therefore, the fixed points in \me{\mc{F}^\lpoint} are the minimizers for the objective function \me{f} over the set \me{\mc{F}}.
%
%If the uniqueness of the iterates are guaranteed, then the algorithm will find a unique solution at each iteration as \me{\iterate{\ma}{i+1} = \mc{A}(\iterate{\ma}{i})} and the accumulation point is unique. The convergence of the iterates to a single fixed point is guaranteed by the discussions in \cite{zangwill1969nonlinear,meyer1976sufficient}. Even though the algorithm finds a single fixed point, all fixed points in \me{\mc{F}^\lpoint} are equally valid as a minimizer for the function \me{f} in \eqref{con}, since the \ac{SINR} in \eqref{eq:SINR} is invariant to the unitary rotations on the beamformers.
%\end{comment}
%
%\begin{comment}
%%\subsection{Stationary Points of the Nonconvex Problem}
%\review{Upon satisfying the conditions (a)-(d), we have shown that the sequence \eqn{\{\iterate{\ma}{i}\}} generated by \eqn{\mc{A}} converges to a limit point \eqn{\ma^{\lpoint}} as \eqn{i \to \infty}. Now, to show that \eqn{\ma^{\lpoint}} is a stationary point of \eqref{con}, it must satisfy the \ac{KKT} gradient condition of \eqref{con}, \textit{i.e.},
%\begin{equation} \label{kkt_expr}
%\textstyle	\nabla \hat{f}(\ma^{\lpoint}) + \mu_0 \big [ \nabla h(\ma^{\lpoint}) - \nabla {g}_0(\ma^{\lpoint}) \big ] + \sum_{i=1}^2 \mu_i \nabla g_i(\ma^{\lpoint}) = 0
%\end{equation}
%for some Lagrange multipliers \eqn{\mu_i \geq 0} and feasible as \eqn{\ma^{\lpoint} \in \mc{F}}.
%
%To prove the feasibility of the limit point \me{\ma^{\lpoint}}, note that in each \ac{SCA} step, the feasible sets \eqn{\mc{X}_{k}^{(i)},\mc{Y}_{k}^{(i)} \subset \mc{F}}. Therefore, the limit point \eqn{\ma^{\lpoint} \in \mc{F}} and it satisfies the Slater's constraint qualifications. Moreover, as discussed in Appendix \ref{c-b}, \eqn{\ma^{\lpoint}} is a regular point of the algorithm \eqn{\mc{A}} and also the solution for \eqref{con-m}. Therefore, there exist Lagrange multipliers \eqn{\mu_i} such that
%\begin{equation} \label{kkt_mod_expr}
%\textstyle \nabla f(\ma^{\lpoint}) + \mu_0 \big [ \nabla h(\ma^{\lpoint}) - \nabla\hat{g}_0(\ma^{\lpoint};\ma^{\lpoint}) \big ] + \sum_{i=1}^2 \mu_i \nabla g_i(\ma^{\lpoint}) = 0.
%\end{equation}
%The relation between \eqref{kkt_mod_expr} and \eqref{kkt_expr} is evident by the following facts: (i) The quadratic term \eqn{\|\ma - \ma^{(i)}\|^2} in \eqref{mod_obj} vanishes as \eqn{i \to \infty, \ma^{(i)} \to \ma^{\lpoint}}, since \eqn{\ma^{\lpoint}} is the fixed point of \eqn{\mc{A}}. Therefore, the gradient evaluated at \eqn{\ma^{\lpoint}} satisfy \eqn{\nabla {f}(\ma^{\lpoint}) = \nabla \hat{f}(\ma^{\lpoint})}. (ii) Additionally, by using the relation in \eqref{con-relax}, the gradient of the constraint \eqref{con-dc} evaluated at \eqn{\ma^{\lpoint}} is given as
%\begin{equation} \label{rhs_equiv}
%\nabla h(\ma^{\lpoint}) - \nabla \hat{g}_o(\mbf{\ma}^{\lpoint};\mbf{\ma}^{\lpoint}) = \nabla h(\ma^{\lpoint}) - \nabla g_o(\mbf{\ma}^{\lpoint}).
%\end{equation}
%Now, by applying the relation in \eqref{rhs_equiv} and \eqn{\nabla {f}(\ma^{\lpoint}) = \nabla \hat{f}(\ma^{\lpoint})} in \eqref{kkt_mod_expr}, we can show that the limit point \eqn{\ma^{\lpoint}} satisfies \eqref{kkt_expr}. By using \cite[Thms. 2 and 11]{scutari_1} or \cite[Prop. 3.2]{amir}, we can show that the limit point of the sequence of iterates \eqn{\{\iterate{\ma}{i}\}} generated by the algorithm \me{\mc{A}} is a stationary point of the problem in \eqref{con}.}
%\end{comment}
%\begin{comment}
%In this section, we show that the limit point \eqn{\ma^{\lpoint}} of the sequence \eqn{\{\iterate{\ma}{i}\}}, obtained from \eqn{\mc{A}} as \eqn{i \to \infty} is a stationary point of the original nonconvex problem in \eqref{con}. In order for \eqn{\ma^{\lpoint}} to be a stationary point, it must satisfy the \ac{KKT} conditions of \eqref{con} and the Slater's constraint qualifications. Note that all points satisfies \eqn{\iterate{\ma}{i} \in \mc{Y}_\lpoint^{(i)} \subset \mc{F}}, and therefore the limit point \eqn{\ma^{\lpoint}} satisfies the constraint qualifications of the problem \eqref{con}.
%
%To show that the limit point \eqn{\mbf{\ma}^{\lpoint}} satisfies the \ac{KKT} expression of the nonconvex problem, we utilize the fact that the limit point \eqn{\mbf{\ma}^{\lpoint}} is the solution of the convex subproblem \eqref{con-m} upon convergence. Therefore, it satisfies the \ac{KKT} conditions of the convex subproblem for some Lagrange multipliers \eqn{\mu_i \geq 0} as 
%\begin{equation} \label{kkt_expr}
%\textstyle	\nabla f(\ma^{\lpoint}) + \mu_0 \big [ \nabla h(\ma^{\lpoint}) - \nabla\hat{g}_0(\ma^{\lpoint};\ma^{\lpoint}) \big ] + \sum_{i=1}^2 \mu_i \nabla g_i(\ma^{\lpoint}) = 0.
%\end{equation}
%It is evident that the modified objective \eqn{f} and the convex function \eqn{\hat{f}} in \eqref{mod_obj} are equal at the limit point due to the vanishing quadratic penalty term. Now, by utilizing the relation \eqn{\nabla \hat{g}_o(\mbf{\ma}^{\lpoint};\mbf{\ma}^{\lpoint}) = \nabla g_o(\mbf{\ma}^{\lpoint})} between the \ac{DC} constraint \eqref{con-dc} and the corresponding convex approximation in \eqref{con-relax}, we can ensure that the limit point \eqn{\mbf{\ma}^{\lpoint}} indeed satisfies the \ac{KKT} condition of the nonconvex problem \eqref{con}. Now, by using \cite[Th. 2 and 11]{scutari_1} or \cite[Prop. 3.2]{amir}, we can show that all the limit points of the sequence \eqn{\{\iterate{\ma}{i}\}} generated by the algorithm \me{\mc{A}} are the \ac{KKT} points of \eqref{con}.
%
%\review{Let \eqn{\ma^{\lpoint}} be the limit point of the sequence of iterates \eqn{\{\iterate{\ma}{i}\}} generated by the algorithm \eqn{\mc{A}} as \eqn{i \to \infty}. The limit point \eqn{\ma^\lpoint} is a stationary point of \eqref{con} if it satisfies the \ac{KKT} conditions. 
%%and there exists no descent direction in the feasible set as
%%\begin{equation}
%%\nabla {f}(\ma^{\lpoint})^\tran \, (\ma - \ma^{\lpoint}) \geq 0, \: \forall \, \ma \in \mc{Y}^{\lpoint}
%%\end{equation}
%%where \eqn{\mc{Y}^{\lpoint}} is the feasible set of \eqref{con-m} upon convergence, \textit{i.e.}, \eqn{\mc{Y}_{\lpoint}^{i}} as \eqn{i \to \infty}. 
%Let us consider the problem in \eqref{con-m}. Using \eqref{con-relax}, we can show that the feasible set \eqn{\mc{Y}_{\lpoint}^{i}} in each \ac{AO} iteration \eqn{i} is a subset of \eqn{\mc{F}}. Therefore, all iterates \eqn{\{\iterate{\ma}{i}\}} of the algorithm \eqn{\mc{A}} are feasible. Additionally, by the strong convexity of \eqn{f(\ma)}, following conditions are satisfied.
%\begin{IEEEeqnarray}{rCl}  \neqsub \label{strong_cvxty}
%\nabla {f}(\ma^{(i)})^\tran \, (\ma^{(i-1)} - \ma^{(i)}) &\geq& 0 \eqsub \\
%f(\ma^{(i-1)}) - f(\ma^{(i)}) &\geq& c \: \|\ma^{(i-1)} - \ma^{(i)}\|^2. \eqsub
%\end{IEEEeqnarray}
%Using (a),(b),(c) and (d), we can show that \eqn{\|\ma^{(i)} - \mc{A}(\ma^{(i)})\| \to 0} as \eqn{i \to \infty}, which is the limit point of the algorithm \eqn{\mc{A}}.
%
%Now, it remains to show that the \eqn{\ma^{\lpoint}} is a stationary point of the nonconvex problem \eqref{con}. To see this, we note that \eqn{\ma^{\lpoint}} must satisfy the \ac{KKT} conditions of \eqref{con}. Since \eqn{\ma^{\lpoint}} is the fixed point of the algorithm \eqn{\mc{A}}, it satisfies the \ac{KKT} conditions of the convex subproblem \eqref{con-m} given by
%\begin{equation} \label{kkt_expr}
%\textstyle	\nabla f(\ma^{\lpoint}) + \mu_0 \big [ \nabla h(\ma^{\lpoint}) - \nabla\hat{g}_0(\ma^{\lpoint}) \big ] + \sum_{i=1}^2 \mu_i \nabla g_i(\ma^{\lpoint}) = 0
%\end{equation}
%for some Lagrange multipliers \eqn{\mu_i \geq 0}. Using the equivalence between \eqn{\hat{f}(\ma^{\lpoint}) = f(\ma^{\lpoint})} and the relation in \eqref{con-relax} in \eqref{kkt_expr}, we can see that \eqref{kkt_expr} is indeed the \ac{KKT} condition for the nonconvex problem \eqref{con}. Since the gradient is zero in \eqref{kkt_expr}, the objective cannot be decreased any further. Now, by using \cite[Th. 2 and 11]{scutari_1} or \cite[Prop. 3.2]{amir}, we can show that the limit points \eqn{\ma^\lpoint} of \me{\mc{A}} are the \ac{KKT} points of \eqref{con}.}
%
%%In order for \eqn{\ma^{\lpoint}} to be stationary point, it must satisfy the \ac{KKT} conditions of \eqref{con}. In each iteration, the algorithm \eqn{\mc{A}} in \eqref{g-algo} finds a point \eqn{\iterate{\ma}{i} \in \mc{F}} with \eqn{f(\iterate{\ma}{i}) < f(\iterate{\ma}{i-1})}, therefore, it satisfies all the constraints of \eqref{con}. Since the feasible set is compact and strict monotonicity is ensured, as \eqn{i \to \infty}, the objective no longer decreases as the algorithm \eqn{\mc{A}(\ma^{\lpoint}) = \ma^{\lpoint}} finds a unique \eqn{\ma^{\lpoint} \in \mc{F}^{\lpoint}} due to the strong convexity of \eqn{f}.
%
%%To show the equivalence in the gradients of \eqref{con} and \eqref{con-m} at \eqn{\ma^{\lpoint}}, let us consider the limit point of the \ac{SCA} update in \eqn{\mc{A}} as it converges to a fixed point \eqn{\ma^{\lpoint} \in \mc{F}^{\lpoint}}. The \ac{KKT} expression for the convex subproblem \eqref{con-cvx} is given by
%%\begin{equation} \label{kkt_expr}
%%\nabla f(\ma^{\lpoint}) + \mu_1 \big [ \nabla h(\ma^{\lpoint}) - \nabla\hat{g}_0(\ma^{\lpoint}) \big ] + \sum_{i=1}^2 \mu_i \nabla g_i(\ma^{\lpoint}) = 0
%%\end{equation}
%%for some Lagrange multipliers \eqn{\mu_i \geq 0}. Using \eqref{con-relax} in \eqref{kkt_expr}, we can ensure the equivalence of the gradient between \eqref{con} and \eqref{con-m}. Moreover, \eqn{\ma^{\lpoint}} is a regular point \textit{i.e.} \eqn{\|\iterate{\ma}{i} - \iterate{\ma}{i+1} \| \to 0} as \eqn{i \to 0}. Now, with the above conditions and by using \cite[Th. 2 and 11]{scutari_1} and \cite[Prop. 3.2]{amir}, we can show that the fixed points obtained by \me{\mc{A}} under the mapping \me{f} is a stationary point of the nonconvex problem \eqref{con}.
%
%\begin{comment}
%To show the limit point of the sequence of iterates \me{\{\iterate{\ma}{i}\}} is a stationary point, it must satisfy the \ac{KKT} conditions of the problem in \eqref{con}. As \me{i \to \infty}, the objective converges as
%\iftoggle{single_column}{
%\begin{equation} \label{con-opt}\allowdisplaybreaks
%f(\iter{\mx}{\lpoint}{i},\iter{\my}{\lpoint}{i},\iter{\mz}{\lpoint|x}{i}) = f(\iter{\mx}{\lpoint}{i+1},\iter{\my}{\lpoint}{i},\iter{\mz}{\lpoint|y}{i+1}) = 
%f(\iter{\mx}{\lpoint}{i+1},\iter{\my}{\lpoint}{i+1},\iter{\mz}{\lpoint|x}{i+1})
%\end{equation}}{
%\begin{multline} \label{con-opt}\allowdisplaybreaks
%f(\iter{\mx}{\lpoint}{i},\iter{\my}{\lpoint}{i},\iter{\mz}{\lpoint|x}{i}) = f(\iter{\mx}{\lpoint}{i+1},\iter{\my}{\lpoint}{i},\iter{\mz}{\lpoint|y}{i+1}) \\ = 
%f(\iter{\mx}{\lpoint}{i+1},\iter{\my}{\lpoint}{i+1},\iter{\mz}{\lpoint|x}{i+1})
%\end{multline}}
%where \me{\{\iter{\mx}{\lpoint}{i+1},\iter{\my}{\lpoint}{i+1},\iter{\mz}{\lpoint|x}{i+1}\} \in \mc{F}^\lpoint} is a limit point of \me{\mc{A}} for a given initial point. Since the function \eqn{f} is strongly convex, the limit point is unique. As discussed in Appendix \ref{c-b}, the unique limit point of the sequence of iterates \me{\{\iterate{\ma}{i}\}} is a fixed point of \me{\mc{A}}. 
%
%Using \cite[Th. 10]{lanckriet2009convergence} and \cite[Th. 2 and 11]{scutari_1}, we can show that the fixed points obtained by the algorithm \me{\mc{A}} under the mapping \me{f} is a stationary point of the nonconvex problem \eqref{con}. 
%
%In this section, we show that the limit point \eqn{\ma^{\lpoint}} of the sequence \eqn{\{\iterate{\ma}{i}\}}, obtained from \eqn{\mc{A}} as \eqn{i \to \infty} is a stationary point of the original nonconvex problem in \eqref{con}. In order for \eqn{\ma^{\lpoint}} to be a stationary point, it must satisfy the \ac{KKT} conditions of \eqref{con} and the Slater's constraint qualifications. Note that all points satisfies \eqn{\iterate{\ma}{i} \in \mc{Y}_\lpoint^{(i)} \subset \mc{F}}, and therefore the limit point \eqn{\ma^{\lpoint}} satisfies the constraint qualifications of the problem \eqref{con}.
% 
%To show that the limit point \eqn{\mbf{\ma}^{\lpoint}} satisfies the \ac{KKT} expression of the nonconvex problem, we utilize the fact that the limit point \eqn{\mbf{\ma}^{\lpoint}} is the solution of the convex subproblem \eqref{con-m} upon convergence. Therefore, it satisfies the \ac{KKT} conditions of the convex subproblem for some Lagrange multipliers \eqn{\mu_i \geq 0} as 
% \begin{equation} \label{kkt_expr}
% \textstyle	\nabla f(\ma^{\lpoint}) + \mu_0 \big [ \nabla h(\ma^{\lpoint}) - \nabla\hat{g}_0(\ma^{\lpoint};\ma^{\lpoint}) \big ] + \sum_{i=1}^2 \mu_i \nabla g_i(\ma^{\lpoint}) = 0.
% \end{equation}
% It is evident that the modified objective \eqn{f} and the convex function \eqn{\hat{f}} in \eqref{mod_obj} are equal at the limit point due to the vanishing quadratic penalty term. Now, by utilizing the relation \eqn{\nabla \hat{g}_o(\mbf{\ma}^{\lpoint};\iterate{\ma}{\lpoint}) = \nabla g_o(\mbf{\ma}^{\lpoint})} between the \ac{DC} constraint \eqref{con-dc} and the corresponding convex approximation in \eqref{con-relax}, we can ensure that the limit point \eqn{\mbf{\ma}^{\lpoint}} indeed satisfies the \ac{KKT} condition of the nonconvex problem \eqref{con}. Now, by using \cite[Th. 2 and 11]{scutari_1} or \cite[Prop. 3.2]{amir}, we can show that all the limit points of the sequence \eqn{\{\iterate{\ma}{i}\}} generated by the algorithm \me{\mc{A}} are the \ac{KKT} points of \eqref{con}.
%\end{comment}


\begin{comment}
The following conditions are required to show the convergence of Algorithm \ref{algo-1}, which is used to solve problems \eqref{eqn-6} and \eqref{eqn-mse-1} in an iterative manner.
\begin{itemize}
\item[(a)] Objective function should be bounded below
\item[(b)] Feasible set should be compact
\item[(c)] Sequence of objective values should be strictly decreasing
\item[(d)] Uniqueness of the minimizer in each step.
\end{itemize}
\review{Note that condition (d) is required to ensure strict monotonicity of the objective sequence. Therefore, by using above conditions (a)-(c) along with \cite[Th. 3.14]{rudin1964principles}, we show the convergence objective sequence generated by Algorithm \ref{algo-1}. Then, by using the discussions in \cite{marks1978technical,lanckriet2009convergence,scutari_1}, we show that \reviewF{every limit point of the sequence of iterates generated by Algorithm \ref{algo-1} is a stationary point of nonconvex problems \eqref{eqn-6} and \eqref{eqn-mse-1}}. The term iterates correspond to the stacked vector of solution variables obtained in each iteration.}

Before discussing the convergence analysis, let us consider a generalized formulation for the problems in \eqref{eqn-6} and \eqref{eqn-mse-1} as
\begin{IEEEeqnarray}{RcL} \label{con} \neqsub
\underset{\mx,\my,\mz}{\text{minimize}} &\quad& \hat{f}(\mx,\my,\mz) \eqsub \label{con-obj} \\
\text{subject to} &\quad& h(\mz) - g_0(\mx,\my) \leq 0 \eqsub \label{con-dc} \\
&\quad& g_1(\mx,\my) \leq 0 \eqsub \label{con-cvx-blk} \\
&\quad& g_2(\mx) \leq 0 \eqsub \label{con-cvx}
\end{IEEEeqnarray}
where functions \me{g_2} and \me{\hat{f}} are convex, and function \me{h} is linear. Let \me{g_0} and \me{g_1} be a convex functions w.r.t either \me{\mx} or \me{\my}, but not jointly convex on both variables. The constraint \eqref{con-dc} corresponds to either \eqref{eqn-6.2} or \eqref{eqn-mse-1.2} and the constraint \eqref{con-cvx-blk} represents to either  \eqref{eqn-6.3} or \eqref{eqn-mse-1.3} correspondingly. Other convex constraints are handled by \eqref{con-cvx} and the feasible set of \eqref{con} is given by 
\iftoggle{single_column}{
\begin{equation}
\mc{F} = \{ \; \mx,\my,\mz \; \big | h(\mz) - g_0(\mx,\my) \leq 0, g_1(\mx,\my) \leq 0, g_2(\mx) \leq 0 \}.
\end{equation}
}{\begin{align}
\mc{F} = \{ \; \mx,\my,\mz \; \big | & \quad h(\mz) - g_0(\mx,\my) \leq 0, \nonumber \\
& \quad g_1(\mx,\my) \leq 0, g_2(\mx) \leq 0 \}.
\end{align}}

\review{We use following notations to discuss the convergence of algorithm in Algorithm \ref{algo-1}. Since it involves two nested loops, \textit{i.e.}, one for \ac{SCA} and another for \ac{AO}, we denote \ac{AO} step by a superscript \me{(i)} and \ac{SCA} iteration by a subscript \me{k}. Let \me{\mx}, \me{\my}, and \me{\mz} be the stacked vector all transmit precoders, receive beamformers, and other optimization variables used in \eqref{eqn-6} and \eqref{eqn-mse-1} respectively. Let us consider \ac{AO} step \eqn{i} with fixed \eqn{\my}. The solution obtained in the \eqn{\ith{k}} \ac{SCA} step is given by \eqn{\mx=\iter{\mx}{k}{i}} and \eqn{\mz=\iter{\mz}{k}{i}}. As \eqn{k \to \infty}, objective sequence converges, and the respective solution is denoted by \me{\iter{\mx}{\lpointx}{i}} and \me{\iter{\mz}{\lpointx|\my}{i}}. Similarly, while alternating the optimization variables for fixed \eqn{\mx}, as \eqn{k \to \infty}, the solution obtained is represented as \me{\my = \iter{\my}{\lpointx}{i}} and \me{\mz = \iter{\mz}{\lpointx|\mx}{i}}. In the following, we show that the conditions listed in (a)-(d) are satisfied by Algorithm \ref{algo-1}.}

\subsection{Bounded Objective Function and Compact Feasible Set} \label{b_obj}
The feasible sets of the problems \eqref{eqn-6} and \eqref{eqn-mse-1} are bounded and closed, which is due to the total power constraint on the transmit precoders \eqref{eqn-6.4}, therefore, the sets are compact. 

The minimum of the norm in the objective is \me{\|x\|_q > -\infty}, therefore, it is bounded below. The objective function is Lipschitz continuous over the feasible set. The objective function is bounded from above as well, since the feasible set is bounded.

\subsection{Uniqueness of the Iterates and Strong Convexity} \label{c-a}
The uniqueness of the iterates \eqn{\{\iter{\mx}{k}{i},\iter{\my}{\lpointx}{i-1},\iter{\mz}{k|\my}{i}\}} can be ensured for the \ac{MSE} reformulated problem in \eqref{eqn-mse-2} when all the constraints are active. However when \eqn{\hat{f}(\iter{\mx}{k}{i},\iter{\my}{\lpointx}{i-1},\iter{\mz}{k|\my}{i}) = 0} after some iteration, say \me{k} and \me{i}, the uniqueness cannot be guaranteed as there can be multiple solutions with the same objective value in the feasible set. Similarly, when the objective is non-zero for \eqref{eqn-9}, the minimizer is unique only if the receivers are matched to the transmit beamformers.

To ensure the uniqueness of the transmit and the receive beamformers, the objective function of the convex subproblems in \eqref{eqn-9} and \eqref{eqn-mse-2} are regularized by a strongly convex function in each \ac{SCA} iteration \eqn{k} and \ac{AO} update step \eqn{i} as 
\begin{IEEEeqnarray}{rCl} \neqsub
\hat{f}(\mbf{z}) &=& \| \tilde{\mbf{v}} \|_q \eqsub \label{orig_obj} \\ 
f(\mbf{z}) &=& \hat{f}(\mbf{z}) + {\tau}_k^{(i)} \, \| \mbf{z} - \mbf{z}^{(i)}_{k} \|^2_2 \eqsub \label{mod_obj} 
\end{IEEEeqnarray}
where \eqn{\mbf{z}} is a vector stacking all optimization variables, which can be either \eqn{[\mx,\iter{\my}{\lpoint}{i-1},\mz]} or \eqn{[\iter{\mx}{\lpoint}{i},\my,\mz]} depending on the problem and \eqn{\mbf{z}^{(i)}_{k} \triangleq [\mx_{k}^{(i)},\iter{\my}{\lpoint}{i-1},\mz_{k}^{(i)}]} is the solution obtained in the previous \eqn{\ith{(k-1)}} \ac{SCA} step. The positive constant \me{{\tau}_{k}^{(i)} > 0} ensures the strong convexity of \eqn{f} in each step and \eqn{c > 0} be the constant of strong convexity, defined as \eqn{c \triangleq \min_{k} \{{\tau}_{k}^{(i)}\}}, as shown in \cite{scutari-1}. Thus, due to the strong convexity of \eqn{f} in \eqref{mod_obj}, uniqueness of the minimizer is guaranteed in each \ac{SCA} step as discussed in \cite{yang_yang,scutari-1}. 

\subsection{Strict Monotonicity of the Objective Sequence} \label{mcity}
In the following discussions, we consider the modified objective \eqn{f} in \eqref{mod_obj} instead of \eqn{\hat{f}} due to the uniqueness of the minimizer and upon convergence of the objective, \eqn{f} is equal to \eqn{\hat{f}}. Therefore, the discussions are valid for the \ac{JSFRA} problem in \eqref{eqn-6} by regularizing the objective \eqref{eqn-6.1} as \eqref{mod_obj}.

To begin with, let us consider the variable \me{\my} is fixed for the \me{\ith{i}} \ac{AO} with the optimal value found in previous iteration \me{i-1} as \me{\iter{\my}{\lpoint}{i-1}}. In order to solve for \me{\mx} in \ac{SCA} iteration \me{k}, we linearize the nonconvex function \me{g_0} using previous \ac{SCA} iterate \me{\iter{\mx}{k}{i}} of \me{\mx} as
\iftoggle{single_column}{
\begin{equation}\label{con-relax}
\hat{g}_o(\mx,\iter{\my}{\lpoint}{i-1};\iter{\mx}{k}{i}) = {g}_0(\iter{\mx}{k}{i},\iter{\my}{\lpoint}{i-1}) + \nabla g_0(\iter{\mx}{k}{i},\iter{\my}{\lpoint}{i-1})^{\mathrm{T}} (\mx - \iter{\mx}{k}{i}).
\end{equation}}{
\begin{multline} \label{con-relax}
\hat{g}_o(\mx,\iter{\my}{\lpoint}{i-1};\iter{\mx}{k}{i}) = {g}_0(\iter{\mx}{k}{i},\iter{\my}{\lpoint}{i-1}) \\ + \nabla g_0(\iter{\mx}{k}{i},\iter{\my}{\lpoint}{i-1})^{\mathrm{T}} (\mx - \iter{\mx}{k}{i}).
\end{multline}}
Let \me{\iter{\mc{X}}{k}{i}} be the feasible set for the \eqn{\ith{i}} \ac{AO} iteration and the \me{\ith{k}} \ac{SCA} point for a fixed \me{\iter{\my}{\lpoint}{i-1}} and \me{\iter{\mx}{k}{i}}. Similarly, \me{\iter{\mc{Y}}{k}{i}} denotes the feasible set for a fixed \me{\iter{\mx}{\lpoint}{i}} and \me{\iter{\my}{k}{i}}. Using \eqref{con-relax}, the convex subproblem for the \me{\ith{i}} \ac{AO} iteration and the \me{\ith{k}} \ac{SCA} point for variables \me{\mx} and \me{\mz} is given by
\begin{IEEEeqnarray}{rCl} \label{con-m} \neqsub
\underset{\mx,\mz}{\text{minimize}} &\quad& f(\mx,\iter{\my}{\lpoint}{i-1},\mz) \eqsub \label{con-obj-m} \\
\text{subject to} &\quad& h(\mz) - \hat{g}_0(\mx,\iter{\my}{\lpoint}{i-1};\iter{\mx}{k}{i}) \leq 0 \eqsub \label{con-dc-m} \\
&\quad& g_1(\mx,\iter{\my}{\lpoint}{i-1}) \leq 0, \quad g_2(\mx) \leq 0 \eqsub \label{con-cvx-blk-m}
\end{IEEEeqnarray}
The feasible set defined by the problem \eqref{con-m} is denoted by \me{\iter{\mc{X}}{k}{i} \subset \mc{F}}. To prove the convergence of the \ac{SCA} updates in the \me{\ith{i}} \ac{AO} iteration, let us consider that \eqref{con-m} yields 
\me{\iter{\mx}{k+1}{i}} and \me{\iter{\mz}{k+1}{i}} as the solution in the \me{\ith{k}} iteration. The point \me{\iter{\mx}{k+1}{i}} and \me{\iter{\mz}{k+1}{i}}, which minimizes the objective function satisfies
\iftoggle{single_column}{
\begin{equation}\label{con-sub-set}\allowdisplaybreaks
h(\iter{\mz}{k+1}{i}) - {g}_0(\iter{\mx}{k+1}{i},\iter{\my}{\lpoint}{i-1}) \leq h(\iter{\mz}{k+1}{i}) - \hat{g}_0(\iter{\mx}{k+1}{i},\iter{\my}{\lpoint}{i-1};\iter{\mx}{k}{i}) \leq 0. 
\end{equation}}{
\begin{multline}\label{con-sub-set}\allowdisplaybreaks
h(\iter{\mz}{k+1}{i}) - {g}_0(\iter{\mx}{k+1}{i},\iter{\my}{\lpoint}{i-1}) \leq \\
h(\iter{\mz}{k+1}{i}) - \hat{g}_0(\iter{\mx}{k+1}{i},\iter{\my}{\lpoint}{i-1};\iter{\mx}{k}{i}) \leq 0. 
\end{multline}}
Using \eqref{con-sub-set}, we can show that \me{\{\iter{\mx}{k+1}{i},\iter{\my}{\lpoint}{i-1}, \iter{\mz}{k+1}{i}\}} is feasible, since the initial \ac{SCA} operating point \me{\iter{\mx}{\lpoint}{i-1}} was chosen to be feasible from the \me{\ith{(i-1)}} \ac{AO} iteration. In each \ac{SCA} step, the feasible set includes the solution from the previous iteration as \me{\{\iter{\mx}{k+1}{i},\iter{\my}{\lpoint}{i-1},\iter{\mz}{k+1}{i}\} \in \iter{\mc{X}}{k+1}{i} \subset \mc{F}}, therefore, it decreases the objective as \cite{lanckriet2009convergence,scutari_1,amir}
\iftoggle{single_column}{
\begin{equation} \label{con-convergence}
f(\iter{\mx}{0}{i},\iter{\my}{\lpoint}{i-1},\iter{\mz}{0}{i}) \geq f(\iter{\mx}{k}{i},\iter{\my}{\lpoint}{i-1},\iter{\mz}{k}{i}) \geq f(\iter{\mx}{k+1}{i},\iter{\my}{\lpoint}{i-1},\iter{\mz}{k+1}{i}) \geq f(\iter{\mx}{\lpoint}{i},\iter{\my}{\lpoint}{i-1},\iter{\mz}{\lpoint|\my}{i}). 
\end{equation}}{
\begin{multline} \label{con-convergence} \allowdisplaybreaks
f(\iter{\mx}{0}{i},\iter{\my}{\lpoint}{i-1},\iter{\mz}{0}{i}) \geq f(\iter{\mx}{k}{i},\iter{\my}{\lpoint}{i-1},\iter{\mz}{k}{i}) \\ \geq f(\iter{\mx}{k+1}{i},\iter{\my}{\lpoint}{i-1},\iter{\mz}{k+1}{i}) \geq f(\iter{\mx}{\lpoint}{i},\iter{\my}{\lpoint}{i-1},\iter{\mz}{\lpoint|\my}{i}). 
\end{multline}}
Thus, the sequence \me{\{f(\iter{\mx}{k}{i},\iter{\my}{\lpoint}{i-1},\iter{\mz}{k}{i})\}} is nonincreasing. However, to ensure strict monotonicity of the objective sequence, we rely on the modified objective in \eqref{con-obj-m}, which is strongly convex with some parameter \eqn{c > 0}. Now, to show strict monotonicity, let us consider \eqn{\mbf{z}_{k}^{(i)} \triangleq [\mx_{k}^{(i)},\iter{\my}{\lpoint}{i-1},\mz_{k}^{(i)}]} as the minimizer for \eqref{con-m} in the \eqn{\ith{(k-1)}} \ac{SCA} step and let \eqn{\iter{\mbf{z}}{k+1}{i}} be the minimizer in the \eqn{\ith{k}} step. At the \eqn{\ith{k}} \ac{SCA} iteration, \eqn{\forall \mbf{z} \in \mc{X}_{k}^{(i)}}, it follows
\begin{IEEEeqnarray}{rCl} \neqsub \label{strict_monotonicity}
\nabla f(\iter{\mbf{z}}{k+1}{i})^\tran (\mbf{z} - \iter{\mbf{z}}{k+1}{i}) &\geq& 0 \eqsub \\
f(\mbf{z}) - f(\iter{\mbf{z}}{k+1}{i}) &\geq& c \, \|\mbf{z} - \iter{\mbf{z}}{k+1}{i}\|^2 \eqsub
\end{IEEEeqnarray}
since \eqn{\iter{\mbf{z}}{k+1}{i}} is the solution. Using \eqref{strict_monotonicity} and \eqn{\iter{\mbf{z}}{k}{i} \in \iter{\mc{X}}{k}{i}}, we can show that \eqn{f(\iter{\mbf{z}}{k}{i}) > f(\iter{\mbf{z}}{k+1}{i})} holds in each \ac{SCA} step unless \eqn{\mbf{z}_{k}^{(i)} \to \mbf{z}_{\lpoint}^{(i)}} as \eqn{k \to \infty}. Now using the facts that (i) \eqn{\{f(\mbf{z}^{(i)}_k)\}} is monotonic, and (ii) the uniqueness of the minimizer (see \eqref{strict_monotonicity}), strict monotonicity of \eqn{\{f(\mbf{z}^{(i)}_k)\}} is ensured \cite{scutari2010convex}. Now, by using the fact that the objective sequence is strictly nonincreasing and bounded, we can ensure the convergence of objective sequence in each \ac{AO} step \eqn{{i}} and \reviewF{\eqn{\mbf{z}_{\lpoint}^{(i)} \triangleq \{\iter{\mx}{\lpoint}{i},\iter{\my}{\lpoint}{i-1},\iter{\mz}{\lpoint|\my}{i}\}} is the solution obtained upon convergence of the objective sequence \eqn{\{f(\mbf{z}_k^{(i)})\}} in the \eqn{\ith{i}} \ac{AO} iteration.}

Once \me{\iter{\mx}{\lpoint}{i}} is obtained for fixed \me{\my}, then \eqref{con} is solved for \me{\my} with fixed \me{\mx}. However, after fixing \me{\mx} as \me{\iter{\mx}{\lpoint}{i}} in \eqref{con}, the problem is still nonconvex due to \eqref{con-dc}. Following similar approach as above, the minimizer \me{ \{\iter{\mx}{\lpoint}{i},\iter{\my}{k+1}{i},\iter{\mz}{k+1}{i}\}} can be found in each \ac{SCA} step \me{k} by solving \eqref{con-m} iteratively. Note that \me{\iter{\mz}{k+1}{i}} is reused since the variable \me{\mx} is fixed in the \me{\ith{i}} \ac{AO} iteration. The convergence and the nonincreasing behavior of the objective follow similar arguments as above. \footnote{Note that the \ac{MMSE} receiver in \eqref{eqn-10} can also be used instead of performing the \ac{SCA} updates until convergence for the optimal receiver.} The solution obtained by solving \eqref{con-m} iteratively until convergence of the objective value is given as \me{\{\iter{\mx}{\lpoint}{i}, \iter{\my}{\lpoint}{i},\iter{\mz}{\lpoint|\mx}{i}\} \in \iter{\mc{Y}}{\lpoint}{i} \subset \mc{F}}. 

Finally, to prove the global convergence of the \reviewF{objective sequence}, we need to show that the \ac{AO} updates also produce a nonincreasing sequence of objectives, \textit{i.e.}, 
\iftoggle{single_column}{
\begin{equation}
f(\iter{\mx}{\lpoint}{i},\iter{\my}{0}{i},\iter{\mz}{0}{i}) \leq f(\iter{\mx}{\lpoint}{i},\iter{\my}{\lpoint}{i-1},\iter{\mz}{\lpoint|\my}{i}).
\end{equation}}{
\begin{equation}
f(\iter{\mx}{\lpoint}{i},\iter{\my}{0}{i},\iter{\mz}{0}{i}) \leq f(\iter{\mx}{\lpoint}{i},\iter{\my}{\lpoint}{i-1},\iter{\mz}{\lpoint|\my}{i}).
\end{equation}}
Let \me{\iter{\mx}{\lpoint}{i}} and \me{\iter{\mz}{\lpoint|\my}{i}} be the solution obtained by solving \eqref{con-m} iteratively until \ac{SCA} convergence in the \me{\ith{i}} \ac{AO} iteration for \eqn{\mx} and \eqn{\mz} with fixed \eqn{\my = \iter{\my}{\lpoint}{i-1}}. To find \me{\iter{\my}{0}{i}}, we fix \eqn{\mx} as \me{\iter{\mx}{\lpoint}{i}} and optimize for \me{\my}. Since the convex function is linearized in \eqref{con-dc}, the fixed operating point is also included in the feasible set \eqn{\{\iter{\mx}{\lpoint}{i},\iter{\my}{\lpoint}{i-1},\iter{\mz}{\lpoint|\my}{i}\} \in \iter{\mc{Y}}{0}{i}} by the equality in \eqref{con-sub-set}. Using this, we can show the monotonicity of the objective as
\begin{equation} \label{fixed_point}
f(\iter{\mx}{\lpoint}{i},\iter{\my}{0}{i},\iter{\mz}{0}{i}) \leq f(\iter{\mx}{\lpoint}{i},\iter{\my}{\lpoint}{i-1},\iter{\mz}{\lpoint|\mx}{i}).
\end{equation}
The \ac{AO} update follows \eqn{\{\iter{\mx}{\lpoint}{i},\iter{\my}{\lpoint}{i-1},\iter{\mz}{\lpoint|\my}{i}\} \in \{ \iter{\mc{X}}{\lpoint}{i} \cap \iter{\mc{Y}}{0}{i} \}}. Using \eqref{strict_monotonicity}, strict monotonicity of the objective \eqref{mod_obj} while alternating the optimization variables from \eqn{\mx,\mz} to \eqn{\my,\mz} in the \eqn{\ith{i}} \ac{AO} update can also be ensured. It follows from that fact that \eqn{\iter{\mbf{z}}{\lpoint}{i}} is included in the feasible set \eqn{\mc{Y}_{0}^{i}}, and therefore the objective function follows \eqref{fixed_point} strictly in each \ac{AO} update.

\subsection{Stationarity of Limit Points} \label{c-b}
\reviewF{We discuss the convergence of whole sequence of iterates generated by the iterative algorithm. In order to do that, let us use single index \eqn{t} to represent both \ac{SCA} step \eqn{k} and \ac{AO} index \eqn{i}. It corresponds to every \ac{AO} iterations and \ac{SCA} updates performed within each of the \ac{AO} step.
Using global index \eqn{t}, we denote iterate \eqn{\ma^{t}} as a stacked vector of solution variables obtained in every \ac{SCA} iteration \eqn{{k}} and \ac{AO} step \eqn{{i}}.

Using the arguments in Appendices \ref{b_obj} and \ref{mcity}, we can show that the sequence of objective values \eqn{\{f(\ma^t)\}} converges. Similar claim cannot be made on the convergence of \eqn{\{\ma^t\}}. However, due to the compactness of feasible set, the sequence of iterates \eqn{\{\ma^t\}} is bounded. Therefore, there exists at least one subsequence that converges to a limit point in the feasible set \cite[Th. 3.6]{rudin1964principles}. Note that the sequence of iterates \eqn{\{\ma^t\}} need not be convergent, as there can possibly be two subsequences of \eqn{\{\ma^t\}} that converges to different limit points.

Therefore, we can only claim that every limit point of the sequence \eqn{\{\ma^t\}} is a stationary point, \textit{i.e.,} the limit point of every convergent subsequence is a stationary point. To show that, let us consider a subsequence \eqn{\{\ma^{t_j} |j = 0,1,\dotsc\}} of \eqn{\{\ma^t\}} that converges to \eqn{\bar{\ma}}, where \eqn{\bar{\ma} = \lim_{j \to \infty} \ma^{t_j}} is a limit point of \eqn{\{\ma^t\}}. Since \eqn{\{\ma^{t_j}\} \subset \{\ma^t\}} and \eqn{\{f(\ma^{t_j})\} \subset \{f(\ma^t)\}}, it follows from Appendices \ref{c-a} and \ref{mcity} that
\begin{equation} \label{seq_mon}
f(\ma^{t_{j+1}}) < f(\ma^{t_{j}}) < f(\ma^{t_{j-1}}).
\end{equation}
Note that \eqref{seq_mon} holds strictly while alternating the optimization variables as shown in \eqref{fixed_point}. Now, by taking the limit as \eqn{j \to \infty} and by using the continuity of objective function \eqn{f}, we obtain
\begin{equation} \label{limpoint}
\lim_{t \to \infty} f(\ma^{t}) = \lim_{j \to \infty} f(\ma^{t_j}) = f(\lim_{j \to \infty} \ma^{t_{j}}) = f(\bar{\ma}).
\end{equation}
Similarly, we can show that for every limit point of \eqn{\{\ma^t\}}, there exists a convergent subsequence \eqn{\{\ma^{t_k}|k=0,1,\dotsc\}}, such that 
\begin{equation} \label{limpoint1}
\lim_{t \to \infty} f(\ma^{t}) = \lim_{k \to \infty} f(\ma^{t_k}) = f(\lim_{k \to \infty} \ma^{t_{k}}) = f(\hat{\ma}) = f(\bar{\ma})
\end{equation}
where \eqn{\lim_{k\to\infty}\ma^{t_k} \to \hat{\ma}} is another limit point of \eqn{\{\ma^t\}}.

Let us prove the stationarity of \eqn{\bar{\ma}} by contradiction. Assume that \eqn{\bar{\ma}} is not a stationary point, then there exist some other point \eqn{\ma^\prime}, such that \eqn{\nabla f (\bar{\ma})^\tran (\ma^\prime - \bar{\ma}) < 0} and \eqn{f(\bar{\ma}) > f(\ma^\prime)}. Since \eqn{\ma^\prime} is a point in \eqn{\{\ma^t\}}, there exist a subsequence that converges to \eqn{\ma^\prime}, for which \eqn{\ma^\prime} is a limit point. However, by \eqref{limpoint} and \eqref{limpoint1}, we have \eqn{f(\bar{\ma}) = f(\ma^\prime)} for all limit points of any convergent subsequence of \eqn{\{\ma^t\}}, which is a contradiction. 

Now, by using strict monotonicity of \eqn{\{f(\ma^{t_j})\}}, it follows that as \eqn{j \to \infty}, \eqn{\ma^{t_j} \to \bar{\ma}} and \eqn{f(\ma^{t_{j}}) - f(\ma^{t_{j+1}}) \to 0}. Moreover, since \eqn{\bar{\ma}} is the solution of \eqref{con-m}, it satisfies \eqref{strict_monotonicity} over convex subset \eqn{\mc{Q}}, which can either be \eqn{\iter{\mc{X}}{k}{i}} or \eqn{\iter{\mc{Y}}{k}{i}} as \eqn{k \to \infty, i \to \infty}. Using the above statements, we can show that \eqn{\bar{\ma}} satisfies the optimality condition  \cite[Prop. 2.1.2]{bertsekas1999nonlinear} as 
\begin{equation} \label{seq_mon1}
\nabla f (\bar{\ma})^\tran (\ma - \bar{\ma}) \geq 0, \; \fall \ma \in \mc{Q} \subset \mc{F} \eqsub.
\end{equation}
Hence, \eqn{\bar{\ma}} is a stationary point and by using \eqref{limpoint} and \eqref{limpoint1}, we can show that every limit point of \eqn{\{\ma^t\}} is a stationary point.

Now to show that \eqn{\bar{\ma}} is also a stationary point of original nonconvex problem \eqref{con}, we show the equivalence between gradients of subproblem \eqref{con-m} and \eqref{con} at stationary point \eqn{\bar{\ma}}. In order to show that \eqn{\bar{\ma}} is a \ac{KKT} point for \eqref{con}, it must satisfy the gradient condition of \eqref{con}, \textit{i.e.},
\begin{equation} \label{kkt_expr}
\textstyle	\nabla \hat{f}(\bar{\ma}) + \mu_0 \big [ \nabla h(\bar{\ma}) - \nabla {g}_0(\bar{\ma}) \big ] + \sum_{i=1}^2 \mu_i \nabla g_i(\bar{\ma}) = 0
\end{equation}
for some Lagrange multipliers \eqn{\mu_i \geq 0} and feasible as \eqn{\bar{\ma} \in \mc{F}}.

Using \eqref{con-sub-set}, we have \eqn{\mc{X}_{k}^{(i)},\mc{Y}_{k}^{(i)} \subset \mc{F}}, therefore, \eqn{\bar{\ma}} is a feasible point for \eqref{con}. Moreover, the interiors of \eqn{\mc{X}_{k}^{(i)}} and \eqn{\mc{Y}_{k}^{(i)}} are non-empty, therefore, Slater's constraint qualification holds for \eqref{con-m}. Since \eqn{\bar{\ma}} is the solution for \eqref{con-m} as \eqn{k \to \infty,i \to \infty}, there exist Lagrange multipliers \eqn{\mu_i \geq 0} that  satisfies
\begin{equation} \label{kkt_mod_expr}
\textstyle \nabla f(\bar{\ma}) + \mu_0 \big [ \nabla h(\bar{\ma}) - \nabla\hat{g}_0(\bar{\ma};\bar{\ma}) \big ] + \sum_{i=1}^2 \mu_i \nabla g_i(\bar{\ma}) = 0.
\end{equation}
The relation between \eqref{kkt_expr} and \eqref{kkt_mod_expr} is evident by the following facts: (i) The quadratic term \eqn{\|\ma - \ma^{t_j}\|^2} in \eqref{mod_obj} vanishes as \eqn{j \to \infty, \ma^{t_j} \to \bar{\ma}}, since we assume that \eqn{\bar{\ma}} is the limit point of a convergent subsequence \eqn{\{\ma^{t_j}\}}. Therefore, the gradient evaluated at \eqn{\bar{\ma}} satisfies \eqn{\nabla {f}(\bar{\ma}) = \nabla \hat{f}(\bar{\ma})}. (ii) Additionally, by using the continuity of function \eqn{g_o} and \eqref{con-relax}, equivalence with the gradient of \eqref{con-dc} is obtained as \eqn{\ma^{t_j} \to \bar{\ma}}
\begin{equation} \label{rhs_equiv}
\nabla h(\bar{\ma}) - \nabla \hat{g}_o(\bar{\ma};\bar{\ma}) = \nabla h(\bar{\ma}) - \nabla g_o(\bar{\ma}).
\end{equation}
Now, using the relation \eqref{rhs_equiv} and \eqn{\nabla {f}(\bar{\ma}) = \nabla \hat{f}(\bar{\ma})} in \eqref{kkt_mod_expr}, we can show that limit point \eqn{\bar{\ma}} satisfies \eqref{kkt_expr}. Finally, by following \cite[Thms. 2 and 11]{scutari_1} and \cite[Prop. 3.2]{amir}, we can show that every limit point of the sequence of iterates \eqn{\{\ma^t\}} generated by the iterative method is a stationary point of problem \eqref{con}.}
\end{comment}


The following conditions are required to show the convergence of Algorithm \ref{algo-1}, which is used to solve problems \eqref{eqn-6} and \eqref{eqn-mse-1} in an iterative manner.
\begin{itemize}
\item[(a)] Uniqueness of the minimizer in each step
\item[(b)] Objective function should be bounded below
\item[(c)] Feasible set should be compact
\item[(d)] Sequence of objective values should be strictly decreasing
\end{itemize}
Upon satisfying (a)-(d), we can show the convergence of the objective sequence generated by the Algorithm \ref{algo-1}, using \cite[Th. 3.14]{rudin1964principles}. However, it is not  sufficient enough to ensure the convergence of the corresponding sequence of iterates. Therefore, by following the discussions in \cite{marks1978technical,lanckriet2009convergence,scutari_1}, we can only claim that \reviewF{every limit point of the sequence of iterates generated by the iterative method is a stationary point of original nonconvex formulations in \eqref{eqn-6} and \eqref{eqn-mse-1}}.

Before discussing further on the analysis, let us consider a generalized formulation for the problems in \eqref{eqn-6} and \eqref{eqn-mse-1} as
\begin{IEEEeqnarray}{RcL} \label{con} \neqsub
\underset{\mx,\my,\mz}{\text{minimize}} &\quad& \hat{f}(\mx,\my,\mz) \eqsub \label{con-obj} \\
\text{subject to} &\quad& h(\mz) - g_0(\mx,\my) \leq 0 \eqsub \label{con-dc} \\
&\quad& g_1(\mx,\my) \leq 0 \eqsub \label{con-cvx-blk} \\
&\quad& g_2(\mx) \leq 0 \eqsub \label{con-cvx}
\end{IEEEeqnarray}
where \me{g_2} and \me{\hat{f}} are convex, \me{h} is linear, and \me{g_0} and \me{g_1} are convex w.r.t either \me{\mx} or \me{\my}, but not jointly convex on both variables. The constraint \eqref{con-dc} corresponds to either \eqref{eqn-6.2} or \eqref{eqn-mse-1.2} and the constraint \eqref{con-cvx-blk} represents to either  \eqref{eqn-6.3} or \eqref{eqn-mse-1.3} correspondingly. Other convex constraints are handled by \eqref{con-cvx} and the feasible set of \eqref{con} is given by 
\iftoggle{single_column}{
\begin{equation}
\mc{F} = \{ \; \mx,\my,\mz \; \big | h(\mz) - g_0(\mx,\my) \leq 0, g_1(\mx,\my) \leq 0, g_2(\mx) \leq 0 \}.
\end{equation}
}{\begin{align}
\mc{F} = \{ \; \mx,\my,\mz \; \big | & \quad h(\mz) - g_0(\mx,\my) \leq 0, \nonumber \\
& \quad g_1(\mx,\my) \leq 0, g_2(\mx) \leq 0 \}.
\end{align}}

We use the following notations to discuss the convergence of Algorithm \ref{algo-1}. Since it involves two nested loops, \textit{i.e.}, one for \ac{SCA} and another for \ac{AO}, we denote the \ac{AO} step by a superscript \me{(i)} and the \ac{SCA} iteration by a subscript \me{k}. Let \me{\mx}, \me{\my}, and \me{\mz} be the vector stacking all transmit precoders, receive beamformers, and other optimization variables used in \eqref{eqn-6} and \eqref{eqn-mse-1} respectively. Let us consider \ac{AO} step \eqn{i} with fixed \eqn{\my}. The solution obtained in the \eqn{\ith{k}} \ac{SCA} step is given by \eqn{\mx=\iter{\mx}{k}{i}} and \eqn{\mz=\iter{\mz}{k}{i}}. As \eqn{k \to \infty}, objective sequence converges, and the respective solution is denoted by \me{\iter{\mx}{\lpointx}{i}} and \me{\iter{\mz}{\lpointx|\my}{i}}. Similarly, while alternating the optimization variables for fixed \eqn{\mx}, as \eqn{k \to \infty}, the solution obtained is represented as \me{\my = \iter{\my}{\lpointx}{i}} and \me{\mz = \iter{\mz}{\lpointx|\mx}{i}}. In the following, we show that the conditions listed in (a)-(d) are satisfied by Algorithm \ref{algo-1}.

\subsection{Uniqueness of the Iterates and Strong Convexity} \label{c-a}
The uniqueness of the iterates \eqn{\{\iter{\mx}{k}{i},\iter{\my}{\lpointx}{i-1},\iter{\mz}{k|\my}{i}\}} can be ensured for the \ac{MSE} reformulated problem in \eqref{eqn-mse-2} when all the constraints are active. However when \eqn{\hat{f}(\iter{\mx}{k}{i},\iter{\my}{\lpointx}{i-1},\iter{\mz}{k|\my}{i}) = 0} after some iteration, say \me{k} and \me{i}, the uniqueness cannot be guaranteed as there can be multiple solutions with the same objective value in the feasible set. Similarly, when the objective is non-zero for \eqref{eqn-9}, the minimizer is unique only if the receivers are matched to the transmit beamformers.

To ensure the uniqueness of the transmit and the receive beamformers, the objective function of the convex subproblems in \eqref{eqn-9} and \eqref{eqn-mse-2} are regularized by a strongly convex function in each \ac{SCA} iteration \eqn{k} and \ac{AO} update step \eqn{i} as 
\begin{IEEEeqnarray}{rCl} \neqsub
\hat{f}(\mbf{z}) &=& \| \tilde{\mbf{v}} \|_q \eqsub \label{orig_obj} \\ 
f(\mbf{z}) &=& \hat{f}(\mbf{z}) + {\tau}_k^{(i)} \, \| \mbf{z} - \mbf{z}^{(i)}_{k} \|^2_2 \eqsub \label{mod_obj} 
\end{IEEEeqnarray}
where \eqn{\mbf{z}} is a vector stacking all optimization variables, which can be either \eqn{[\mx,\iter{\my}{\lpoint}{i-1},\mz]} or \eqn{[\iter{\mx}{\lpoint}{i},\my,\mz]} depending on the problem and \eqn{\mbf{z}^{(i)}_{k} \triangleq [\mx_{k}^{(i)},\iter{\my}{\lpoint}{i-1},\mz_{k}^{(i)}]} is the solution obtained in the previous \eqn{\ith{(k-1)}} \ac{SCA} step. The positive constant \me{{\tau}_{k}^{(i)} > 0} ensures strong convexity of \eqn{f} in each step \cite[Sec. 3.4.3]{bertsekas1989parallel}. Thus, due to strong convexity in \eqn{f}, uniqueness of the minimizer is ensured in each \ac{SCA} step \cite{yang_yang,scutari-1}. 

\subsection{Bounded Objective Function and Compact Feasible Set} \label{b_obj}
The feasible set of problems \eqref{eqn-6} and \eqref{eqn-mse-1} is bounded and closed due to the total power constraint on transmit precoders by \eqref{eqn-6.4} and \eqref{eqn-mse-1.4}, therefore, the feasible region is compact. 

The minimum value of the norm function present in the objective is \me{\|\tilde{\mbf{v}}\|_q \geq 0} for all exponents used in \eqn{\ell_q} norm, therefore, it is bounded from below. Moreover, by the fact that the feasible set is compact, the objective function is Lipschitz continuous, and therefore bounded from above as well.

\subsection{Strict Monotonicity of the Objective Sequence} \label{mcity}
In the following discussions, we consider the modified objective \eqn{f} in \eqref{mod_obj} instead of \eqn{\hat{f}} to exploit the uniqueness of the minimizer at each iteration. However, we note that upon convergence \eqn{f} is equal to \eqn{\hat{f}}. Therefore, the discussions are valid for the \ac{JSFRA} problems in \eqref{eqn-6} and \eqref{eqn-mse-1} by regularizing the objective functions \eqref{eqn-6.1} and \eqref{eqn-mse-1.1} as \eqref{mod_obj}.

To begin with, let us consider the variable \me{\my} begin fixed for the \me{\ith{i}} \ac{AO} with the optimal value found in previous iteration \me{i-1} as \me{\iter{\my}{\lpoint}{i-1}}. In order to solve for \me{\mx} in \ac{SCA} iteration \me{k}, we linearize the nonconvex function \me{g_0} using previous \ac{SCA} iterate \me{\iter{\mx}{k}{i}} of \me{\mx} as
\iftoggle{single_column}{
\begin{equation}\label{con-relax}
\hat{g}_o(\mx,\iter{\my}{\lpoint}{i-1};\iter{\mx}{k}{i}) \triangleq {g}_0(\iter{\mx}{k}{i},\iter{\my}{\lpoint}{i-1}) + \nabla g_0(\iter{\mx}{k}{i},\iter{\my}{\lpoint}{i-1})^{\mathrm{T}} (\mx - \iter{\mx}{k}{i}).
\end{equation}}{
\begin{multline} \label{con-relax}
\hat{g}_o(\mx,\iter{\my}{\lpoint}{i-1};\iter{\mx}{k}{i}) \triangleq {g}_0(\iter{\mx}{k}{i},\iter{\my}{\lpoint}{i-1}) \\ + \nabla g_0(\iter{\mx}{k}{i},\iter{\my}{\lpoint}{i-1})^{\mathrm{T}} (\mx - \iter{\mx}{k}{i}).
\end{multline}}
Let \me{\iter{\mc{X}}{k}{i}} be the feasible set for the \eqn{\ith{i}} \ac{AO} iteration and the \me{\ith{k}} \ac{SCA} point for a fixed \me{\iter{\my}{\lpoint}{i-1}} and \me{\iter{\mx}{k}{i}}. Similarly, \me{\iter{\mc{Y}}{k}{i}} denotes the feasible set for a fixed \me{\iter{\mx}{\lpoint}{i}} and \me{\iter{\my}{k}{i}}. Using \eqref{con-relax}, the convex subproblem for the \me{\ith{i}} \ac{AO} iteration and the \me{\ith{k}} \ac{SCA} point for variables \me{\mx} and \me{\mz} is given by
\begin{IEEEeqnarray}{rCl} \label{con-m} \neqsub
\underset{\mx,\mz}{\text{minimize}} &\quad& f(\mx,\iter{\my}{\lpoint}{i-1},\mz) \eqsub \label{con-obj-m} \\
\text{subject to} &\quad& h(\mz) - \hat{g}_0(\mx,\iter{\my}{\lpoint}{i-1};\iter{\mx}{k}{i}) \leq 0 \eqsub \label{con-dc-m} \\
&\quad& g_1(\mx,\iter{\my}{\lpoint}{i-1}) \leq 0, \quad g_2(\mx) \leq 0 \eqsub \label{con-cvx-blk-m}
\end{IEEEeqnarray}
The feasible set defined by the problem \eqref{con-m} is denoted by \me{\iter{\mc{X}}{k}{i} \subset \mc{F}}. To prove the convergence of the \ac{SCA} updates in the \me{\ith{i}} \ac{AO} iteration, let us consider that \eqref{con-m} yields 
\me{\iter{\mx}{k+1}{i}} and \me{\iter{\mz}{k+1}{i}} as the solution in the \me{\ith{k}} iteration. The point \me{\iter{\mx}{k+1}{i}} and \me{\iter{\mz}{k+1}{i}}, which minimize the objective function satisfy
\iftoggle{single_column}{
\begin{equation}\label{con-sub-set}\allowdisplaybreaks
h(\iter{\mz}{k+1}{i}) - {g}_0(\iter{\mx}{k+1}{i},\iter{\my}{\lpoint}{i-1}) \leq h(\iter{\mz}{k+1}{i}) - \hat{g}_0(\iter{\mx}{k+1}{i},\iter{\my}{\lpoint}{i-1};\iter{\mx}{k}{i}) \leq 0. 
\end{equation}}{
\begin{multline}\label{con-sub-set}\allowdisplaybreaks
h(\iter{\mz}{k+1}{i}) - {g}_0(\iter{\mx}{k+1}{i},\iter{\my}{\lpoint}{i-1}) \leq \\
h(\iter{\mz}{k+1}{i}) - \hat{g}_0(\iter{\mx}{k+1}{i},\iter{\my}{\lpoint}{i-1};\iter{\mx}{k}{i}) \leq 0. 
\end{multline}}
Using \eqref{con-sub-set}, we can show that \me{\{\iter{\mx}{k+1}{i},\iter{\my}{\lpoint}{i-1}, \iter{\mz}{k+1}{i}\}} is feasible, since the initial \ac{SCA} operating point \me{\iter{\mx}{\lpoint}{i-1}} was chosen to be feasible from the \me{\ith{(i-1)}} \ac{AO} iteration. In each \ac{SCA} step, the feasible set includes the solution from the previous iteration as \me{\{\iter{\mx}{k+1}{i},\iter{\my}{\lpoint}{i-1},\iter{\mz}{k+1}{i}\} \in \iter{\mc{X}}{k+1}{i} \subset \mc{F}}, therefore, it decreases the objective as \cite{lanckriet2009convergence,scutari_1,amir}
\iftoggle{single_column}{
\begin{equation} \label{con-convergence}
f(\iter{\mx}{0}{i},\iter{\my}{\lpoint}{i-1},\iter{\mz}{0}{i}) \geq f(\iter{\mx}{k}{i},\iter{\my}{\lpoint}{i-1},\iter{\mz}{k}{i}) \geq f(\iter{\mx}{k+1}{i},\iter{\my}{\lpoint}{i-1},\iter{\mz}{k+1}{i}) \geq f(\iter{\mx}{\lpoint}{i},\iter{\my}{\lpoint}{i-1},\iter{\mz}{\lpoint|\my}{i}). 
\end{equation}}{
\begin{multline} \label{con-convergence} \allowdisplaybreaks
f(\iter{\mx}{0}{i},\iter{\my}{\lpoint}{i-1},\iter{\mz}{0}{i}) \geq f(\iter{\mx}{k}{i},\iter{\my}{\lpoint}{i-1},\iter{\mz}{k}{i}) \\ \geq f(\iter{\mx}{k+1}{i},\iter{\my}{\lpoint}{i-1},\iter{\mz}{k+1}{i}) \geq f(\iter{\mx}{\lpoint}{i},\iter{\my}{\lpoint}{i-1},\iter{\mz}{\lpoint|\my}{i}). 
\end{multline}}
Thus, the sequence \me{\{f(\iter{\mx}{k}{i},\iter{\my}{\lpoint}{i-1},\iter{\mz}{k}{i})\}} is nonincreasing. Using strong convexity of the objective, we can show that the inequalities in \eqref{con-convergence} are strict, \textit{i.e.}, the sequence of objectives returned by Algorithm \ref{algo-1} is strictly decreasing. To this end, let us consider \eqn{\mbf{z}_{k}^{(i)} \triangleq [\mx_{k}^{(i)},\iter{\my}{\lpoint}{i-1},\mz_{k}^{(i)}]} as the minimizer for \eqref{con-m} in the \eqn{\ith{(k-1)}} \ac{SCA} step and let \eqn{\iter{\mbf{z}}{k+1}{i}} be the minimizer in the \eqn{\ith{k}} step. At the \eqn{\ith{k}} \ac{SCA} iteration, \eqn{\forall \mbf{z} \in \mc{X}_{k}^{(i)}}, it follows
\begin{IEEEeqnarray}{rCl} \neqsub \label{strict_monotonicity}
\nabla f(\iter{\mbf{z}}{k+1}{i})^\tran (\mbf{z} - \iter{\mbf{z}}{k+1}{i}) &\geq& 0 \eqsub \\
f(\mbf{z}) - f(\iter{\mbf{z}}{k+1}{i}) &\geq& c \, \|\mbf{z} - \iter{\mbf{z}}{k+1}{i}\|^2 \eqsub
\end{IEEEeqnarray}
where \eqn{c > 0} be the constant of strong convexity, defined as \eqn{c \triangleq \min_{k} \{{\tau}_{k}^{(i)}\}}, and \eqn{\iter{\mbf{z}}{k+1}{i}} is the solution. Therefore, by using \eqref{strict_monotonicity} and \eqn{\iter{\mbf{z}}{k}{i} \in \iter{\mc{X}}{k}{i}}, we can show that \eqn{f(\iter{\mbf{z}}{k}{i}) > f(\iter{\mbf{z}}{k+1}{i})} holds strictly in each \ac{SCA} step unless \eqn{\mbf{z}_{k}^{(i)} \to \mbf{z}_{\lpoint}^{(i)}} as \eqn{k \to \infty}. Now, by utilizing the facts that (i) \eqn{\{f(\mbf{z}^{(i)}_k)\}} is monotonic, and (ii) the uniqueness of the minimizer (see \eqref{strict_monotonicity}), strict monotonicity of \eqn{\{f(\mbf{z}^{(i)}_k)\}} is ensured in every \ac{SCA} update \cite{scutari2010convex}. \reviewF{Since the objective is strictly decreasing and bounded, convergence of \eqn{\{f(\mbf{z}_k^{(i)})\}} is guaranteed in every \ac{AO} step \eqn{i}. Let \eqn{\mbf{z}_{\lpoint}^{(i)} \triangleq \{\iter{\mx}{\lpoint}{i},\iter{\my}{\lpoint}{i-1},\iter{\mz}{\lpoint|\my}{i}\}} be the solution obtained upon convergence of the objective sequence in the \eqn{\ith{i}} \ac{AO} update.}

Once \me{\iter{\mx}{\lpoint}{i}} is obtained for fixed \me{\my}, then \eqref{con} is solved for \me{\my} with fixed \me{\mx}. However, after fixing \me{\mx} as \me{\iter{\mx}{\lpoint}{i}} in \eqref{con}, the problem is still nonconvex due to \eqref{con-dc}. Following similar approach as above, the minimizer \me{ \{\iter{\mx}{\lpoint}{i},\iter{\my}{k+1}{i},\iter{\mz}{k+1}{i}\}} can be found in each \ac{SCA} step \me{k} by solving \eqref{con-m} iteratively. Note that \me{\iter{\mz}{k+1}{i}} is reused since the variable \me{\mx} is fixed in the \me{\ith{i}} \ac{AO} iteration. The convergence and the nonincreasing behavior of the objective follow similar arguments as above.\footnote{Note that the receive beamformers can also be designed by the \ac{MMSE} receiver using closed-form \eqref{eqn-10} instead designing recursively by updating the \ac{SCA} operating point as in optimal receiver \eqref{opt-rx}.} The solution obtained by solving \eqref{con-m} iteratively until convergence of the objective value is given as \me{\{\iter{\mx}{\lpoint}{i}, \iter{\my}{\lpoint}{i},\iter{\mz}{\lpoint|\mx}{i}\} \in \iter{\mc{Y}}{\lpoint}{i} \subset \mc{F}}. 

Finally, to prove the global convergence of the \reviewF{objective sequence}, we need to show that the \ac{AO} updates also produce a nonincreasing sequence of objectives, \textit{i.e.}, 
\iftoggle{single_column}{
\begin{equation}
f(\iter{\mx}{\lpoint}{i},\iter{\my}{0}{i},\iter{\mz}{0}{i}) \leq f(\iter{\mx}{\lpoint}{i},\iter{\my}{\lpoint}{i-1},\iter{\mz}{\lpoint|\my}{i}).
\end{equation}}{
\begin{equation}
f(\iter{\mx}{\lpoint}{i},\iter{\my}{0}{i},\iter{\mz}{0}{i}) \leq f(\iter{\mx}{\lpoint}{i},\iter{\my}{\lpoint}{i-1},\iter{\mz}{\lpoint|\my}{i}).
\end{equation}}
Let \me{\iter{\mx}{\lpoint}{i}} and \me{\iter{\mz}{\lpoint|\my}{i}} be the solution obtained by solving \eqref{con-m} iteratively until \ac{SCA} convergence in the \me{\ith{i}} \ac{AO} iteration for \eqn{\mx} and \eqn{\mz} with fixed \eqn{\my = \iter{\my}{\lpoint}{i-1}}. To find \me{\iter{\my}{0}{i}}, we fix \eqn{\mx} as \me{\iter{\mx}{\lpoint}{i}} and optimize for \me{\my}. Since the constraint \eqref{con-dc-m} is linearized around \eqn{\iter{\mbf{z}}{\lpoint}{i}}, the fixed operating point is also included in the feasible set by following \eqref{con-sub-set} as \eqn{\{\iter{\mx}{\lpoint}{i},\iter{\my}{\lpoint}{i-1},\iter{\mz}{\lpoint|\my}{i}\} \in \iter{\mc{Y}}{0}{i}}, therefore, we have
\begin{equation} \label{fixed_point}
f(\iter{\mx}{\lpoint}{i},\iter{\my}{0}{i},\iter{\mz}{0}{i}) \leq f(\iter{\mx}{\lpoint}{i},\iter{\my}{\lpoint}{i-1},\iter{\mz}{\lpoint|\my}{i}).
\end{equation}
\reviewF{Now, by using \eqref{con-dc-m}, we can ensure that \eqn{\iter{\mbf{z}}{\lpoint}{i} \in \{ \iter{\mc{X}}{\lpoint}{i} \cap \iter{\mc{Y}}{0}{i} \}} in each \ac{AO} update \eqn{i} while alternating the optimization variables from \eqn{\mx,\mz} to \eqn{\my,\mz}. Moreover, due to strong convexity of the objective function \eqref{mod_obj}, by following \eqref{strict_monotonicity}, we can show that \eqref{fixed_point} holds with strict inequality in each \ac{AO} update unless the overall objective sequence converges. Now, by combining \eqref{con-convergence}, \eqref{strict_monotonicity} and \eqref{fixed_point}, we can ensure strictly nonincreasing nature of the objective sequence \eqn{\{f(\mbf{z}^{(i)}_k)\}} in every \ac{SCA} and \ac{AO} step.}

\subsection{Stationarity of Limit Points} \label{c-b}
\reviewF{We discuss the convergence of the iterates generated by the iterative algorithm. In order to do that, let us consider a single global indexing \eqn{t} to represent both \ac{SCA} step \eqn{k} and \ac{AO} index \eqn{i}. It corresponds to every \ac{AO} iterations and \ac{SCA} updates performed within each \ac{AO} step.
Using this unified index \eqn{t}, we denote iterate \eqn{\ma^{t}} as a vector stacking all optimization variables produced in every \ac{SCA} iteration \eqn{{k}} and \ac{AO} step \eqn{{i}}.

\review{By following the discussions in Appendices \ref{b_obj} and \ref{mcity}, the sequence \eqn{\{f(\ma^t)\}} generated by the algorithm is bounded and monotonically decreasing, therefore, by using \cite[Prop. A.3]{bertsekas1999nonlinear}, convergence of the sequence \eqn{\{f(\ma^t)\}} can be ensured. Unfortunately, such a claim cannot be made on the convergence of \eqn{\{\ma^t\}}. However, due to the compactness of feasible set, the sequence of iterates \eqn{\{\ma^t\}} is bounded. Therefore, we can only claim that there exists at least one subsequence that converges to a limit point in the feasible set \cite[Th. 3.6]{rudin1964principles}.} 

We now show that every limit point of the sequence \eqn{\{\ma^t\}} is a stationary point, \textit{i.e.,} the limit point of every convergent subsequence is a stationary point. To do this, let us consider a subsequence \eqn{\{\ma^{t_j} |j = 0,1,\dotsc\}} of \eqn{\{\ma^t\}} that converges to \eqn{\bar{\ma}}, where \eqn{\bar{\ma} = \lim_{j \to \infty} \ma^{t_j}} is a limit point of \eqn{\{\ma^t\}}. Since \eqn{\{\ma^{t_j}\} \subset \{\ma^t\}} and \eqn{\{f(\ma^{t_j})\} \subset \{f(\ma^t)\}}, we have from Appendix \ref{mcity} that \eqn{f(\ma^{t_{j+1}}) \leq f(\ma^{t_{j}})}. Now, by taking the limit as \eqn{j \to \infty} and by using the continuity of objective function \eqn{f}, we obtain
\begin{equation} \label{limpoint}
\lim_{t \to \infty} f(\ma^{t}) = \lim_{j \to \infty} f(\ma^{t_j}) = f(\lim_{j \to \infty} \ma^{t_{j}}) = f(\bar{\ma}).
\end{equation}

Let us prove the stationarity of \eqn{\bar{\ma}} by contradiction. Assume that \eqn{\bar{\ma}} is not a stationary point, then there exist some other point \eqn{\ma^\prime \in \{\ma^t\}}, such that \eqn{\nabla f (\bar{\ma})^\tran (\ma^\prime - \bar{\ma}) < 0} and \eqn{f(\bar{\ma}) > f(\ma^\prime)}. Since \eqn{\ma^\prime} is a point in \eqn{\{\ma^t\}}, there exist a subsequence that converges to \eqn{\ma^\prime}, for which \eqn{\ma^\prime} is a limit point. However, by using \eqref{limpoint}, we have \eqn{f(\bar{\ma}) = f(\ma^\prime)} for all limit points of \eqn{\{\ma^t\}}, which is a contradiction to our initial assumption. 

Now, by using strict monotonicity of \eqn{\{f(\ma^{t_j})\}}, it follows that as \eqn{j \to \infty}, \eqn{\ma^{t_j} \to \bar{\ma}} and \eqn{f(\ma^{t_{j}}) - f(\ma^{t_{j+1}}) \to 0}. Moreover, since \eqn{\bar{\ma}} is the solution of \eqref{con-m}, it satisfies \eqref{strict_monotonicity} over convex subset \eqn{\mc{Q}}, which can either be \eqn{\iter{\mc{X}}{k}{i}} or \eqn{\iter{\mc{Y}}{k}{i}} as \eqn{k \to \infty, i \to \infty}. Using the above statements, we can show that \eqn{\bar{\ma}} satisfies the optimality condition  \cite[Prop. 2.1.2]{bertsekas1999nonlinear} as 
\begin{equation} \label{seq_mon1}
\nabla f (\bar{\ma})^\tran (\ma - \bar{\ma}) \geq 0, \; \fall \ma \in \mc{Q} \subset \mc{F} \eqsub.
\end{equation}
\review{Hence, \eqn{\bar{\ma}} is a stationary point and by using \eqref{limpoint}, we can show that every limit point of \eqn{\{\ma^t\}} is a stationary point. 
%In order to ensure the convergence of \eqn{\{\hat{f}(\ma^t)\}}, let us consider an instant \eqn{t} in which \eqn{\mat{t+1}} is the minimizer of \eqref{con-m}. Then, by using \eqref{mod_obj} and \eqref{strict_monotonicity}, monotonicity of \eqn{\{\hat{f}(\ma^t)\}} is shown by using \eqref{mod_obj} as
%\begin{equation} \label{mcity_origfunc}
%\hat{f}(\mat{t+1}) + \tau^t \|\mat{t+1} - \mat{t} \| \leq \hat{f}(\mat{t}) + \tau^t \|\mat{t} - \mat{t} \|
%\end{equation}
%where \eqn{\tau^t} is as in \eqref{mod_obj}. Additionally, by using Appendix \ref{c-a}, convergence of \eqn{\{\hat{f}(\ma^t)\}} can be established by \cite[Prop. A.3]{bertsekas1999nonlinear}.
}
	
\review{Before proceeding further, we ensure the convergence of \eqn{\{\hat{f}(\ma^t)\}} by showing that \eqn{\{\hat{f}(\ma^t)\}} is decreasing in every update and it is bounded. To do so, let us consider the minimizers \eqn{\mat{t}} and \eqn{\mat{t+1}} of problem \eqref{con-m} in iterations \eqn{{t-1}} and \eqn{{t}}, respectively. Since the proximal term in \eqref{mod_obj} is given as \eqn{\|\mat{} - \mat{t}\|^2} for iteration \eqn{t}, the relation between \eqn{\mat{t}} and \eqn{\mat{t+1}} satisfies
\begin{equation} \label{mcity_origfunc}
\hat{f}(\mat{t+1}) + \tau^t \|\mat{t+1} - \mat{t} \|^2 \leq \hat{f}(\mat{t}) + \tau^t \|\mat{t} - \mat{t} \|^2
\end{equation}
where \eqn{\tau^t} denotes \eqn{\tau_k^{(i)}} in \eqref{mod_obj} and \eqn{\|\mat{t+1} - \mat{t} \|^2 \geq 0}. Thus, by extending \eqref{mcity_origfunc} to every update and by using Appendix \ref{b_obj}, convergence of \eqn{\{\hat{f}(\ma^t)\}} can be shown using \cite[Prop. A.3]{bertsekas1999nonlinear}.}
	
Now to prove that \eqn{\bar{\ma}} is also a stationary point of original problem \eqref{con}, we show the equivalence between gradients of subproblems \eqref{con-m} and \eqref{con} at \eqn{\bar{\ma}}. In order to ensure \eqn{\bar{\ma}} is a \ac{KKT} point of \eqref{con}, it must satisfy the gradient condition of \eqref{con}, \textit{i.e.},
\begin{equation} \label{kkt_expr}
\textstyle	\nabla \hat{f}(\bar{\ma}) + \mu_0 \big [ \nabla h(\bar{\ma}) - \nabla {g}_0(\bar{\ma}) \big ] + \sum_{i=1}^2 \mu_i \nabla g_i(\bar{\ma}) = 0
\end{equation}
for some Lagrange multipliers \eqn{\mu_i \geq 0} and feasible as \eqn{\bar{\ma} \in \mc{F}}.

Using \eqref{con-sub-set}, we have \eqn{\mc{X}_{k}^{(i)} \subset \mc{F}} and \eqn{\mc{Y}_{k}^{(i)} \subset \mc{F}}, therefore, \eqn{\bar{\ma}} is a feasible point for \eqref{con}. Moreover, the interiors of \eqn{\mc{X}_{k}^{(i)}} and \eqn{\mc{Y}_{k}^{(i)}} are non-empty, therefore, Slater's constraint qualification holds for \eqref{con-m}. Since \eqn{\bar{\ma}} is the solution for \eqref{con-m} as \eqn{k \to \infty,i \to \infty}, there exist Lagrange multipliers \eqn{\mu_i \geq 0} that  satisfies
\begin{equation} \label{kkt_mod_expr}
\textstyle \nabla f(\bar{\ma}) + \mu_0 \big [ \nabla h(\bar{\ma}) - \nabla\hat{g}_0(\bar{\ma};\bar{\ma}) \big ] + \sum_{i=1}^2 \mu_i \nabla g_i(\bar{\ma}) = 0.
\end{equation}

The relation between \eqref{kkt_expr} and \eqref{kkt_mod_expr} is evident by using following facts: (i) The quadratic term in \eqref{mod_obj} \eqn{\|\ma^{t_{j+1}} - \ma^{t_j}\|^2 \to 0}  as \eqn{j \to \infty, \ma^{t_j} \to \bar{\ma}}, since we assume that \eqn{\bar{\ma}} is the limit point of a convergent subsequence \eqn{\{\ma^{t_j}\}}. Therefore, the gradient evaluated at \eqn{\bar{\ma}} satisfies \eqn{\nabla {f}(\bar{\ma}) = \nabla \hat{f}(\bar{\ma})}. (ii) Additionally, by using the continuity of function \eqn{g_o} and \eqref{con-relax}, equivalence between the gradient of \eqref{con-dc} and \eqref{con-dc-m} is obtained as \eqn{\ma^{t_j} \to \bar{\ma}}
\begin{equation} \label{rhs_equiv}
\nabla h(\bar{\ma}) - \nabla \hat{g}_o(\bar{\ma};\bar{\ma}) = \nabla h(\bar{\ma}) - \nabla g_o(\bar{\ma}).
\end{equation}
Now, by applying the relation \eqref{rhs_equiv} and \eqn{\nabla {f}(\bar{\ma}) = \nabla \hat{f}(\bar{\ma})} in \eqref{kkt_mod_expr}, we can show that the limit point \eqn{\bar{\ma}} satisfies \eqref{kkt_expr}. Finally, by using \cite[Thms. 2 and 11]{scutari_1} and \cite[Prop. 3.2]{amir}, we can show that every limit point of sequence of iterates \eqn{\{\ma^t\}} generated by the iterative algorithm is a stationary point of problem \eqref{con}.}








