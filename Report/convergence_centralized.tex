To prove the convergence of the transmit, receive beamformers, and the objective values of the centralized algorithms in \eqref{eqn-6} and \eqref{eqn-mse-1}, we need to show that the following conditions are satisfied by the centralized formulations.
\begin{itemize}
	\item[(a)] The function should be bounded below
	\item[(b)] The feasible set should be a compact set
	\item[(c)] The sequence of the objective values should be decreasing in each iteration
	\item[(d)] Uniqueness of the minimizer, \textit{i.e}, the transmit and the receive beamformers should be unique in each iteration.
\end{itemize}
The conditions (a) (b) and (c) are required for the convergence of the objective values generated by the iterative algorithm. Since the feasible set is not fixed in each iteration, we require the condition (d) to prove the convergence of the objective function and the corresponding arguments namely, the transmit and the receive precoders to a stationary point \cite{meyer1976sufficient}. Assuming the conditions (a), (b) and (c) are satisfied, using \cite[Th. 3.14]{rudin1964principles}, we can show that the bounded monotonically decreasing objective value sequence has a unique minimum. Finally, we use the discussions in \cite{marks1978technical,lanckriet2009convergence,scutari_1} to show that the limit point of the iterative algorithm is indeed a stationary point of the nonconvex problem in \eqref{eqn-6} and \eqref{eqn-mse-1}.

\subsection{Bounded Objective Function and Compact Feasible Set}
The feasible sets of the problems \eqref{eqn-6} and \eqref{eqn-mse-1} are bounded and closed, which can be verified by the total power constraint on the transmit precoders \eqref{eqn-6.4}, and therefore, a compact set. 

The minimum value of the norm operator in the objective is zero, \textit{i.e}, \me{\|x\|_q > -\infty}, therefore it is bounded below. The objective function is Lipschitz continuous over the feasible set, and therefore, it is bounded from above as well, since the feasible set is bounded. %The objective function in \eqref{eqn-6} and \eqref{eqn-mse-1} is continuous and approaches \me{\infty} as \me{\mbf{t}_k \rightarrow \infty}, and therefore it is coercive in the feasible set. %Using conditions (a) and (b), we can guarantee the existence of a minimizer to the problem in \eqref{eqn-6} and \eqref{eqn-mse-1}.

\subsection{Monotonicity of the Objective Sequence} \label{mcity}
In this section, we show the nonincreasing behavior of the objective values in each \ac{SCA} and \ac{AO} update. Let us consider a general formulation for the problems in \eqref{eqn-6} and \eqref{eqn-mse-1} as
\begin{subeqnarray} \label{con}
	\underset{\mx,\my,\mz}{\text{minimize}} &\quad& f(\mx,\my,\mz) \eqsub \slabel{con-obj} \\
	\text{subject to} &\quad& h(\mz) - g_0(\mx,\my) \leq 0 \eqsub \slabel{con-dc} \\
	&\quad& g_1(\mx,\my) \leq 0 \eqsub \slabel{con-cvx-blk} \\
	&\quad& g_2(\mx) \leq 0 \eqsub \slabel{con-cvx}
\end{subeqnarray}
where \me{g_2,f} are convex and \me{h} is a linear function. Let \me{g_0,g_1} be convex in either \me{\mx} or \me{\my} but not on both. The constraint in \eqref{con-dc} corresponds to \eqref{eqn-6.2} or \eqref{eqn-mse-1.2} and the constraint \eqref{con-cvx-blk} correspond to \eqref{eqn-6.3} or \eqref{eqn-mse-1.3}. Other convex constraints are addressed by \eqref{con-cvx} and the feasible set of \eqref{con} is given by 
\iftoggle{single_column}{
\begin{equation}
\mc{F} = \{ \; \mx,\my,\mz \; \big | h(\mz) - g_0(\mx,\my) \leq 0, g_1(\mx,\my) \leq 0, g_2(\mx) \leq 0 \}.
\end{equation}
}{\begin{align}
\mc{F} = \{ \; \mx,\my,\mz \; \big | & \quad h(\mz) - g_0(\mx,\my) \leq 0, \nonumber \\
								 & \quad g_1(\mx,\my) \leq 0, g_2(\mx) \leq 0 \}.
\end{align}}

To solve \eqref{con}, we adopt \ac{AO} by fixing a block of variables and optimize for others \cite{bezdek2002some}. In \eqref{con}, even after fixing the variable \me{\my}, the problem is nonconvex due to the \ac{DC} constraint \eqref{con-dc}. We adopt \ac{SCA} presented in \cite{lipp2014variations,lanckriet2009convergence,scutari_1} by relaxing the nonconvex set by a sequence of convex subsets. Since it involves two nested iterations, we denote the \ac{AO} iteration index by a superscript \me{(i)} and the \ac{DC} constraint relaxations by a subscript \me{k}. Let \me{\iter{\mx}{\ast}{i}} be the value of \me{\mx} when the \ac{SCA} iteration converges in the \me{\ith{i}} \ac{AO} iteration and let \me{\iter{\mz}{\ast|\my}{i}} be the solution for \me{\mz} obtained in the \me{\ith{i}} \ac{AO} iteration for a fixed \me{\my}. 

To begin with, let us consider the variable \me{\my} is fixed for the \ac{AO} \me{i} with the optimal value achieved from the previous iteration \me{i-1} as \me{\iter{\my}{\ast}{i-1}}. In order to solve for \me{\mx} in the \ac{SCA} iteration \me{k}, we linearize the nonconvex function \me{g_0} using previous \ac{SCA} iterate \me{\iter{\mx}{k}{i}} of \me{\mx} as
\iftoggle{single_column}{
\begin{equation}\label{con-relax}
\hat{g}_o(\mx,\iter{\my}{\ast}{i-1};\iter{\mx}{k}{i}) = {g}_0(\iter{\mx}{k}{i},\iter{\my}{\ast}{i-1}) + \nabla g_0(\iter{\mx}{k}{i},\iter{\my}{\ast}{i-1})^{\mathrm{T}} (\mx - \iter{\mx}{k}{i}).
\end{equation}}{
\begin{multline} \label{con-relax}
\hat{g}_o(\mx,\iter{\my}{\ast}{i-1};\iter{\mx}{k}{i}) = {g}_0(\iter{\mx}{k}{i},\iter{\my}{\ast}{i-1}) \\ + \nabla g_0(\iter{\mx}{k}{i},\iter{\my}{\ast}{i-1})^{\mathrm{T}} (\mx - \iter{\mx}{k}{i}).
\end{multline}}
Let \me{\iter{\mc{X}}{k}{i}} be the feasible set for the \eqn{\ith{i}} \ac{AO} iteration and the \me{\ith{k}} \ac{SCA} point for a fixed \me{\iter{\my}{\ast}{i-1}} and \me{\iter{\mx}{k}{i}}. Similarly, \me{\iter{\mc{Y}}{k}{i}} denotes the feasible set for a fixed \me{\iter{\mx}{\ast}{i}} and \me{\iter{\my}{k}{i}}. Using \eqref{con-relax}, the convex subproblem for \me{\ith{i}} \ac{AO} iteration and \me{\ith{k}} \ac{SCA} point for the variable \me{\mx} and \me{\mz} is given by
\begin{subeqnarray} \label{con-m}
	\underset{\mx,\mz}{\text{minimize}} && f(\mx,\iter{\my}{\ast}{i-1},\mz) \eqsub \slabel{con-obj-m} \\
	\text{subject to} && h(\mz) - \hat{g}_0(\mx,\iter{\my}{\ast}{i-1};\iter{\mx}{k}{i}) \leq 0 \eqsub \slabel{con-dc-m} \\
	&& g_1(\mx,\iter{\my}{\ast}{i-1}) \leq 0, \quad g_2(\mx) \leq 0 \eqsub \slabel{con-cvx-blk-m}
\end{subeqnarray}
The feasible set defined by the problem in \eqref{con-m} is denoted as \me{\iter{\mc{X}}{k}{i} \subset \mc{F}}. To prove the convergence of the \ac{SCA} updates in the \me{\ith{i}} \ac{AO} iteration, let us consider that \eqref{con-m} yields 
\me{\iter{\mx}{k+1}{i}} and \me{\iter{\mz}{k+1}{i}} as the solution in the \me{\ith{k}} iteration. The point \me{\iter{\mx}{k+1}{i}} and \me{\iter{\mz}{k+1}{i}}, which minimizes the objective function satisfies
\iftoggle{single_column}{
\begin{equation}\label{con-sub-set}\allowdisplaybreaks
h(\iter{\mz}{k+1}{i}) - {g}_0(\iter{\mx}{k+1}{i},\iter{\my}{\ast}{i-1}) \leq - \hat{g}_0(\iter{\mx}{k+1}{i},\iter{\my}{\ast}{i-1};\iter{\mx}{k}{i}) + h(\iter{\mz}{k+1}{i}) \leq h(\iter{\mz}{k}{i}) - \hat{g}_0(\iter{\mx}{k}{i},\iter{\my}{\ast}{i-1};\iter{\mx}{k}{i}) \leq 0. 
\end{equation}}{
\begin{multline}\label{con-sub-set}\allowdisplaybreaks
h(\iter{\mz}{k+1}{i}) - {g}_0(\iter{\mx}{k+1}{i},\iter{\my}{\ast}{i-1}) \leq - \hat{g}_0(\iter{\mx}{k+1}{i},\iter{\my}{\ast}{i-1};\iter{\mx}{k}{i}) \\ + h(\iter{\mz}{k+1}{i}) \leq h(\iter{\mz}{k}{i}) - \hat{g}_0(\iter{\mx}{k}{i},\iter{\my}{\ast}{i-1};\iter{\mx}{k}{i}) \leq 0. 
\end{multline}}
Using \eqref{con-sub-set}, we can show that \me{\{\iter{\mx}{k+1}{i},\iter{\my}{\ast}{i-1}, \iter{\mz}{k+1}{i}\}} is feasible, since the initial \ac{SCA} operating point \me{\iter{\mx}{\ast}{i-1}} was chosen to be feasible from the \me{\ith{i-1}} \ac{AO} iteration. At each \ac{SCA} update, the feasible set includes the solution from the previous iteration as \me{\{\iter{\mx}{k+1}{i},\iter{\my}{\ast}{i-1},\iter{\mz}{k+1}{i}\} \in \iter{\mc{X}}{k+1}{i} \subset \mc{F}}, therefore, it decreases the objective as \cite{lanckriet2009convergence,scutari_1,quoc2011sequential}
\iftoggle{single_column}{
\begin{equation} \label{con-convergence}
f(\iter{\mx}{0}{i},\iter{\my}{\ast}{i-1},\iter{\mz}{0}{i}) \geq f(\iter{\mx}{k}{i},\iter{\my}{\ast}{i-1},\iter{\mz}{k}{i}) \geq f(\iter{\mx}{k+1}{i},\iter{\my}{\ast}{i-1},\iter{\mz}{k+1}{i}) \geq f(\iter{\mx}{\ast}{i},\iter{\my}{\ast}{i-1},\iter{\mz}{\ast|\my}{i}). 
\end{equation}}{
\begin{multline} \label{con-convergence}
f(\iter{\mx}{0}{i},\iter{\my}{\ast}{i-1},\iter{\mz}{0}{i}) \geq f(\iter{\mx}{k}{i},\iter{\my}{\ast}{i-1},\iter{\mz}{k}{i}) \\ \geq f(\iter{\mx}{k+1}{i},\iter{\my}{\ast}{i-1},\iter{\mz}{k+1}{i}) \geq f(\iter{\mx}{\ast}{i},\iter{\my}{\ast}{i-1},\iter{\mz}{\ast|\my}{i}). 
\end{multline}}
Thus the sequence \me{f(\iter{\mx}{k}{i},\iter{\my}{\ast}{i-1},\iter{\mz}{k}{i})} is nonincreasing and approaches a limit point of the \ac{SCA} as \me{k \rightarrow \infty}. The equality in \eqref{con-convergence} is achieved upon the \ac{SCA} convergence. The feasible point \me{\{\iter{\mx}{\ast}{i}, \iter{\my}{\ast}{i-1},\iter{\mz}{\ast|\my}{i}\}} need not to be a stationary point of \eqref{con}, since it is a minimizer over the set \me{\iter{\mc{X}}{\ast}{i} \subset \mc{F}}.

Once the solution is found for a fixed \me{\my}, it is then optimized by fixing \me{\mx} as \me{\iter{\mx}{\ast}{i}}. Even after treating \me{\mx} as a constant, the problem is nonconvex due to the \ac{DC} constraint. Following similar approach as above, we can find a minimizer \me{ \{\iter{\mx}{\ast}{i},\iter{\my}{k+1}{i},\iter{\mz}{k+1}{i}\}} for a similar convex subproblem \eqref{con-m} at each iteration \me{{k}}. Note that \me{\iter{\mz}{k+1}{i}} is reused since the variable \me{\mx} is fixed in the \me{\ith{i}} \ac{AO} iteration. The convergence and the nonincreasing behavior of the objective follows similar arguments as above.\footnote{Note that we can also use the \ac{MMSE} receiver in \eqref{eqn-10} instead of performing the \ac{SCA} updates until convergence for the optimal receiver.} The limit point of the converged \ac{SCA} subproblems with \me{\my} as a variable is given by \me{\{\iter{\mx}{\ast}{i}, \iter{\my}{\ast}{i},\iter{\mz}{\ast|\mx}{i}\}}, which is a minimizer in the set \me{\iter{\mc{Y}}{\ast}{i} \subset \mc{F}}.

Finally, to prove the global convergence of the iterative algorithm, we need to show the nonincreasing behavior of the objective function in between the \ac{AO} updates, \textit{i.e}, 
\iftoggle{single_column}{
\begin{equation}\allowdisplaybreaks
f(\iter{\mx}{\ast}{i},\iter{\my}{\ast}{i},\iter{\mz}{\ast|\mx}{i}) \leq f(\iter{\mx}{\ast}{i},\iter{\my}{0}{i},\iter{\mz}{0}{i}) \leq f(\iter{\mx}{\ast}{i},\iter{\my}{\ast}{i-1},\iter{\mz}{\ast|\my}{i}).
\end{equation}}{
\begin{multline}\allowdisplaybreaks
f(\iter{\mx}{\ast}{i},\iter{\my}{\ast}{i},\iter{\mz}{\ast|\mx}{i}) \leq f(\iter{\mx}{\ast}{i},\iter{\my}{0}{i},\iter{\mz}{0}{i}) \\
\leq f(\iter{\mx}{\ast}{i},\iter{\my}{\ast}{i-1},\iter{\mz}{\ast|\my}{i}).
\end{multline}}
Let \me{\iter{\mx}{\ast}{i}} and \me{\iter{\mz}{\ast|\my}{i}} be the solution obtained by solving \eqref{con-m} iteratively until \ac{SCA} convergence in the \me{\ith{i}} \ac{AO} iteration for \eqn{\mx} and \eqn{\mz} with fixed \eqn{\my = \iter{\my}{\ast}{i-1}}. To find \me{\iter{\my}{0}{i}}, we fix \eqn{\mx} as \me{\iter{\mx}{\ast}{i}} and optimize for \me{\my}. Since the convex function is linearized in \eqref{con-dc}, the fixed operating point is also included in the feasible set \eqn{\{\iter{\mx}{\ast}{i},\iter{\my}{\ast}{i-1},\iter{\mz}{\ast|\my}{i}\} \in \iter{\mc{Y}}{0}{i}} by following \eqref{con-sub-set}. Using this, we can show the monotonicity of the objective sequence as
\begin{equation} \label{fixed_point}
f(\iter{\mx}{\ast}{i},\iter{\my}{0}{i},\iter{\mz}{0}{i}) \leq f(\iter{\mx}{\ast}{i},\iter{\my}{\ast}{i-1},\iter{\mz}{\ast|\mx}{i}).
\end{equation}
The \ac{AO} update follows \eqn{\{\iter{\mx}{\ast}{i},\iter{\my}{\ast}{i-1},\iter{\mz}{\ast|\my}{i}\} \in \{ \iter{\mc{X}}{\ast}{i} \cap \iter{\mc{Y}}{0}{i} \}}. Using (a),(b) and (c), we can guarantee the convergence of the objective value sequence generated by the iterative algorithm.

\subsection{Uniqueness of the Beamformer Iterates}
The uniqueness of the iterates \eqn{\{\iter{\mx}{k}{i},\iter{\my}{\ast}{i-1},\iter{\mz}{k|\my}{i}\}} are guaranteed for the \ac{MSE} reformulated problem in \eqref{eqn-mse-2} when all the constraints are active. However when \eqn{f(\iter{\mx}{k}{i},\iter{\my}{\ast}{i-1},\iter{\mz}{k|\my}{i}) = 0} after some iteration, say \me{k} and \me{i}, the uniqueness of the iterates are not guaranteed since there are many solutions satisfying the constraints without affecting the objective value. Similarly, when the objective is non-zero, problem defined in \eqref{eqn-9} has a unique minimizer if the receivers are matched to the corresponding transmit beamformers, \textit{i.e}, \me{q_{l,k,n} = 0} in \eqref{eqn-wsrm-expr},  while choosing the feasible operating point \me{\iter{\mx}{0}{0}} and \me{\iter{\my}{0}{0}} to begin the iterative procedure.

In general, to ensure the uniqueness of the transmit precoders and the receive beamformers in all iterations, \textit{i.e}, even when the objective value is zero, the convex subproblems in \eqref{eqn-9} and \eqref{eqn-mse-2} can be regularized with a strongly convex function. Now, the objective function of the \me{\ith{i+1}} iteration is given as
\begin{equation}
\| \tilde{\mbf{v}} \|_q + c \, \| \mbf{x} - \mbf{x}^{(i)} \|^2
\end{equation}
where \me{\mbf{x}} is the vector containing all the optimization variables and \me{c} is a positive constant to make the objective strongly convex. The uniqueness of the minimizer is guaranteed by the strongly convex objective function \cite{scutari_1}. The vector \me{\mbf{x}^{(i)}} is the value of \me{\mbf{x}} obtained from the \me{\ith{i}} iteration.

%The uniqueness of the transmit precoders and the receive beamformers for the \ac{MSE} reformulation in \eqref{eqn-mse-2} can be guaranteed in each iteration by the \ac{MSE} constraint in \eqref{eqn-mse-1.3}. The uniqueness of the iterates for the problem in \eqref{eqn-9} can be guaranteed in each convex subproblem if the receivers are matched to the equivalent downlink channel with the initially chosen feasible transmit precoders, \textit{i.e}, \me{q_{l,k,n} = 0 \: \forall \, l,k,n} in \eqref{eqn-wsrm-expr} while beginning the \ac{SCA} iteration.
%
%When the objective is zero, the uniqueness of the transmit and the receive beamformers are not guaranteed in both problem formulations, since they are the under-estimators and need not be active at the minimum. To obtain a unique set of transmit precoders and the receive beamformers in all scenarios, we can regularize the objective with a strongly convex function as
%\begin{equation}
%\| \tilde{\mbf{v}} \|_q + c \, \| \mbf{x} - \mbf{x}^{(i)} \|^2
%\end{equation}
%where \me{\mbf{x}} is the vector containing all the optimization variables and \me{c} is a positive constant. The vector \me{ \mbf{x}^{(i)}} is the value of \me{\mbf{x}} in the \me{\ith{i}} iteration. %Note that the uniqueness of the transmit precoders and the receive beamformers for each convex subproblem can be shown by using the constraint \eqref{eqn-8} for the problem \eqref{eqn-9} and \eqref{eqn-mse-1.3} for the \ac{MSE} reformulation in \eqref{eqn-mse-2}. The receive beamformers \me{\mvec{w}{l,k,n}} in \eqref{eqn-10} are also unique for the given set of transmit precoders, since the convex subproblems in \eqref{eqn-9} and \eqref{eqn-mse-2} are susceptible to the transmit precoder phase rotations, \textit{i.e}, unitary transformations. When the objective function is zero, the uniqueness of the transmit precoders are not guaranteed by using \eqref{eqn-8} and \eqref{eqn-mse-1.3} constraints, since they are not active at the optimal solution of the subproblems. To obtain unique set of transmit precoders when the objective is zero even for a single \ac{BS}, we can regularize the objective with the transmit power as discussed in Appendix \ref{a-3} without affecting the optimal value.

\subsection{Convergence of the Beamformer Iterates} \label{c-b}
So far, we have considered the convergence of the objective sequence. In this section, we discuss the convergence of the beamformer iterates generated by the iterative algorithm for \eqref{con} using \cite{zangwill1969nonlinear,meyer1976sufficient}. Let \me{\ma \triangleq [\iter{\mx}{\ast}{i},\iter{\my}{\ast}{i},\iter{\mz}{\ast|\mx}{i}]} be the stacked vector of the solution point in the \eqn{\ith{i}} \ac{AO} iteration and let \me{\mc{A}} be the point-to-set mapping algorithm defined as
\begin{equation} \label{algorithm_fixed}
\iterate{\ma}{i+1} \in \mc{A}(\iterate{\ma}{i}) : \underset{\ma}{\mathrm{argmin}} \; f(\ma;\iterate{\ma}{i}) \; \forall \ma \in \iter{\mc{Y}}{\ast}{i} \subset \mc{F}.
\end{equation}
Since the algorithm finds a unique solution in each iteration,\footnote{Index \me{i} denotes the \ac{AO} iteration to update the vector \me{\ma}, which involves two \ac{SCA} iterations performed until convergence, \textit{i.e}, one for the variable \me{\mx} by keeping \me{\my} fixed and another for the variable \me{\my} by fixing \me{\mx} as constant.} the mapping denoted by \me{\iterate{\ma}{i+1} = \mc{A}(\iterate{\ma}{i})}. Let \me{\mc{F}^\ast \subset \mc{F}} be the set of fixed points, \textit{i.e}, \me{\forall {\ma}^{\ast} \in \mc{F}^\ast, \, {\ma}^{\ast} = \mc{A}({\ma}^{\ast})}, identified by the algorithm \me{\mc{A}} for different initialization points.

Using \cite[Th. 3.1]{meyer1976sufficient}, convergence of the iterates to a fixed point can be shown, if the following conditions are satisfied.
\begin{itemize}
\item Objective function \me{f} should be bounded and continuous.
\item The feasible set \me{\iter{\mc{Y}}{\ast}{i}} in each iteration should be compact.
\item The sequence \me{\{\iterate{\ma}{i}\}} generated by the algorithm \me{\mc{A}} should be strictly monotonic with respect to the objective function \me{f}, \textit{i.e}, \me{\ma^\prime = \mc{A}(\ma)} implies \me{f(\ma^\prime) < f(\ma)} whenever \me{\ma} is not a fixed point.
\end{itemize}
Note that the above conditions are satisfied by the proposed algorithm \me{\mc{A}} in \eqref{algorithm_fixed} and the strict monotonicity can also be verified from \eqref{fixed_point}. Using \cite{zangwill1969nonlinear} and \cite[Th. 3.1]{meyer1976sufficient}, we can show that (i) all limit points will be a fixed point, (ii) \eqn{f(\iterate{\ma}{i}) \rightarrow f(\ma^\ast)}, where \eqn{\ma^\ast} is a fixed point, and (iii) \eqn{\ma^\ast} is a regular point, \textit{i.e}, \eqn{\|\iterate{\ma}{i} - \mc{A}(\iterate{\ma}{i}) \| \rightarrow 0}. Even though the fixed points in the set \me{\mc{F}^\ast} can be achieved by the algorithm \me{\mc{A}} for different initial points, the algorithm \me{\mc{A}} achieves a unique limit point, \textit{i.e}, \me{\mc{F}^\ast = \{\ma^\ast\}}, once a feasible point is chosen to be the operating point while initializing \me{\mc{A}}.

\begin{comment}
and \me{\mc{A}} be a point-to-set mapping algorithm from \me{\mc{F}} into the nonempty subsets of \me{\mc{F}}, \textit{i.e}, \eqn{\iterate{\ma}{i+1} \in \mc{A}(\iterate{\ma}{i})}. The set of accumulation or the limit points in the compact set \me{\mc{F}} be denoted by \me{\mc{F}^\ast} and the objective function be \me{f} is closed and continuous. Let \me{\{\iterate{\ma}{i}\}} be the sequence of iterates generated by the algorithm \me{\mc{A}} and the objective function \me{f}. If the mapping is strictly monotonic and uniformly compact, then the following conditions hold \cite{zangwill1969nonlinear} and \cite[Theorem 3.1]{meyer1976sufficient}.
\begin{itemize}
\item[(i)] all accumulation points will be a fixed point,
\item[(ii)] \eqn{f(\iterate{\ma}{i}) \rightarrow f(\ma^\ast)}, where \eqn{\ma^\ast} is a fixed point,
\item[(iii)] \eqn{\ma^\ast} is a regular point, \textit{i.e} \eqn{\|\iterate{\ma}{i} - \iterate{\ma}{i+1} \| \rightarrow 0}.
\end{itemize}

To show that the proposed iterative method satisfies the above conditions,  the mapping \eqn{\mc{A}} for the iterative algorithm is given by
\begin{equation}
\iterate{\ma}{i+1} \in \mc{A}(\iterate{\ma}{i}) : \underset{\ma}{\mathrm{argmin}} \; f(\ma;\iterate{\ma}{i}) \; \forall \ma \in \iterate{\mc{X}}{i} \subset \mc{F}
\end{equation}
where \me{\mc{A}(\iterate{\ma}{i})} denotes the set of iterates in the subset \me{\iterate{\mc{X}}{i}}, which has the same objective value under the mapping \me{f} in the \me{\ith{i}} iteration The monotonicity of the proposed algorithm is guaranteed based on the arguments presented in Appendix \ref{mcity}. The mapping is strictly monotonic on \me{\mc{F}} with respect to the objective function as \me{\ma^\prime \in \mc{A}(\ma)} implies \me{f(\ma^\prime) < f(\ma)} whenever \me{\ma} is not a fixed point, \textit{i.e}, \me{\ma \notin \mc{F}^\ast}, as in \eqref{fixed_point}. Using the above discussions, we can show that the sequence \me{\{\iterate{\ma}{i}\}} converges to a fixed point in the set \me{\mc{F}^\ast} for any feasible starting point \me{\iterate{\ma}{0} \in \mc{F}}

The sequence \me{\{\iterate{\ma}{i}\}} converges to the set of fixed points \me{\mc{F}^\ast} for any feasible starting point \me{\iterate{\ma}{0} \in \mc{F}}, therefore the mapping is uniformly compact.

To show the set of fixed points \me{\mc{F}^\ast} are the minimizers for the objective function \me{f}, we rely on the strict monotonicity of the objective function for each iteration. Since \me{f(\mc{A}(\iterate{\ma}{i})) < f(\iterate{\ma}{i})}, the objective decreases monotonically for each iteration and the equality is achieved when \me{\ma^\ast \in \mc{A}(\ma^\ast)}, which is a generalized fixed point. Therefore, the fixed points in \me{\mc{F}^\ast} are the minimizers for the objective function \me{f} over the set \me{\mc{F}}.

If the uniqueness of the iterates are guaranteed, then the algorithm will find a unique solution at each iteration as \me{\iterate{\ma}{i+1} = \mc{A}(\iterate{\ma}{i})} and the accumulation point is unique. The convergence of the iterates to a single fixed point is guaranteed by the discussions in \cite{zangwill1969nonlinear,meyer1976sufficient}. Even though the algorithm finds a single fixed point, all fixed points in \me{\mc{F}^\ast} are equally valid as a minimizer for the function \me{f} in \eqref{con}, since the \ac{SINR} in \eqref{eq:SINR} is invariant to the unitary rotations on the beamformers.
\end{comment}

\subsection{Stationary Points of the Nonconvex Problem}
To show the limit point of the sequence of iterates \me{\{\iterate{\ma}{i}\}} is a stationary point, it must satisfy the \ac{KKT} conditions of the problem in \eqref{con}. As \me{i \rightarrow \infty}, the objective converges as
\iftoggle{single_column}{
\begin{equation} \label{con-opt}\allowdisplaybreaks
f(\iter{\mx}{\ast}{i},\iter{\my}{\ast}{i},\iter{\mz}{\ast|x}{i}) = f(\iter{\mx}{\ast}{i+1},\iter{\my}{\ast}{i},\iter{\mz}{\ast|y}{i+1}) = 
f(\iter{\mx}{\ast}{i+1},\iter{\my}{\ast}{i+1},\iter{\mz}{\ast|x}{i+1})
\end{equation}}{
\begin{multline} \label{con-opt}\allowdisplaybreaks
f(\iter{\mx}{\ast}{i},\iter{\my}{\ast}{i},\iter{\mz}{\ast|x}{i}) = f(\iter{\mx}{\ast}{i+1},\iter{\my}{\ast}{i},\iter{\mz}{\ast|y}{i+1}) \\ = 
f(\iter{\mx}{\ast}{i+1},\iter{\my}{\ast}{i+1},\iter{\mz}{\ast|x}{i+1})
\end{multline}}
where \me{\{\iter{\mx}{\ast}{i+1},\iter{\my}{\ast}{i+1},\iter{\mz}{\ast|x}{i+1}\} \in \mc{F}^\ast} is the limit point of \me{\mc{A}} for a given initial point. As discussed in Appendix \ref{c-b}, the unique limit point of the sequence of iterates \me{\{\iterate{\ma}{i}\}} is a fixed point of \me{\mc{A}}. Using \cite[Th. 10]{lanckriet2009convergence} and \cite[Th. 2 and 11]{scutari_1}, we can show that the fixed points obtained by the algorithm \me{\mc{A}} under the mapping \me{f} is a stationary point of the nonconvex problem \eqref{con}. It can be verified by writing the \ac{KKT} expressions for the original problem in \eqref{con} and the relaxed one in \eqref{con-m} at the limit point \eqref{con-opt}, using the linear relaxation \eqref{con-relax} for the nonconvex constraint \cite{marks1978technical}.

