
\newcommand{\mx}{\mbf{x}}
\newcommand{\my}{\mbf{y}}
\newcommand{\mz}{\mbf{z}}

\newcommand{\mxb}{{\mbf{x}}}
\newcommand{\myb}{{\mbf{y}}}
\newcommand{\iterate}[2]{{#1}^{(#2)}}

Let us write the \ac{JSFRA} problem in \eqref{eqn-6} and \eqref{eqn-mse-1} as
\begin{IEEEeqnarray}{RCL} \label{con}
\underset{\mx,\my,\mz}{\text{minimize}} &\quad& f(\mz) \eqsub \label{con-obj} \\
\text{subject to} &\quad& h(\mz) - g_0(\mx,\my) \leq 0 \eqsub \label{con-dc} \\
&\quad& g_1(\mx,\my) \leq 0, \eqsub \label{con-cvx-blk} \\
&\quad& g_2(\mx) \leq 0, \eqsub \label{con-cvx}
\end{IEEEeqnarray}
wher \me{g_2,f} are convex functions and \me{h} is a linear function. Let \me{g_0,g_1} are convex functions only on \me{\mx} or \me{\my} as the variable but not on both. Note that the \eqref{con-dc} correspond to the constraints in \eqref{eqn-6.2} and \eqref{eqn-mse-1.2} and \eqref{con-cvx-blk} corresponds to the constraints in \eqref{eqn-6.3} and \eqref{eqn-mse-1.3}. Other convex constraints are addressed by the constraint \eqref{con-cvx}. With this, the feasible set of the problem \eqref{con} is given by 
\begin{IEEEeqnarray}{rl}
\mc{F} &= \{ \mx,\my,\mz | h(\mz) - g_0(\mx,\my) \leq 0, g_1(\mx,\my) \leq 0, g_2(\mx) \leq 0\} \eqspace
\end{IEEEeqnarray}

Before proceeding further, let us define the following notations. Let \me{\iterate{\mx}{k}} denote the \me{\ith{k}} \ac{BCDM} operating point and \me{\mx_k} represent the \me{\ith{k}} \ac{SCA} approximation point. Since the problem \eqref{con} is nonconvex, it is not guaranteed to find an optimal solution. In order to solve for a suboptimal solution, let us resort to the \ac{BCDM} by fixing a block of variables and optimize for others. Since \ac{BCDM} is an iterative method, let us initialize the variable \me{\my} as \me{\iterate{\myb}{0} \in \mc{F}}. Now the problem in \eqref{con} is still nonconvex due to the constraint \eqref{con-dc}, which is now in \ac{DC} form. Following the approach presented in \cite{lipp2014variations,lanckriet2009convergence}, the function \me{g_0(\mx,\iterate{\myb}{0})} is majorized around \me{\mxb_0 \in \mc{F}} as
\begin{equation} \label{con-relax}
\hat{g}_o(\mx,\iterate{\myb}{0};\mxb_0) = {g}_0(\mxb_0,\iterate{\myb}{0}) + \nabla g_0(\mxb_0,\iterate{\myb}{0})^\mathrm{T} (\mx - \mx_0).
\end{equation}
Using the relaxation \eqref{con-relax}, the convex subproblem for the \me{\ith{i}} \ac{BCDM} iterate of \me{\myb} and the \me{\ith{k}} linear approximation of \me{\mxb} as
\begin{IEEEeqnarray}{RCL} \label{con-m}
	\underset{\mx,\my,\mz}{\text{minimize}} &\quad& f(\mz) \eqsub \label{con-obj-m} \\
	\text{subject to} &\quad& h(\mz) - \hat{g}_0(\mx,\iterate{\myb}{i};\mx_k) \leq 0 \eqsub \label{con-dc-m} \\
	&\quad& g_1(\mx,\iterate{\myb}{i}) \leq 0, \eqsub \label{con-cvx-blk-m} \\
	&\quad& g_2(\mx) \leq 0, \eqsub \label{con-cvx-m}
\end{IEEEeqnarray}
Let the set defined by the problem in \eqref{con-relax} be represented as \me{\iterate{\mc{F}}{i}_k}. In order to prove the convergence of the convex subproblems \eqref{con-relax} for a fixed \me{\iterate{\myb}{i}}, let us assume that \eqref{con-relax} yields \me{\mx_{k+1}} as the final solution at the \me{\ith{k}} iteration. To show that \me{\mx_{k+1}} minimizes the objective function, let us assume that the previous point \me{\mx_k} is feasible around which the relaxation is made. It can be seen that 
\begin{equation*}
h(\mz) - {g}_0(\mx,\iterate{\myb}{i}) \leq h(\mz) - \hat{g}_0(\mx,\iterate{\myb}{i};\mxb_k) \leq 0
\end{equation*}











\begin{comment}
In order to prove the convergence of the proposed iterative algorithm, following conditions are to be satisfied \cite{scutari}
\begin{itemize}
	\item convergence of the \ac{SCA} subproblem	
	\item uniqueness of the transmit and the receive beamformers
	\item monotonic convergence of the objective function
\end{itemize}
In the proposed solution, we replaced \eqref{eqn-6.2} by a convex constraint using the first order approximation, which is majorized by the quadratic-over-linear function in \eqref{eqn-6.2} from below around a fixed point \me{\tilde{\mbf{u}}^{(i)}_{l,k,n}}. Since the \ac{SCA} method is adopted in the proposed algorithm, the constraint approximation satisfies the following conditions as in \cite{marks1978technical}
\begin{subeqnarray} \label{sca-req}
	f(\tilde{\mbf{u}}_{l,k,n}) &\leq& \bar{f}(\tilde{\mbf{u}}_{l,k,n},\tilde{\mbf{u}}^{(i)}_{l,k,n}) \\
	f(\tilde{\mbf{u}}^{(i)}_{l,k,n}) &=& \bar{f}(\tilde{\mbf{u}}^{(i)}_{l,k,n},\tilde{\mbf{u}}^{(i)}_{l,k,n}) \\
	\nabla f(\tilde{\mbf{u}}^{(i)}_{l,k,n}) &=& \nabla \bar{f}(\tilde{\mbf{u}}^{(i)}_{l,k,n},\tilde{\mbf{u}}^{(i)}_{l,k,n}),
\end{subeqnarray}
where \me{\bar{f}(\mbf{x},\mbf{x}^{(i)})} is the approximate function of \me{f(\mbf{x})} around the point \me{\mbf{x}^{\ast(i)}}. The stationary point of the relaxed convex problem satisfies the \ac{KKT} conditions of the original nonconvex problem, which can be obtained by using conditions in \eqref{sca-req}. It can be seen that the  \ac{SCA} relaxed formulation converges to a local stationary point at each iteration.

The uniqueness of the transmit and the receive beamformers can be justified by forcing one antenna to be real valued to exclude the phase ambiguity arising from the complex precoders. The monotonic convergence of the objective function can be justified by the following arguments. At each \ac{SCA} iteration, the relaxed subproblem is solved for the locally optimal transmit precoders to minimize the objective function. Since the \ac{SCA} subproblem is relaxed around the \me{\ith{i-1}} optimal point, \textit{i.e},  \me{\mbf{x}^{\ast(i-1)}} for the \me{\ith{i}} iteration, the domain of the problem in the \me{\ith{i}} step includes optimal point from the  \me{\ith{i-1}} iteration as well. Therefore, at each \ac{SCA} step, the objective function can either be equal to or smaller than the previous value, thereby leading to the monotonic convergence of the objective function.

Once the problem is converged to a stationary transmit precoders, the receive beamformers are updated based on the receivers in \eqref{opt-rx} or \eqref{eqn-10}. The monotonic nature of the objective function is preserved by the receive beamformer update, since the receiver minimizes the objective value for the fixed transmit precoders, and hence the proposed \ac{JSFRA} scheme is guaranteed to converge to a stationary point of the original nonconvex problem.


Following similar approach as in Section \ref{sec-3.2.1}, at each iteration, the \ac{SCA} subproblems converge to a stationary point of the original nonconvex problem. The uniqueness of the precoders are justified if there is no phase ambiguity in the stationary solution. By reorganizing \eqref{eqn-mse-1.3} as
\iftoggle{single_column}{
	\begin{equation}
		\epsilon_{l,k,n} \geq  1 - 2 \Re{ \left \lbrace \mvec{w}{l,k,n}^\herm \mvec{H}{b_k,k,n} \mvec{m}{l,k,n} \right \rbrace} + \sum_{\mathclap{\forall (j,i)}} \left | \mvec{w}{l,k,n}^\herm \mvec{H}{b_i,k,n} \mvec{m}{j,i,n} \right |^2 + N_0 \, \|\mvec{w}{l,k,n}\|^2,
	\end{equation}
}{
\begin{multline}
\epsilon_{l,k,n} \geq  1 - 2 \Re{ \left \lbrace \mvec{w}{l,k,n}^\herm \mvec{H}{b_k,k,n} \mvec{m}{l,k,n} \right \rbrace} \\ 
+ \sum_{\mathclap{\forall (j,i)}} \left | \mvec{w}{l,k,n}^\herm \mvec{H}{b_i,k,n} \mvec{m}{j,i,n} \right |^2 + N_0 \, \|\mvec{w}{l,k,n}\|^2,
\end{multline}
}
we can see that the ambiguity in the phase rotations for the transmit and the receive beamformers are eliminated by the presence of real component in the \ac{MSE} expression. 

At each \ac{SCA} update, the transmit precoders are obtained uniquely by minimizing \eqref{eqn-mse-1} due to the convex nature of the relaxed problem. For a fixed transmit precoders, the \ac{MMSE} receiver improves the objective value \cite{christensen2008weighted,wmmse_shi}, leading to the monotonic convergence of the objective function. At each \ac{SCA} step, the optimal value of the previous iteration is also included in the domain of the problem, and the objective value can either decrease or stays the same after each iteration. Note that the objective function improves at each iteration, whereas the sum rate need not follow the same behavior.
\end{comment}