To prove the convergence of the transmit, receive beamformers, and the objective values of the centralized algorithms in \eqref{eqn-6} and \eqref{eqn-mse-1}, we will show that the following conditions are satisfied by the centralized formulations.
\begin{itemize}
	\item[(a)] The function should be bounded below
	\item[(b)] The feasible set should be a compact set
	\item[(c)] The sequence of the objective values should be strictly decreasing in each iteration
	\item[(d)] Uniqueness of the minimizer, \textit{i.e.}, the transmit and the receive beamformers should be unique in each iteration.
\end{itemize}
The conditions (a) (b) and (c) are required for the convergence of the objective values generated by the iterative algorithm. Since the feasible set is not fixed in each iteration, we require the condition (d) to prove the convergence of the objective function and the corresponding arguments namely, the transmit and the receive precoders to a stationary point \cite{meyer1976sufficient}. Assuming the conditions (a), (b) and (c) are satisfied, using \cite[Th. 3.14]{rudin1964principles}, we can show that the bounded and strictly decreasing objective sequence has a unique minimum in the feasible set. Finally, we use the discussions in \cite{marks1978technical,lanckriet2009convergence,scutari_1} to show that the limit point of the iterative algorithm is indeed a stationary point of the nonconvex problem in \eqref{eqn-6} and \eqref{eqn-mse-1}.

Before proceeding further, let us consider a generalized formulation for the problems in \eqref{eqn-6} and \eqref{eqn-mse-1} as
\begin{IEEEeqnarray}{RcL} \label{con} \neqsub
	\underset{\mx,\my,\mz}{\text{minimize}} &\quad& \hat{f}(\mx,\my,\mz) \eqsub \label{con-obj} \\
	\text{subject to} &\quad& h(\mz) - g_0(\mx,\my) \leq 0 \eqsub \label{con-dc} \\
	&\quad& g_1(\mx,\my) \leq 0 \eqsub \label{con-cvx-blk} \\
	&\quad& g_2(\mx) \leq 0 \eqsub \label{con-cvx}
\end{IEEEeqnarray}
where \me{g_2}, \me{\hat{f}} are convex and \me{h} is a linear function. Let \me{g_0,g_1} be convex w.r.t either \me{\mx} or \me{\my}, but not on both. The constraint in \eqref{con-dc} corresponds to \eqref{eqn-6.2} or \eqref{eqn-mse-1.2} and the constraint \eqref{con-cvx-blk} corresponds to \eqref{eqn-6.3} or \eqref{eqn-mse-1.3}. Other convex constraints are addressed by \eqref{con-cvx} and the feasible set of \eqref{con} is given by 
\iftoggle{single_column}{
	\begin{equation}
	\mc{F} = \{ \; \mx,\my,\mz \; \big | h(\mz) - g_0(\mx,\my) \leq 0, g_1(\mx,\my) \leq 0, g_2(\mx) \leq 0 \}.
	\end{equation}
}{\begin{align}
\mc{F} = \{ \; \mx,\my,\mz \; \big | & \quad h(\mz) - g_0(\mx,\my) \leq 0, \nonumber \\
& \quad g_1(\mx,\my) \leq 0, g_2(\mx) \leq 0 \}.
\end{align}}

To solve \eqref{con}, we adopt \ac{AO} by fixing a block of variables and optimize for others \cite{bezdek2002some}. However, after fixing the variable \me{\my}, the problem is still nonconvex due to the constraint \eqref{con-dc}. To overcome this issue, we resort to \ac{SCA} principle presented in \cite{lanckriet2009convergence,scutari_1,scutari-1} by relaxing \eqref{con-dc} by a sequence of convex subsets. Since it involves two nested iterations, we denote the \ac{AO} step by a superscript \me{(i)} and the \ac{SCA} iteration by a subscript \me{k}. Let \me{\iter{\mx}{\ast}{i}} be the value of \me{\mx} when \ac{SCA} converges in the \me{\ith{i}} \ac{AO} step and let \me{\iter{\mz}{\ast|\my}{i}} be the solution for \me{\mz} in the \me{\ith{i}} \ac{AO} step for fixed \me{\my}. The stacked transmit precoders of all users in the \eqn{\ith{k}} \ac{SCA} step of the \eqn{\ith{i}} \ac{AO} is given by \eqn{\mx_k^{(i)}}.

\subsection{Bounded Objective Function and Compact Feasible Set}
The feasible sets of the problems \eqref{eqn-6} and \eqref{eqn-mse-1} are bounded and closed, which is verified by the total power constraint on the transmit precoders \eqref{eqn-6.4}, therefore, the sets are compact. 

The minimum of the norm in the objective is \me{\|x\|_q > -\infty}, therefore, it is bounded below. The objective function is Lipschitz continuous over the feasible set, and therefore, it is bounded from above as well, since the feasible set is bounded. %The objective function in \eqref{eqn-6} and \eqref{eqn-mse-1} is continuous and approaches \me{\infty} as \me{\mbf{t}_k \rightarrow \infty}, and therefore it is coercive in the feasible set. %Using conditions (a) and (b), we can guarantee the existence of a minimizer to the problem in \eqref{eqn-6} and \eqref{eqn-mse-1}.

\subsection{Uniqueness of the Iterates and Strong Convexity} \label{c-a}
The uniqueness of the iterates \eqn{\{\iter{\mx}{k}{i},\iter{\my}{\ast}{i-1},\iter{\mz}{k|\my}{i}\}} can be ensured for the \ac{MSE} reformulated problem in \eqref{eqn-mse-2} when all the constraints are active. However when \eqn{\hat{f}(\iter{\mx}{k}{i},\iter{\my}{\ast}{i-1},\iter{\mz}{k|\my}{i}) = 0} after some iteration, say \me{k} and \me{i}, the uniqueness cannot be guaranteed as there can be multiple solutions with the same objective value in the feasible set. Similarly, when the objective is non-zero for \eqref{eqn-9}, the minimizer is unique only if the receivers are matched to the transmit beamformers. %, \textit{i.e.}, \me{q_{l,k,n} = 0} in \eqref{eqn-wsrm-expr}, while initializing the feasible point \eqn{\iter{\mx}{0}{0}} and \eqn{\iter{\my}{0}{0}}.

\review{To ensure the uniqueness of the transmit precoders and the receive beamformers on all iterations, the convex subproblems in \eqref{eqn-9} and \eqref{eqn-mse-2} can be regularized by a strongly convex function in each \ac{SCA} iteration \eqn{k} and \ac{AO} update \eqn{i} as 
\begin{IEEEeqnarray}{rCl} \neqsub
	\hat{f}(\mbf{z}) &=& \| \tilde{\mbf{v}} \|_q \eqsub \label{orig_obj} \\ 
	f(\mbf{z}) &=& \hat{f}(\mbf{z}) + c_k^{(i)} \, \| \mbf{z} - \mbf{z}^{(i)}_{k} \|^2_2 \eqsub \label{mod_obj} 
\end{IEEEeqnarray}
where \eqn{\mbf{z}} is vector stacking all optimization variables, which can be either \eqn{[\mx,\iter{\my}{\ast}{i-1},\mz]} or \eqn{[\iter{\mx}{\ast}{i},\my,\mz]} depending on the problem and \me{c_{k}^{(i)} > 0} is a positive constant to ensure strong convexity of the objective with some parameter \eqn{m \triangleq \min_{k} \{c_{k}^{(i)}\}} and \eqn{m > 0} as discussed in \cite{scutari-1}. Now by using the strong convexity of the modified objective in \eqref{mod_obj}, we can guarantee the uniqueness of the minimizer in each \ac{SCA} step \eqn{k} \cite{yang_yang,scutari-1}. Note that the quadratic term in \eqref{mod_obj} includes the previous solution \eqn{\mbf{z}^{(i)}_{k} \triangleq [\mx_{k}^{(i)},\iter{\my}{\ast}{i-1},\mz_{k}^{(i)}]} in the objective of \eqref{con-m}, which is solved in the \eqn{\ith{k}} \ac{SCA} iteration.}

%The uniqueness of the transmit precoders and the receive beamformers for the \ac{MSE} reformulation in \eqref{eqn-mse-2} can be guaranteed in each iteration by the \ac{MSE} constraint in \eqref{eqn-mse-1.3}. The uniqueness of the iterates for the problem in \eqref{eqn-9} can be guaranteed in each convex subproblem if the receivers are matched to the equivalent downlink channel with the initially chosen feasible transmit precoders, \textit{i.e.}, \me{q_{l,k,n} = 0 \: \forall \, l,k,n} in \eqref{eqn-wsrm-expr} while beginning the \ac{SCA} iteration.
%of the objective
%When the objective is zero, the uniqueness of  the transmit and the receive beamformers are not guaranteed in both problem formulations, since they are the under-estimators and need not be active at the minimum. To obtain a unique set of transmit precoders and the receive beamformers in all scenarios, we can regularize the objective with a strongly convex function as
%\begin{equation}
%\| \tilde{\mbf{v}} \|_q + c \, \| \mbf{x} - \mbf{x}^{(i)} \|^2
%\end{equation}
%where \me{\mbf{x}} is the vector containing all the optimization variables and \me{c} is a positive constant. The vector \me{ \mbf{x}^{(i)}} is the value of \me{\mbf{x}} in the \me{\ith{i}} iteration. %Note that the uniqueness of the transmit precoders and the receive beamformers for each convex subproblem can be shown by using the constraint \eqref{eqn-8} for the problem \eqref{eqn-9} and \eqref{eqn-mse-1.3} for the \ac{MSE} reformulation in \eqref{eqn-mse-2}. The receive beamformers \me{\mvec{w}{l,k,n}} in \eqref{eqn-10} are also unique for the given set of transmit precoders, since the convex subproblems in \eqref{eqn-9} and \eqref{eqn-mse-2} are susceptible to the transmit precoder phase rotations, \textit{i.e.}, unitary transformations. When the objective function is zero, the uniqueness of the transmit precoders are not guaranteed by using \eqref{eqn-8} and \eqref{eqn-mse-1.3} constraints, since they are not active at the optimal solution of the subproblems. To obtain unique set of transmit precoders when the objective is zero even for a single \ac{BS}, we can regularize the objective with the transmit power as discussed in Appendix \ref{a-3} without affecting the optimal value.

\subsection{Strict Monotonicity of the Objective Sequence} \label{mcity}
\review{In the forthcoming discussions, we consider the modified objective \eqn{f} in \eqref{mod_obj} instead of \eqn{\hat{f}} due to the uniqueness property. Note that the modified objective \eqn{f} is equivalent to \eqn{\hat{f}} upon convergence, \textit{i.e.}, when the solution is a limit point. Therefore, the discussions are valid for the \ac{JSFRA} problem in \eqref{eqn-6} by regularizing the original objective \eqref{eqn-6.1} as \eqref{mod_obj}.}

To begin with, let us consider the variable \me{\my} is fixed for the \me{\ith{i}} \ac{AO} with the optimal value found in previous iteration \me{i-1} as \me{\iter{\my}{\ast}{i-1}}. In order to solve for \me{\mx} in \ac{SCA} iteration \me{k}, we linearize the nonconvex function \me{g_0} using previous \ac{SCA} iterate \me{\iter{\mx}{k}{i}} of \me{\mx} as
\iftoggle{single_column}{
\begin{equation}\label{con-relax}
\hat{g}_o(\mx,\iter{\my}{\ast}{i-1};\iter{\mx}{k}{i}) = {g}_0(\iter{\mx}{k}{i},\iter{\my}{\ast}{i-1}) + \nabla g_0(\iter{\mx}{k}{i},\iter{\my}{\ast}{i-1})^{\mathrm{T}} (\mx - \iter{\mx}{k}{i}).
\end{equation}}{
\begin{multline} \label{con-relax}
\hat{g}_o(\mx,\iter{\my}{\ast}{i-1};\iter{\mx}{k}{i}) = {g}_0(\iter{\mx}{k}{i},\iter{\my}{\ast}{i-1}) \\ + \nabla g_0(\iter{\mx}{k}{i},\iter{\my}{\ast}{i-1})^{\mathrm{T}} (\mx - \iter{\mx}{k}{i}).
\end{multline}}
Let \me{\iter{\mc{X}}{k}{i}} be the feasible set for the \eqn{\ith{i}} \ac{AO} iteration and the \me{\ith{k}} \ac{SCA} point for a fixed \me{\iter{\my}{\ast}{i-1}} and \me{\iter{\mx}{k}{i}}. Similarly, \me{\iter{\mc{Y}}{k}{i}} denotes the feasible set for a fixed \me{\iter{\mx}{\ast}{i}} and \me{\iter{\my}{k}{i}}. Using \eqref{con-relax}, the convex subproblem for the \me{\ith{i}} \ac{AO} iteration and the \me{\ith{k}} \ac{SCA} point for the variable \me{\mx} and \me{\mz} is given by
\begin{IEEEeqnarray}{rCl} \label{con-m} \neqsub
	\underset{\mx,\mz}{\text{minimize}} &\quad& f(\mx,\iter{\my}{\ast}{i-1},\mz) \eqsub \label{con-obj-m} \\
	\text{subject to} &\quad& h(\mz) - \hat{g}_0(\mx,\iter{\my}{\ast}{i-1};\iter{\mx}{k}{i}) \leq 0 \eqsub \label{con-dc-m} \\
	&\quad& g_1(\mx,\iter{\my}{\ast}{i-1}) \leq 0, \quad g_2(\mx) \leq 0 \eqsub \label{con-cvx-blk-m}
\end{IEEEeqnarray}
The feasible set defined by the problem in \eqref{con-m} is denoted as \me{\iter{\mc{X}}{k}{i} \subset \mc{F}}. To prove the convergence of the \ac{SCA} updates in the \me{\ith{i}} \ac{AO} iteration, let us consider that \eqref{con-m} yields 
\me{\iter{\mx}{k+1}{i}} and \me{\iter{\mz}{k+1}{i}} as the solution in the \me{\ith{k}} iteration. The point \me{\iter{\mx}{k+1}{i}} and \me{\iter{\mz}{k+1}{i}}, which minimizes the objective function satisfies
\iftoggle{single_column}{
\begin{equation}\label{con-sub-set}\allowdisplaybreaks
h(\iter{\mz}{k+1}{i}) - {g}_0(\iter{\mx}{k+1}{i},\iter{\my}{\ast}{i-1}) \leq h(\iter{\mz}{k+1}{i}) - \hat{g}_0(\iter{\mx}{k+1}{i},\iter{\my}{\ast}{i-1};\iter{\mx}{k}{i}) \leq 0. 
\end{equation}}{
\begin{multline}\label{con-sub-set}\allowdisplaybreaks
h(\iter{\mz}{k+1}{i}) - {g}_0(\iter{\mx}{k+1}{i},\iter{\my}{\ast}{i-1}) \leq \\
h(\iter{\mz}{k+1}{i}) - \hat{g}_0(\iter{\mx}{k+1}{i},\iter{\my}{\ast}{i-1};\iter{\mx}{k}{i}) \leq 0. 
\end{multline}}
Using \eqref{con-sub-set}, we can show that \me{\{\iter{\mx}{k+1}{i},\iter{\my}{\ast}{i-1}, \iter{\mz}{k+1}{i}\}} is feasible, since the initial \ac{SCA} operating point \me{\iter{\mx}{\ast}{i-1}} was chosen to be feasible from the \me{\ith{(i-1)}} \ac{AO} iteration. In each \ac{SCA} step, the feasible set includes the solution from the previous iteration as \me{\{\iter{\mx}{k+1}{i},\iter{\my}{\ast}{i-1},\iter{\mz}{k+1}{i}\} \in \iter{\mc{X}}{k+1}{i} \subset \mc{F}}, therefore, it decreases the objective as \cite{lanckriet2009convergence,scutari_1,amir}
\iftoggle{single_column}{
\begin{equation} \label{con-convergence}
f(\iter{\mx}{0}{i},\iter{\my}{\ast}{i-1},\iter{\mz}{0}{i}) \geq f(\iter{\mx}{k}{i},\iter{\my}{\ast}{i-1},\iter{\mz}{k}{i}) \geq f(\iter{\mx}{k+1}{i},\iter{\my}{\ast}{i-1},\iter{\mz}{k+1}{i}) \geq f(\iter{\mx}{\ast}{i},\iter{\my}{\ast}{i-1},\iter{\mz}{\ast|\my}{i}). 
\end{equation}}{
\begin{multline} \label{con-convergence} \allowdisplaybreaks
f(\iter{\mx}{0}{i},\iter{\my}{\ast}{i-1},\iter{\mz}{0}{i}) \geq f(\iter{\mx}{k}{i},\iter{\my}{\ast}{i-1},\iter{\mz}{k}{i}) \\ \geq f(\iter{\mx}{k+1}{i},\iter{\my}{\ast}{i-1},\iter{\mz}{k+1}{i}) \geq f(\iter{\mx}{\ast}{i},\iter{\my}{\ast}{i-1},\iter{\mz}{\ast|\my}{i}). 
\end{multline}}
Thus the sequence \me{\{f(\iter{\mx}{k}{i},\iter{\my}{\ast}{i-1},\iter{\mz}{k}{i})\}} is nonincreasing. \review{Since the objective is strongly convex due to the introduced quadratic term in \eqref{mod_obj}, the minimizer \eqn{\{\iter{\mx}{k}{i},\iter{\my}{\ast}{i-1},\iter{\mz}{k}{i}\}} in each \ac{SCA} step is unique. Using this and the fact that the sequence of objectives is nonincreasing ensure that the iterates of the \ac{SCA} update converges to a limit point in the \eqn{\ith{i}} \ac{AO} step.  However, note that the limit point need not be a stationary point of \eqref{con} since the receive beamformers \eqn{\my} have been fixed.}

% and approaches limit point of the \ac{SCA} as \me{k \rightarrow \infty}. The equality in \eqref{con-convergence} is achieved upon the \ac{SCA} convergence. The limit point obtained in the \eqn{\ith{i}} \ac{AO} step need not to be a stationary point of \eqref{con}, since it is the minimizer in \me{\iter{\mc{X}}{\ast}{i}} only. \review{Note that for a fixed initial operating point of the iterative \ac{SCA} procedure, the minimizer in each \ac{SCA} step is unique and so the limit point due to the strong convexity of the modified objective \eqref{mod_obj}.} 


%\review{Since the objective is strongly convex, due to the quadratic term in \eqref{mod_obj}, the minimizer \me{\{\iter{\mx}{\ast}{i}, \iter{\my}{\ast}{i-1},\iter{\mz}{\ast|\my}{i}\}} in each \ac{SCA} step is unique.}

Once \me{\iter{\mx}{\ast}{i}} is found for fixed \me{\my}, then \eqref{con} is solved for \me{\my} with fixed \me{\mx}. However, after fixing \me{\mx} as \me{\iter{\mx}{\ast}{i}} in \eqref{con}, the problem is still nonconvex due to \eqref{con-dc}. Following similar approach as above, the minimizer \me{ \{\iter{\mx}{\ast}{i},\iter{\my}{k+1}{i},\iter{\mz}{k+1}{i}\}} can be found in each \ac{SCA} step \me{k} by solving \eqref{con-m} iteratively. Note that \me{\iter{\mz}{k+1}{i}} is reused since the variable \me{\mx} is fixed in the \me{\ith{i}} \ac{AO} iteration. The sequence convergence and the nonincreasing behavior of the objective follow similar arguments as above.\footnote{Note that we can also use the \ac{MMSE} receiver in \eqref{eqn-10} instead of performing the \ac{SCA} updates until convergence for the optimal receiver.} The limit point of the sequence of iterates generated by iteratively solving \eqref{con-m} is given by \me{\{\iter{\mx}{\ast}{i}, \iter{\my}{\ast}{i},\iter{\mz}{\ast|\mx}{i}\} \in \iter{\mc{Y}}{\ast}{i} \subset \mc{F}}. 

Finally, to prove the global convergence of the iterative algorithm, we need to show that the \ac{AO} updates also produce a nonincreasing sequence of objectives, \textit{i.e.}, 
\iftoggle{single_column}{
\begin{equation}
f(\iter{\mx}{\ast}{i},\iter{\my}{0}{i},\iter{\mz}{0}{i}) \leq f(\iter{\mx}{\ast}{i},\iter{\my}{\ast}{i-1},\iter{\mz}{\ast|\my}{i}).
\end{equation}}{
\begin{equation}
f(\iter{\mx}{\ast}{i},\iter{\my}{0}{i},\iter{\mz}{0}{i}) \leq f(\iter{\mx}{\ast}{i},\iter{\my}{\ast}{i-1},\iter{\mz}{\ast|\my}{i}).
\end{equation}}
Let \me{\iter{\mx}{\ast}{i}} and \me{\iter{\mz}{\ast|\my}{i}} be the solution obtained by solving \eqref{con-m} iteratively until \ac{SCA} convergence in the \me{\ith{i}} \ac{AO} iteration for \eqn{\mx} and \eqn{\mz} with fixed \eqn{\my = \iter{\my}{\ast}{i-1}}. To find \me{\iter{\my}{0}{i}}, we fix \eqn{\mx} as \me{\iter{\mx}{\ast}{i}} and optimize for \me{\my}. Since the convex function is linearized in \eqref{con-dc}, the fixed operating point is also included in the feasible set \eqn{\{\iter{\mx}{\ast}{i},\iter{\my}{\ast}{i-1},\iter{\mz}{\ast|\my}{i}\} \in \iter{\mc{Y}}{0}{i}} by the equality in \eqref{con-sub-set}. Using this, we can show the monotonicity of the objective as
\begin{equation} \label{fixed_point}
f(\iter{\mx}{\ast}{i},\iter{\my}{0}{i},\iter{\mz}{0}{i}) \leq f(\iter{\mx}{\ast}{i},\iter{\my}{\ast}{i-1},\iter{\mz}{\ast|\mx}{i}).
\end{equation}
The \ac{AO} update follows \eqn{\{\iter{\mx}{\ast}{i},\iter{\my}{\ast}{i-1},\iter{\mz}{\ast|\my}{i}\} \in \{ \iter{\mc{X}}{\ast}{i} \cap \iter{\mc{Y}}{0}{i} \}}.

\review{Using the above discussions, the monotonicity of the objective seqeunce is guaranteed in each \ac{AO} and \ac{SCA} updates. However, to ensure strict monotonicity, we rely on the modified objective in \eqref{con-obj-m}, which is strongly convex with some parameter \eqn{m > 0}. Now, to show strict monotonicity, let us consider \eqn{\mbf{z}_{k}^{(i)} \triangleq [\mx_{k}^{(i)},\iter{\my}{\ast}{i-1},\mz_{k}^{(i)}]} as the minimizer for \eqref{con-m} in the \eqn{\ith{(k-1)}} \ac{SCA} step and let \eqn{\iter{\mbf{z}}{k+1}{i}} be the minimizer in the \eqn{\ith{k}} step. At the \eqn{\ith{k}} \ac{SCA} iteration, \eqn{\forall \mbf{z} \in \mc{X}_{k}^{(i)}}, it follows
\begin{IEEEeqnarray}{rCl} \neqsub \label{strict_monotonicity}
\nabla f(\iter{\mbf{z}}{k+1}{i})^\tran (\mbf{z} - \iter{\mbf{z}}{k+1}{i}) &\geq& 0 \eqsub \\
f(\mbf{z}) - f(\iter{\mbf{z}}{k+1}{i}) &\geq& \tfrac{m}{2} \, \|\mbf{z} - \iter{\mbf{z}}{k+1}{i}\|^2 \eqsub
\end{IEEEeqnarray}
since \eqn{\iter{\mbf{z}}{k+1}{i}} is the solution. Using \eqref{strict_monotonicity} and \eqn{\iter{\mbf{z}}{k}{i} \in \iter{\mc{X}}{k}{i}}, we can show that \eqn{f(\iter{\mbf{z}}{k}{i}) > f(\iter{\mbf{z}}{k+1}{i})} holds at each \ac{SCA} step unless \eqn{\mbf{z}_{k}^{(i)} \rightarrow \mbf{z}_{\ast}^{(i)}} as \eqn{k \rightarrow \infty}. Now using the facts that (i) \eqn{\{f(\mbf{z}^{(i)}_k)\}} is monotonic \cite{marks1978technical}, and (ii) the uniqueness of the minimizer (see \eqref{strict_monotonicity}), strict monotonicity of \eqn{\{f(\mbf{z}^{(i)}_k)\}} is ensured. However, if \eqn{\hat{f}} is used instead of \eqn{f} in \eqref{con-m}, then strict monotonicity of \eqn{\{\hat{f}(\mbf{z}^{(i)}_k)\}} cannot be guaranteed due to multiple solutions.

Similarly, we can also guarantee strict monotonicity of the objective \eqref{mod_obj} while alternating the optimization variables from \eqn{\mx,\mz} to \eqn{\my,\mz} in the \eqn{\ith{i}} \ac{AO} update. It follows from that fact that \eqn{\iter{\mbf{z}}{\ast}{i}} is included in the feasible set \eqn{\mc{Y}_{0}^{i}}, and therefore the objective function follows \eqref{fixed_point} strictly in each \ac{AO} iteration unless the sequence \eqn{\{\mbf{z}_{\ast}^{i}\}} converges to a limit point as \eqn{i \rightarrow \infty}.}

\begin{comment}

\begin{equation} \label{strict_monotonicity}
f(\iter{\mbf{z}}{k}{i}) - f(\iter{\mbf{z}}{k+1}{i}) \geq \nabla f(\iter{\mbf{z}}{k+1}{i})^\tran (\mbf{z}_{k}^{(i)} - \iter{\mbf{z}}{k+1}{i}) + \frac{m}{2} \, \|\mbf{z}_{k+1}^{(i)} - \iter{\mbf{z}}{k}{i}\|^2  \eqspace
\end{equation}



Note that the \ac{AO} and \ac{SCA} procedures generate a nonincreasing sequence of objectives for the original objective in \eqref{orig_obj} using the above discussions. However, by using the regularized objective in \eqref{mod_obj}, we can show that the sequence of objectives provided by \ac{AO} and \ac{SCA} steps is strictly decreasing. Let \eqn{\mbf{z}_{k}^{(i)} \triangleq [\mx_{k}^{(i)},\iter{\my}{\ast}{i-1},\mz_{k}^{(i)}]} be the solution of \eqref{con-m} in the \eqn{\ith{(k-1)}} \ac{SCA} step and let \eqn{\iter{\mbf{z}}{k+1}{i}} be the minimizer in the \eqn{\ith{k}} \ac{SCA} step. Note that the objective \eqref{con-obj-m} is strongly convex with some parameter \eqn{m > 0}, and therefore \eqn{\forall \mbf{z} \in \mc{X}_{k}^{(i)}}
\begin{IEEEeqnarray}{l}
f(\mbf{z}) \geq f(\iter{\mbf{z}}{k+1}{i}) + \nabla f(\iter{\mbf{z}}{k+1}{i})^\tran (\mbf{z} - \iter{\mbf{z}}{k+1}{i}) + \tfrac{m}{2} \|\mbf{z} - \iter{\mbf{z}}{k+1}{i}\|^2. \eqspace 
\end{IEEEeqnarray}
Since \eqn{\iter{\mbf{z}}{k+1}{i}} is the minimizer for \eqref{con-m} in the \eqn{\ith{k}} \ac{SCA} step, 
\begin{IEEEeqnarray}{rCl} \neqsub \label{s_ineq_g}
\nabla f(\iter{\mbf{z}}{k+1}{i})^\tran (\mbf{z}_{k}^{(i)} - \iter{\mbf{z}}{k+1}{i}) &\geq& 0, \quad \iter{\mbf{z}}{k}{i} \in \iter{\mc{X}}{k}{i} \eqsub \\
f(\iter{\mbf{z}}{k}{i}) - f(\iter{\mbf{z}}{k+1}{i}) &\geq& \tfrac{m}{2} \, \|\mbf{z}_{k}^{(i)} - \iter{\mbf{z}}{k+1}{i}\|^2. \eqsub \label{s_ineq}
\end{IEEEeqnarray}
Note that \eqref{s_ineq} holds with strict inequality in each update as \eqn{f(\iter{\mbf{z}}{k}{i}) > f(\iter{\mbf{z}}{k+1}{i})}, unless \eqn{\mbf{z}_{k}^{(i)} = \mbf{z}_{k+1}^{(i)}}, which occurs at \eqn{\mbf{z}_{k}^{(i)} \rightarrow \mbf{z}_{\ast}^{(i)}} as \eqn{k \rightarrow \infty}. Using the facts that (i) \eqn{\{f(\mbf{z}^{(i)}_k)\}} is monotonic \cite{marks1978technical}, and (ii) the uniqueness of the minimizer (see \eqref{s_ineq_g}), strict monotonicity of \eqn{\{f(\mbf{z}^{(i)}_k)\}} is ensured. However, if \eqn{\hat{f}} is used as the objective in \eqref{con-m}, then strict monotonicity of \eqn{\{\hat{f}(\mbf{z}^{(i)}_k)\}} cannot be guaranteed due to the existence of multiple solutions.

	In order to show the strict monotonicity of the sequence \eqn{\{f(\mbf{z}^{(i)}_k)\}} generated by the \ac{SCA} method, we rely on the strong convexity of \eqref{mod_obj} with parameter \eqn{m > 0} as %We know that the objective decreases in each \ac{SCA} step, since the \eqn{\ith{(k-1)}} solution \eqn{\mbf{z}^{(i)}_{k}} is a feasible point of \eqn{\mc{X}_{k}^{(i)}} in the \eqn{\ith{k}} \ac{SCA} step \cite{marks1978technical}. 
	%It follows by the strong convexity of \eqref{mod_obj} with \eqn{m > 0} in the \eqn{\ith{k}} update
	\begin{equation} \label{strict_monotonicity}
	f(\iter{\mbf{z}}{k}{i}) - f(\iter{\mbf{z}}{k+1}{i}) \geq \underbrace{\nabla f(\iter{\mbf{z}}{k+1}{i})^\tran (\mbf{z}_{k}^{(i)} - \iter{\mbf{z}}{k+1}{i})}_{= 0} + \frac{m}{2} \, \|\mbf{z}_{k+1}^{(i)} - \iter{\mbf{z}}{k}{i}\|^2  \eqspace
	\end{equation}
	since \eqn{\iter{\mbf{z}}{k+1}{i}} is the minimizer of \eqref{con-m}. Note that \eqref{strict_monotonicity} holds with strict inequality in each update as \eqn{f(\iter{\mbf{z}}{k}{i}) > f(\iter{\mbf{z}}{k+1}{i})}, unless \eqn{\mbf{z}_{k}^{(i)} = \mbf{z}_{k+1}^{(i)}}, which occurs at \eqn{\mbf{z}_{k}^{(i)} \rightarrow \mbf{z}_{\ast}^{(i)}} as \eqn{k \rightarrow \infty}. Using the facts that (i) the sequence \eqn{\{f(\mbf{z}^{(i)}_k)\}} generated by the \ac{SCA} approach is monotonic \cite{marks1978technical}, and  (ii) the uniqueness of the minimizer in each \ac{SCA} step (see \eqref{strict_monotonicity}), we can ensure the strict monotonicity of \eqn{\{f(\mbf{z}^{(i)}_k)\}}. However, if \eqn{\hat{f}} is used as the objective in \eqref{con-m}, then strict monotonicity of \eqn{\{\hat{f}(\mbf{z}^{(i)}_k)\}} cannot be guaranteed due to the existence of multiple solutions.


%Even though the original convex function \eqn{\hat{f}(\mbf{z})} can have multiple limit points upon the \ac{SCA} convergence, due to the strong convexity of \eqn{f(\mbf{z})} in \eqref{mod_obj}, the limit point is unique, and therefore the objective decreases monotonically in a strict sense. It can be proved, if \eqn{f(\mbf{z}^{(i)}_k)} is the unique minimizer in two consecutive steps even if the other limit points are included in the feasible set \eqn{\iter{\mc{X}}{k+1}{i}}, with the same \eqn{\hat{f}(\mbf{z}^{(i)}_{k})}.
%Let \eqn{\mbf{z}^{i}_{k}} be an unique minimizer and also a limit point of the \eqn{\ith{k}} \ac{SCA} step. Let \eqn{\mbf{z}^{i}_{k+1}} be another limit point with the same objective \eqn{\hat{f}(\mbf{z}^{i}_{k}) = \hat{f}(\mbf{z}^{i}_{k+1})} in the \eqn{\ith{k+1}} feasible set \eqn{\iter{\mc{X}}{k+1}{i}}. Due to the convexity of \eqn{\hat{f}(\mbf{z})}, strict monotonicity cannot be ensured since the selection of \eqn{\mbf{z}^{i}_{k}} and \eqn{\mbf{z}^{i}_{k+1}} are equally likely. However, due to the additional quadratic term \eqn{c \, \| \mbf{z} - \mbf{z}^{(i)}_{k} \|^2_2} in \eqn{f(\mbf{z})} for the \eqn{\ith{k+1}} iteration, the update cannot choose \eqn{\mbf{z}^{i}_{k+1}} due to the positive quadratic residual, therefore, it selects the fixed point \eqn{\mbf{z}^{i}_{k}} in the \eqn{\ith{k+1}} iteration. Similarly, using the above argument, we can also prove the strict monotonicity of the objective in each \ac{AO} update \eqn{i}, and thereby proving strict monotonicity of the overall objective sequence.


{Let \eqn{\mbf{z}_{k}^{(i)} \triangleq [\mx_{k}^{(i)},\iter{\my}{\ast}{i-1},\mz_{k}^{(i)}]} be the solution of \eqref{con-m} in the \eqn{\ith{k-1}} \ac{SCA} step. In order to show the strict monotonicity of the sequence \eqn{\{f(\mbf{z}^{(i)}_k)\}} generated by the \ac{SCA} method, we rely on the strong convexity of \eqref{mod_obj}. We know that the objective decreases in each \ac{SCA} step, since the \eqn{\ith{k}} solution \eqn{\mbf{z}^{(i)}_k} is a feasible point in \eqn{\mc{X}_{k+1}^{(i)}} of the \eqn{\ith{k+1}} iteration \cite{marks1978technical}. However, due to the strong convexity of \eqref{mod_obj}, it follows that \eqn{\forall \mbf{z} \in \mc{X}^{(i)}_{k}}
	\begin{equation} \label{strict_monotonicity}
	f(\mbf{z}) \geq f(\mbf{z}_{k}^{(i)}) + \nabla f(\mbf{z}_{k}^{(i)})^\tran (\mbf{z} - \mbf{z}_{k}^{(i)}) + c \, \|\mbf{z} - \mbf{z}_{k}^{(i)}\|^2
	\end{equation}
	where the equality is valid when \eqn{\mbf{z}_{k}^{(i)} = \mbf{z}_{k+1}^{(i)}}, \textit{i.e.}, as \eqn{k\rightarrow\infty}, \eqn{\mbf{z}_{k}^{(i)} \rightarrow \mbf{z}_{\ast}^{(i)}}. Using the facts that (i) the sequence \eqn{\{f(\mbf{z}^{(i)}_k)\}} generated by the \ac{SCA} approach is monotonic \cite{marks1978technical}, and  (ii) the uniqueness of the minimizer in each \ac{SCA} step (see \eqref{strict_monotonicity}), we can ensure the strict monotonicity of \eqn{\{f(\mbf{z}^{(i)}_k)\}}. However, if \eqn{\hat{f}} is used in the objective of \eqref{con-m}, then the limit point is not unique, \textit{i.e.}, \eqn{\{\exists \mbf{z} \in \mc{X}_{\ast}^{(i)} \vert \hat{f}(\mbf{z}) = \hat{f}(\mbf{z}_{\ast}^{(i)})\}}, and therefore the sequence generated by \eqref{con-m} with \eqn{\hat{f}} is not strictly monotonic.
	
	Similarly, we can ensure strict monotonicity of the objective \eqref{mod_obj} while alternating the optimization variables from \eqn{\mx,\mz} to \eqn{\my,\mz} in the \eqn{\ith{i}} \ac{AO} iteration. Since the limit point \eqn{\{\iter{\mx}{\ast}{i},\iter{\my}{\ast}{i-1},\iter{\mz}{\ast|\mx}{i}\}} is also included in the feasible set \eqn{\mc{Y}_{0}^{i}}, the objective function follows \eqref{fixed_point} in strict sense unless the overall sequence \eqn{\{\mbf{z}_{\ast}^{i} \}} converges to a limit point as \eqn{i \rightarrow \infty}.}




\end{comment}

%Due to the convexity in the original objective \eqn{\hat{f}(\mbf{z})}, it can have multiple limit points for the \ac{SCA} update, and therefore it follows \eqn{f(\mbf{z}^{(i)}_{k+1}) \leq f(\mbf{z}^{(i)}_k), \forall k}. However, due to the strong convexity of \eqn{f(\mbf{z})} in \eqref{mod_obj}, the limit point of the \ac{SCA} updates is also unique, and therefore has strict monotonicity in \eqn{f(\mbf{z}^{(i)}_k)} after each update. It can be justified, if \eqn{f(\mbf{z}^{(i)}_k)} is the unique minimizer in two consecutive steps even by including other limit points in the feasible set \eqn{\iter{\mc{X}}{k+1}{i}} with the same original objective value as \eqn{\hat{f}(\mbf{z}^{(i)}_{k})}.
%In spite of choosing \eqn{\mbf{z}_{k}^{(i)}} in both \eqn{k} and \eqn{k+1} iteration, \eqn{f(\mbf{z})} decreases strictly due to the vanishing quadratic term in the \eqn{\ith{k+1}} step, therefore, it is a fixed point. Similarly, by using the above argument, we can also prove the strict monotonicity of the objective in each \ac{AO} update, and thereby proving strict monotonicity of the overall sequence.
%In spite of the additional quadratic term \eqn{\| \mbf{z} - \mbf{z}^{(i)}_{k} \|^2_2} to ensure unique minimizer, strict monotonicity of the objective in each \ac{SCA} update cannot be ensured. However, strict monotonicity can be guaranteed in each update \eqn{i} of the algorithm \me{\mc{A}} in \eqref{g-algo} for the objective. Using (a),(b),(c) along with the above discussion, we can ensure strict monotonicity and the convergence of the objective.}
%Due to the additional quadratic term \eqn{\| \mbf{z} - \mbf{z}^{(i)}_{k} \|^2_2} from the earlier iteration and the unique minimizer from the previous step is also included, the objective decreases strictly in each \ac{SCA} update for an arbitrary value of \eqn{c} in \eqref{mod_obj}. Therefore, in each update \eqn{i} of the algorithm \me{\mc{A}} in \ref{g-algo}, the objective decreases monotonically in strict sense. Using (a),(b),(c) along with the above discussion, we can ensure strict monotonicity and the convergence of the objective.}

\subsection{Convergence of the Beamformer Iterates} \label{c-b}
Let \me{\ma^{(i)} \triangleq [\iter{\mx}{\ast}{i},\iter{\my}{\ast}{i},\iter{\mz}{\ast|\mx}{i}]} be the stacked vector of the solution point from the \eqn{\ith{i}} \ac{AO} iteration and let \me{\mc{A}} be a point-to-set mapping algorithm defined as
\begin{equation} \label{g-algo}
\iterate{\ma}{i+1} \in \mc{A}(\iterate{\ma}{i}) \triangleq
\begin{cases}
\underset{\mx,\mz}{\mathrm{argmin}} \; f(\mx,\mz;\iter{\my}{\ast}{i-1})& \forall \mx,\mz \in \iter{\mc{X}}{\ast}{i} \\
\underset{\my,\mz}{\mathrm{argmin}} \; f(\my,\mz;\iter{\mx}{\ast}{i})& \forall \my,\mz \in \iter{\mc{Y}}{\ast}{i} 
\end{cases} 
\end{equation}
Note that in each iteration \me{i}, the algorithm \eqn{\mc{A}} in \eqref{g-algo} includes two \ac{SCA} updates performed until convergence, \textit{i.e.}, one for variable \me{\mx} and \me{\mz} by keeping \me{\my} fixed and another for \me{\my} and \me{\mz} by fixing \me{\mx} as constant. Since the minimizer is unique, the mapping can be defined as \me{\iterate{\ma}{i+1} = \mc{A}(\iterate{\ma}{i})}. In order to prove the convergence of the beamformer iterates, we define \me{\mc{F}^\ast \subset \mc{F}} as the set of fixed points, \textit{i.e.}, \me{\forall {\ma}^{\ast} \in \mc{F}^\ast, \, {\ma}^{\ast} = \mc{A}({\ma}^{\ast})}, identified by the algorithm \me{\mc{A}} for different initialization points. 

Using \cite[Th. 3.1]{meyer1976sufficient}, convergence of the iterates to a fixed point can be shown, if the following conditions are satisfied.
\begin{itemize}
\item Objective function \me{f} should be bounded and continuous.
\item Feasible set \me{\iter{\mc{X}}{\ast}{i},\iter{\mc{Y}}{\ast}{i}} in each step should be compact.
\item The sequence \me{\{\iterate{\ma}{i}\}} generated by \me{\mc{A}} should be strictly monotonic with respect to the objective \me{f}, \textit{i.e.}, \me{\ma^\prime = \mc{A}(\ma)} implies \me{f(\ma^\prime) < f(\ma)} whenever \me{\ma} is not a fixed point.
\end{itemize}
\review{Note that the above conditions are satisfied by the proposed algorithm \me{\mc{A}} in \eqref{g-algo} and the strict monotonicity is guaranteed from Section \ref{mcity} with modified objective \eqref{mod_obj}}. Using \cite{zangwill1969nonlinear} and \cite[Th. 3.1]{meyer1976sufficient}, we can show that (i) all limit points will be a fixed point, (ii) \eqn{f(\iterate{\ma}{i}) \rightarrow f(\ma^\ast)}, where \eqn{\ma^\ast} is a fixed point, and (iii) \eqn{\ma^\ast} is a regular point, \textit{i.e.}, \eqn{\|\iterate{\ma}{i} - \mc{A}(\iterate{\ma}{i}) \| \rightarrow 0}. Even though the fixed points in the set \me{\mc{F}^\ast} can be achieved by the algorithm \me{\mc{A}} for different initial points, the algorithm \me{\mc{A}} achieves a unique limit point, \textit{i.e.}, \me{\mc{F}^\ast = \{\ma^\ast\}}, once a feasible point is chosen to be the operating point while initializing \me{\mc{A}}.

\begin{comment}
and \me{\mc{A}} be a point-to-set mapping algorithm from \me{\mc{F}} into the nonempty subsets of \me{\mc{F}}, \textit{i.e.}, \eqn{\iterate{\ma}{i+1} \in \mc{A}(\iterate{\ma}{i})}. The set of accumulation or the limit points in the compact set \me{\mc{F}} be denoted by \me{\mc{F}^\ast} and the objective function be \me{f} is closed and continuous. Let \me{\{\iterate{\ma}{i}\}} be the sequence of iterates generated by the algorithm \me{\mc{A}} and the objective function \me{f}. If the mapping is strictly monotonic and uniformly compact, then the following conditions hold \cite{zangwill1969nonlinear} and \cite[Theorem 3.1]{meyer1976sufficient}.
\begin{itemize}
\item[(i)] all accumulation points will be a fixed point,
\item[(ii)] \eqn{f(\iterate{\ma}{i}) \rightarrow f(\ma^\ast)}, where \eqn{\ma^\ast} is a fixed point,
\item[(iii)] \eqn{\ma^\ast} is a regular point, \textit{i.e.} \eqn{\|\iterate{\ma}{i} - \iterate{\ma}{i+1} \| \rightarrow 0}.
\end{itemize}

To show that the proposed iterative method satisfies the above conditions,  the mapping \eqn{\mc{A}} for the iterative algorithm is given by
\begin{equation}
\iterate{\ma}{i+1} \in \mc{A}(\iterate{\ma}{i}) : \underset{\ma}{\mathrm{argmin}} \; f(\ma;\iterate{\ma}{i}) \; \forall \ma \in \iterate{\mc{X}}{i} \subset \mc{F}
\end{equation}
where \me{\mc{A}(\iterate{\ma}{i})} denotes the set of iterates in the subset \me{\iterate{\mc{X}}{i}}, which has the same objective value under the mapping \me{f} in the \me{\ith{i}} iteration The monotonicity of the proposed algorithm is guaranteed based on the arguments presented in Appendix \ref{mcity}. The mapping is strictly monotonic on \me{\mc{F}} with respect to the objective function as \me{\ma^\prime \in \mc{A}(\ma)} implies \me{f(\ma^\prime) < f(\ma)} whenever \me{\ma} is not a fixed point, \textit{i.e.}, \me{\ma \notin \mc{F}^\ast}, as in \eqref{fixed_point}. Using the above discussions, we can show that the sequence \me{\{\iterate{\ma}{i}\}} converges to a fixed point in the set \me{\mc{F}^\ast} for any feasible starting point \me{\iterate{\ma}{0} \in \mc{F}}

The sequence \me{\{\iterate{\ma}{i}\}} converges to the set of fixed points \me{\mc{F}^\ast} for any feasible starting point \me{\iterate{\ma}{0} \in \mc{F}}, therefore the mapping is uniformly compact.

To show the set of fixed points \me{\mc{F}^\ast} are the minimizers for the objective function \me{f}, we rely on the strict monotonicity of the objective function for each iteration. Since \me{f(\mc{A}(\iterate{\ma}{i})) < f(\iterate{\ma}{i})}, the objective decreases monotonically for each iteration and the equality is achieved when \me{\ma^\ast \in \mc{A}(\ma^\ast)}, which is a generalized fixed point. Therefore, the fixed points in \me{\mc{F}^\ast} are the minimizers for the objective function \me{f} over the set \me{\mc{F}}.

If the uniqueness of the iterates are guaranteed, then the algorithm will find a unique solution at each iteration as \me{\iterate{\ma}{i+1} = \mc{A}(\iterate{\ma}{i})} and the accumulation point is unique. The convergence of the iterates to a single fixed point is guaranteed by the discussions in \cite{zangwill1969nonlinear,meyer1976sufficient}. Even though the algorithm finds a single fixed point, all fixed points in \me{\mc{F}^\ast} are equally valid as a minimizer for the function \me{f} in \eqref{con}, since the \ac{SINR} in \eqref{eq:SINR} is invariant to the unitary rotations on the beamformers.
\end{comment}

\subsection{Stationary Points of the Nonconvex Problem}
\review{So far, we have shown that the sequence of iterates \eqn{\{\iterate{\ma}{i}\}} generated by \eqn{\mc{A}} converges to a limit point as \eqn{i \rightarrow \infty}. To show that \eqn{\ma^{\ast}} is a stationary point of \eqref{con}, it must satisfy the \ac{KKT} conditions of the nonconvex problem \eqref{con}, \textit{i.e.},
\begin{equation} \label{kkt_expr}
\textstyle	\nabla \hat{f}(\ma^{\ast}) + \mu_0 \big [ \nabla h(\ma^{\ast}) - \nabla {g}_0(\ma^{\ast}) \big ] + \sum_{i=1}^2 \mu_i \nabla g_i(\ma^{\ast}) = 0
\end{equation}
for some multipliers \eqn{\mu_i \geq 0} and lies in the feasible set \eqn{\ma^{\ast} \in \mc{F}}.

Now, to prove the feasibility of the limit point \me{\ma^{\ast}}, note that in each \ac{SCA} step, the feasible sets \eqn{\mc{X}_{k}^{(i)},\mc{Y}_{k}^{(i)} \subset \mc{F}}. Therefore, the limit point \eqn{\ma^{\ast} \in \mc{F}} and it satisfies the Slater's constraint qualifications. Moreover, as discussed in Appendix \ref{c-b}, \eqn{\ma^{\ast}} is a regular point of the algorithm \eqn{\mc{A}} and also the solution for \eqref{con-m}, therefore, there exist Lagrange multipliers \eqn{\mu_i} such that
\begin{equation} \label{kkt_mod_expr}
\textstyle \nabla f(\ma^{\ast}) + \mu_0 \big [ \nabla h(\ma^{\ast}) - \nabla\hat{g}_0(\ma^{\ast};\ma^{\ast}) \big ] + \sum_{i=1}^2 \mu_i \nabla g_i(\ma^{\ast}) = 0.
\end{equation}
The relation between \eqref{kkt_mod_expr} and \eqref{kkt_expr} is evident by the following facts: (i) The quadratic term \eqn{\|\ma - \ma^{(i)}\|^2} in \eqref{mod_obj} vanishes as \eqn{i \rightarrow \infty, \ma^{(i)} \rightarrow \ma^{\ast}}, since \eqn{\ma^{\ast}} is the fixed point of \eqn{\mc{A}}. Therefore, the gradients evaluated at \eqn{\ma^{\ast}} satisfies \eqn{\nabla {f}(\ma^{\ast}) = \nabla \hat{f}(\ma^{\ast})}. (ii) Additionally, by using the relation in \eqref{con-relax}, the gradient of the constraint \eqref{con-dc} evaluated at \eqn{\ma^{\ast}} is given by
\begin{equation} \label{rhs_equiv}
\nabla h(\ma^{\ast}) - \nabla \hat{g}_o(\mbf{\ma}^{\ast};\mbf{\ma}^{\ast}) = \nabla h(\ma^{\ast}) - \nabla g_o(\mbf{\ma}^{\ast}).
\end{equation}
Now, by applying the relation in \eqref{rhs_equiv} and \eqn{\nabla {f}(\ma^{\ast}) = \nabla \hat{f}(\ma^{\ast})} in \eqref{kkt_mod_expr}, we can show that the limit point \eqn{\ma^{\ast}} satisfies \eqref{kkt_expr}. By using \cite[Th. 2 and 11]{scutari_1} or \cite[Prop. 3.2]{amir}, we can show that the limit point of the sequence of iterates \eqn{\{\iterate{\ma}{i}\}} generated by the algorithm \me{\mc{A}} is a stationary point of the original nonconvex problem in \eqref{con}.}

\begin{comment}
In this section, we show that the limit point \eqn{\ma^{\ast}} of the sequence \eqn{\{\iterate{\ma}{i}\}}, obtained from \eqn{\mc{A}} as \eqn{i \rightarrow \infty} is a stationary point of the original nonconvex problem in \eqref{con}. In order for \eqn{\ma^{\ast}} to be a stationary point, it must satisfy the \ac{KKT} conditions of \eqref{con} and the Slater's constraint qualifications. Note that all points satisfies \eqn{\iterate{\ma}{i} \in \mc{Y}_\ast^{(i)} \subset \mc{F}}, and therefore the limit point \eqn{\ma^{\ast}} satisfies the constraint qualifications of the problem \eqref{con}.

To show that the limit point \eqn{\mbf{\ma}^{\ast}} satisfies the \ac{KKT} expression of the nonconvex problem, we utilize the fact that the limit point \eqn{\mbf{\ma}^{\ast}} is the solution of the convex subproblem \eqref{con-m} upon convergence. Therefore, it satisfies the \ac{KKT} conditions of the convex subproblem for some Lagrange multipliers \eqn{\mu_i \geq 0} as 
\begin{equation} \label{kkt_expr}
\textstyle	\nabla f(\ma^{\ast}) + \mu_0 \big [ \nabla h(\ma^{\ast}) - \nabla\hat{g}_0(\ma^{\ast};\ma^{\ast}) \big ] + \sum_{i=1}^2 \mu_i \nabla g_i(\ma^{\ast}) = 0.
\end{equation}
It is evident that the modified objective \eqn{f} and the convex function \eqn{\hat{f}} in \eqref{mod_obj} are equal at the limit point due to the vanishing quadratic penalty term. Now, by utilizing the relation \eqn{\nabla \hat{g}_o(\mbf{\ma}^{\ast};\mbf{\ma}^{\ast}) = \nabla g_o(\mbf{\ma}^{\ast})} between the \ac{DC} constraint \eqref{con-dc} and the corresponding convex approximation in \eqref{con-relax}, we can ensure that the limit point \eqn{\mbf{\ma}^{\ast}} indeed satisfies the \ac{KKT} condition of the nonconvex problem \eqref{con}. Now, by using \cite[Th. 2 and 11]{scutari_1} or \cite[Prop. 3.2]{amir}, we can show that all the limit points of the sequence \eqn{\{\iterate{\ma}{i}\}} generated by the algorithm \me{\mc{A}} are the \ac{KKT} points of \eqref{con}.

\review{Let \eqn{\ma^{\ast}} be the limit point of the sequence of iterates \eqn{\{\iterate{\ma}{i}\}} generated by the algorithm \eqn{\mc{A}} as \eqn{i \rightarrow \infty}. The limit point \eqn{\ma^\ast} is a stationary point of \eqref{con} if it satisfies the \ac{KKT} conditions. 
%and there exists no descent direction in the feasible set as
%\begin{equation}
%\nabla {f}(\ma^{\ast})^\tran \, (\ma - \ma^{\ast}) \geq 0, \: \forall \, \ma \in \mc{Y}^{\ast}
%\end{equation}
%where \eqn{\mc{Y}^{\ast}} is the feasible set of \eqref{con-m} upon convergence, \textit{i.e.}, \eqn{\mc{Y}_{\ast}^{i}} as \eqn{i \rightarrow \infty}. 
Let us consider the problem in \eqref{con-m}. Using \eqref{con-relax}, we can show that the feasible set \eqn{\mc{Y}_{\ast}^{i}} in each \ac{AO} iteration \eqn{i} is a subset of \eqn{\mc{F}}. Therefore, all iterates \eqn{\{\iterate{\ma}{i}\}} of the algorithm \eqn{\mc{A}} are feasible. Additionally, by the strong convexity of \eqn{f(\ma)}, following conditions are satisfied.
\begin{IEEEeqnarray}{rCl}  \neqsub \label{strong_cvxty}
\nabla {f}(\ma^{(i)})^\tran \, (\ma^{(i-1)} - \ma^{(i)}) &\geq& 0 \eqsub \\
f(\ma^{(i-1)}) - f(\ma^{(i)}) &\geq& c \: \|\ma^{(i-1)} - \ma^{(i)}\|^2. \eqsub
\end{IEEEeqnarray}
Using (a),(b),(c) and (d), we can show that \eqn{\|\ma^{(i)} - \mc{A}(\ma^{(i)})\| \rightarrow 0} as \eqn{i \rightarrow \infty}, which is the limit point of the algorithm \eqn{\mc{A}}.

Now, it remains to show that the \eqn{\ma^{\ast}} is a stationary point of the nonconvex problem \eqref{con}. To see this, we note that \eqn{\ma^{\ast}} must satisfy the \ac{KKT} conditions of \eqref{con}. Since \eqn{\ma^{\ast}} is the fixed point of the algorithm \eqn{\mc{A}}, it satisfies the \ac{KKT} conditions of the convex subproblem \eqref{con-m} given by
\begin{equation} \label{kkt_expr}
\textstyle	\nabla f(\ma^{\ast}) + \mu_0 \big [ \nabla h(\ma^{\ast}) - \nabla\hat{g}_0(\ma^{\ast}) \big ] + \sum_{i=1}^2 \mu_i \nabla g_i(\ma^{\ast}) = 0
\end{equation}
for some Lagrange multipliers \eqn{\mu_i \geq 0}. Using the equivalence between \eqn{\hat{f}(\ma^{\ast}) = f(\ma^{\ast})} and the relation in \eqref{con-relax} in \eqref{kkt_expr}, we can see that \eqref{kkt_expr} is indeed the \ac{KKT} condition for the nonconvex problem \eqref{con}. Since the gradient is zero in \eqref{kkt_expr}, the objective cannot be decreased any further. Now, by using \cite[Th. 2 and 11]{scutari_1} or \cite[Prop. 3.2]{amir}, we can show that the limit points \eqn{\ma^\ast} of \me{\mc{A}} are the \ac{KKT} points of \eqref{con}.}

%In order for \eqn{\ma^{\ast}} to be stationary point, it must satisfy the \ac{KKT} conditions of \eqref{con}. In each iteration, the algorithm \eqn{\mc{A}} in \eqref{g-algo} finds a point \eqn{\iterate{\ma}{i} \in \mc{F}} with \eqn{f(\iterate{\ma}{i}) < f(\iterate{\ma}{i-1})}, therefore, it satisfies all the constraints of \eqref{con}. Since the feasible set is compact and strict monotonicity is ensured, as \eqn{i \rightarrow \infty}, the objective no longer decreases as the algorithm \eqn{\mc{A}(\ma^{\ast}) = \ma^{\ast}} finds a unique \eqn{\ma^{\ast} \in \mc{F}^{\ast}} due to the strong convexity of \eqn{f}.

%To show the equivalence in the gradients of \eqref{con} and \eqref{con-m} at \eqn{\ma^{\ast}}, let us consider the limit point of the \ac{SCA} update in \eqn{\mc{A}} as it converges to a fixed point \eqn{\ma^{\ast} \in \mc{F}^{\ast}}. The \ac{KKT} expression for the convex subproblem \eqref{con-cvx} is given by
%\begin{equation} \label{kkt_expr}
%\nabla f(\ma^{\ast}) + \mu_1 \big [ \nabla h(\ma^{\ast}) - \nabla\hat{g}_0(\ma^{\ast}) \big ] + \sum_{i=1}^2 \mu_i \nabla g_i(\ma^{\ast}) = 0
%\end{equation}
%for some Lagrange multipliers \eqn{\mu_i \geq 0}. Using \eqref{con-relax} in \eqref{kkt_expr}, we can ensure the equivalence of the gradient between \eqref{con} and \eqref{con-m}. Moreover, \eqn{\ma^{\ast}} is a regular point \textit{i.e.} \eqn{\|\iterate{\ma}{i} - \iterate{\ma}{i+1} \| \rightarrow 0} as \eqn{i \rightarrow 0}. Now, with the above conditions and by using \cite[Th. 2 and 11]{scutari_1} and \cite[Prop. 3.2]{amir}, we can show that the fixed points obtained by \me{\mc{A}} under the mapping \me{f} is a stationary point of the nonconvex problem \eqref{con}.

\begin{comment}
To show the limit point of the sequence of iterates \me{\{\iterate{\ma}{i}\}} is a stationary point, it must satisfy the \ac{KKT} conditions of the problem in \eqref{con}. As \me{i \rightarrow \infty}, the objective converges as
\iftoggle{single_column}{
\begin{equation} \label{con-opt}\allowdisplaybreaks
f(\iter{\mx}{\ast}{i},\iter{\my}{\ast}{i},\iter{\mz}{\ast|x}{i}) = f(\iter{\mx}{\ast}{i+1},\iter{\my}{\ast}{i},\iter{\mz}{\ast|y}{i+1}) = 
f(\iter{\mx}{\ast}{i+1},\iter{\my}{\ast}{i+1},\iter{\mz}{\ast|x}{i+1})
\end{equation}}{
\begin{multline} \label{con-opt}\allowdisplaybreaks
f(\iter{\mx}{\ast}{i},\iter{\my}{\ast}{i},\iter{\mz}{\ast|x}{i}) = f(\iter{\mx}{\ast}{i+1},\iter{\my}{\ast}{i},\iter{\mz}{\ast|y}{i+1}) \\ = 
f(\iter{\mx}{\ast}{i+1},\iter{\my}{\ast}{i+1},\iter{\mz}{\ast|x}{i+1})
\end{multline}}
where \me{\{\iter{\mx}{\ast}{i+1},\iter{\my}{\ast}{i+1},\iter{\mz}{\ast|x}{i+1}\} \in \mc{F}^\ast} is a limit point of \me{\mc{A}} for a given initial point. Since the function \eqn{f} is strongly convex, the limit point is unique. As discussed in Appendix \ref{c-b}, the unique limit point of the sequence of iterates \me{\{\iterate{\ma}{i}\}} is a fixed point of \me{\mc{A}}. 



Using \cite[Th. 10]{lanckriet2009convergence} and \cite[Th. 2 and 11]{scutari_1}, we can show that the fixed points obtained by the algorithm \me{\mc{A}} under the mapping \me{f} is a stationary point of the nonconvex problem \eqref{con}. 


 In this section, we show that the limit point \eqn{\ma^{\ast}} of the sequence \eqn{\{\iterate{\ma}{i}\}}, obtained from \eqn{\mc{A}} as \eqn{i \rightarrow \infty} is a stationary point of the original nonconvex problem in \eqref{con}. In order for \eqn{\ma^{\ast}} to be a stationary point, it must satisfy the \ac{KKT} conditions of \eqref{con} and the Slater's constraint qualifications. Note that all points satisfies \eqn{\iterate{\ma}{i} \in \mc{Y}_\ast^{(i)} \subset \mc{F}}, and therefore the limit point \eqn{\ma^{\ast}} satisfies the constraint qualifications of the problem \eqref{con}.
 
 To show that the limit point \eqn{\mbf{\ma}^{\ast}} satisfies the \ac{KKT} expression of the nonconvex problem, we utilize the fact that the limit point \eqn{\mbf{\ma}^{\ast}} is the solution of the convex subproblem \eqref{con-m} upon convergence. Therefore, it satisfies the \ac{KKT} conditions of the convex subproblem for some Lagrange multipliers \eqn{\mu_i \geq 0} as 
 \begin{equation} \label{kkt_expr}
 \textstyle	\nabla f(\ma^{\ast}) + \mu_0 \big [ \nabla h(\ma^{\ast}) - \nabla\hat{g}_0(\ma^{\ast};\ma^{\ast}) \big ] + \sum_{i=1}^2 \mu_i \nabla g_i(\ma^{\ast}) = 0.
 \end{equation}
 It is evident that the modified objective \eqn{f} and the convex function \eqn{\hat{f}} in \eqref{mod_obj} are equal at the limit point due to the vanishing quadratic penalty term. Now, by utilizing the relation \eqn{\nabla \hat{g}_o(\mbf{\ma}^{\ast};\iterate{\ma}{\ast}) = \nabla g_o(\mbf{\ma}^{\ast})} between the \ac{DC} constraint \eqref{con-dc} and the corresponding convex approximation in \eqref{con-relax}, we can ensure that the limit point \eqn{\mbf{\ma}^{\ast}} indeed satisfies the \ac{KKT} condition of the nonconvex problem \eqref{con}. Now, by using \cite[Th. 2 and 11]{scutari_1} or \cite[Prop. 3.2]{amir}, we can show that all the limit points of the sequence \eqn{\{\iterate{\ma}{i}\}} generated by the algorithm \me{\mc{A}} are the \ac{KKT} points of \eqref{con}.


\end{comment}
