\reviewF{The authors have modified the manuscript taken into account the reviewers' comments. However, the proof of convergence still lacks of convincing argumentation to make this paper suitable for publication. Two main aspects are arguable as follows: }

\vspace{1eM}
\underline{\textit{Reply:}} We thank the reviewer for all constructive comments. We agree with the reviewer's concern on the claim that convergence proof still lacks the convincing arguments. We have completely rewritten Appendix A-D with similar kind of arguments used to discuss the convergence of projected gradient algorithms in Nonlinear Programming book as suggested by the reviewer. We hope that the proposed changes would make the convergence proof complete.

\begin{itemize}
	
	\item \reviewF{When discussing the convergence of the SCA algorithm (Appendix A-C), the authors claim that the SCA algorithm converges because the objective function is strictly decreasing and the minimizer in each iteration is unique. However, this is in general not enough to guarantee the convergence of the whole sequence. Instead, one can only claim that every limit point of the whole sequence is a stationary point, or globally optimal solution if the problem is convex. As an example, simply consider the gradient projection algorithm, where each subproblem is strongly convex and the objective function  is strictly decreasing but the whole sequence may not converge. The authors are referred to Bertsekas'  book: Nonlinear Programming.}

	\vspace{1eM}	
	\underline{\textit{Reply:}} We agree with the reviewer comment on the convergence of the sequence of beamformer iterates generated by the SCA algorithm. It is true that since the objective sequence is strictly monotonic and the minimizer is unique in each iteration, it does not guarantee the convergence of the sequence of iterates. We have removed our earlier claim on the convergence of beamformer iterates generated by the SCA algorithm. We now refer the minimizer obtained upon objective convergence is \eqn{\mbf{z}_\ast^{(i)}}, as presented in the paragraph following (51). Now, considering the whole sequence of beamformer iterates generated by repeatedly performing SCA and AO until the objective convergence, we can only show that the limit point of every convergent subsequence of iterates is a stationary point.	
	
	We have revised our convergence of beamformer iterates in Appendix A-D using subsequence convergence proof. In order to do so, the feasible set should be compact. Even though the original set \eqn{\mc{F}} of nonconvex problem is compact, the feasible sets \eqn{\mc{X}^{(i)}_k} and \eqn{\mc{Y}^{(i)}_k} of relaxed subproblems need not be compact on all scenarios due to the newly introduced optimization variable \eqn{\beta_{l,k,n}} to bound the interference. It happens when \eqn{\gamma_{l,k,n} \to 0}, \textit{i.e.}, when the \eqn{\ith{k}} user stream \eqn{l} is not active on sub-channel \eqn{n}. In such a case, \eqn{\beta_{l,k,n}} can take any value, since \eqn{\gamma_{l,k,n} \to 0} as  \eqn{\mvec{m}{l,k,n} \to 0} and \eqn{\mvec{w}{l,k,n} \to 0}. It has no impact on the solution of each convex subproblem even if \eqn{\beta_{l,k,n} \to \infty}. However to discuss the convergence of proposed iterative algorithms, the feasible set needs to be compact. 
	
	Therefore, we include an additional constraint on \eqn{\beta_{l,k,n}} as \eqn{\beta_{l,k,n} \leq P_{\max}} without altering the problem or solution for the sake of convergence analysis. Note that \eqn{\beta_{l,k,n}} can be bounded above with \eqn{P_{\max}}, since the total interference power on each stream cannot exceed the total transmit power, which follows from the passive behavior of wireless channels. Using this additional constraint, the feasible set of each subproblem is guaranteed to be compact, and therefore the sequence of iterates generated by the iterative algorithm is bounded. This information is included in the paragraph after (28) on page 6.
	
	Furthermore, the convergence discussions on the sequence of iterates generated by the iterative algorithm is presented in Appendix A-D. 
	
	\vspace{1eM}	
	\item \reviewF{Even under the assumption that the inner loop SCA converges, there are doubts with respect to the convergence of the alternating optimization algorithm described by (54). The authors cite [27] to justify convergence, but it is not straightforward to claim that the algorithmic model considered in this manuscript is the same as in [27]. More specifically, let us consider the variable \eqn{x} (so it is consistent with the authors' notation in Appendix A-D). In the algorithmic model of [27], at iteration \eqn{t+1}, all elements of \eqn{x} are updated simultaneously, based on \eqn{x^t}. But in the authors' model, the update of \eqn{x} is performed in two phases: in the first phase, only the elements of \eqn{m} and \eqn{\gamma} in \eqn{x} are updated based on \eqn{x^t}. In the second phase, the remaining elements of \eqn{x} are updated, i.e. \eqn{w} and \eqn{\gamma}, based on both \eqn{x^t} and the elements that have been updated in the first phase. This is not a trivial difference and thus the direct application of the conclusion from [27] in the current context cannot be taken for granted. Due to the open concerns after several revision rounds, it is recommended to reject the paper.}
	
	\vspace{1eM}
	\underline{\textit{Reply:}} We agree with the reviewer's concern on lack of relevant references to show that the proposed method of alternating optimization variables can be applicable to the Zangwill's model. In [R1, Appendix I], the convergence of the sequence of iterates generated by the alternating projections algorithm was discussed with Zangwill's model using composition of two subalgorithms. However, based on reviewer's suggestion, we have modified the convergence proof similar to the one provided for gradient projection method in Nonlinear programming book by Bertsekas.	
	
	In our earlier manuscript, the convergence of the sequence of beamformer iterates claim based on the Meyer's paper was incomplete. We apologize for the incomplete usage of the theorem presented in Meyer's paper [R2]. In [R2, Theorem. 3.1], it has been shown that if strict monotonicity of the objective sequence is guaranteed and the sequence of iterates is bounded, then either the sequence of beamformer iterates converges or the accumulation points of \eqn{\{x^t\}} is a continuum. Therefore, our earlier claim on the guaranteed convergence of the sequence of iterates is not correct as pointed out by the reviewer. The current approach of ensuring strict monotonic decrease in the objective sequence by using a quadratic regularization term in the objective as in projected gradient is discussed in [R2, Section 4] and in [R2, Lemma 4.1]. 
	
	\begin{itemize}
		\item[R1]  J. Tropp, I. Dhillon, R. Heath, and T. Strohmer, ``Designing structured tight frames via an alternating projection method," \emph{IEEE Trans. Inform. Theory}, vol. 51, no. 1, pp. 188â€“209, Jan. 2005.
		\item[R2] R.R. Meyer, ``Sufficient Conditions for the Convergence of Monotonic Mathematical Programming Algorithms," \emph{Journal of Computer and System Sciences}, vol. 12, no. 1, pp. 108-121, 1976
	\end{itemize}
	
\end{itemize}