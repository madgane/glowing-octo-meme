\reviewF{The authors have modified the manuscript taken into account the reviewers' comments. However, the proof of convergence still lacks of convincing argumentation to make this paper suitable for publication. Two main aspects are arguable as follows: }

\vspace{1eM}
\underline{\textit{Reply:}} We thank the reviewer for pointing out the flaw in our previous convergence proof. Based on the reviewer's suggestion, we have rewritten Appendix A-D with the subsequence convergence instead of claiming the sequence convergence. We hope that our current changes will be convincing enough to make the convergence proof complete.

In order to discuss the convergence of the sequence of iterates generated by the centralized method in Algorithm 1, we use a unified superscript index \eqn{t} to represent both \ac{AO} index \eqn{i} and \ac{SCA} indexing \eqn{k}. By doing so, we represent iterate \eqn{\mbf{x}^{t}} as the solution variable produced by the iterative algorithm in every \ac{SCA} \eqn{k} and \ac{AO} step {i}. Using this notation, we have revised our convergence discussions in Appendices A-C and A-D. 

As an observation, we have noted that the feasible set of nonconvex problem (16) need not be bounded, even if the feasible set of the original nonconvex problem in (6) and the relaxed \ac{MSE} reformulated problem in (26) is bounded. It happens in problem (16) because of the newly introduced optimization variables in (16b) and (16c) to relax the \ac{SINR} expression, since \eqn{\beta_{l,k,n}} can assume any value satisfying (16c) without violating any other constraints in (16) when \eqn{\gamma_{l,k,n} = 0}. Therefore, to ensure boundedness of the feasible set of problem (16), we can include an additional bounding constraint on \eqn{\beta_{l,k,n}} as \eqn{\beta_{l,k,n} \leq B_{\max}}, where \eqn{B_{\max}} can be a maximum interference threshold seen by any user in the network for a given maximum transmit power budget \eqn{P_{\max}}. Note that the additional constraint on \eqn{\beta_{l,k,n}} will not alter the solution space of other optimization variables. However, as a consequence of considering the regularized objective in (46b), the additional constraint on \eqn{\beta_{l,k,n}} is not required, since \eqn{\beta_{l,k,n}} is bounded due to the proximal term in (46b). We have highlighted this discussion on page 6, after (28). 

\begin{itemize}
	
	\item \reviewF{When discussing the convergence of the \ac{SCA} algorithm (Appendix A-C), the authors claim that the \ac{SCA} algorithm converges because the objective function is strictly decreasing and the minimizer in each iteration is unique. However, this is in general not enough to guarantee the convergence of the whole sequence. Instead, one can only claim that every limit point of the whole sequence is a stationary point, or globally optimal solution if the problem is convex. As an example, simply consider the gradient projection algorithm, where each subproblem is strongly convex and the objective function  is strictly decreasing but the whole sequence may not converge. The authors are referred to Bertsekas'  book: Nonlinear Programming.}

	\vspace{1eM}	
	\underline{\textit{Reply:}} We agree with the reviewer's comment. Convergence of iterates cannot be guaranteed based on the strict monotonicity of the objective sequence and the uniqueness of minimizer in each step. We apologize for our emphasis on the convergence of sequence of iterates. Based on the reviewer's suggestion, we have modified the convergence analysis for the sequence of iterates generated by the centralized method in Appendices A-C and A-D. The revised discussion on the convergence of iterates is presented briefly here for reference. Since the feasible set is bounded, the iterates generated by the iterative algorithm are bounded. Therefore, there exists at least one subsequence that converges to a limit point. Now, by using a convergent subsequence of the sequence of iterates, we argue that every limit point is a stationary point of the original nonconvex problem. 
		
	\vspace{1eM}	
	\item \reviewF{Even under the assumption that the inner loop SCA converges, there are doubts with respect to the convergence of the alternating optimization algorithm described by (54). The authors cite [27] to justify convergence, but it is not straightforward to claim that the algorithmic model considered in this manuscript is the same as in [27]. More specifically, let us consider the variable \eqn{x} (so it is consistent with the authors' notation in Appendix A-D). In the algorithmic model of [27], at iteration \eqn{t+1}, all elements of \eqn{\mbf{x}} are updated simultaneously, based on \eqn{\sol{t}}. But in the authors' model, the update of \eqn{\mbf{x}} is performed in two phases: in the first phase, only the elements of \eqn{\mbf{m}} and \eqn{\mbf{\gamma}} in \eqn{\mbf{x}} are updated based on \eqn{\sol{t}}. In the second phase, the remaining elements of \eqn{\mbf{x}} are updated, i.e. \eqn{\mbf{w}} and \eqn{\mbf{\gamma}}, based on both \eqn{\sol{t}} and the elements that have been updated in the first phase. This is not a trivial difference and thus the direct application of the conclusion from [27] in the current context cannot be taken for granted. Due to the open concerns after several revision rounds, it is recommended to reject the paper.}
	
	\vspace{1eM}
	\underline{\textit{Reply:}} We understand the reviewer's concern on using [R1] to our problem involving both \ac{SCA} and \ac{AO} updates, which solves only for a subset of optimization variables by keeping others fixed. Even though we have revised our convergence proof based on a convergent subsequence of the original sequence without referring [R1], nevertheless, we clarify our usage of [R1] in our previous manuscript for the convergence of iterates. We refer to [R2] for a similar usage of [R1] while discussing the convergence of \ac{AO} method used in designing tight frames by solving matrix nearness problem. The usage of [R1] to address the convergence of \ac{AO} method in [R2] is based on the composition of two sub-algorithms and the compactness of the feasible set, where each sub-algorithm updates only a subset of optimization variables. It is discussed briefly in [Appendix I and II, R2] and in particular for \ac{AO} algorithm in [Appendix I-F, R2].
	
	However, we note that in our previous manuscript, convergence of the sequence of beamformer iterates claim based on [R1] was incomplete and we apologize for that. In [Theorem. 3.1, R1], it has been shown that if the strict monotonicity of the objective sequence is ensured and the iterates are bounded, then \textit{either the sequence of iterates converges or the accumulation points of \eqn{\{\sol{t}\}} is a continuum}. Therefore, our earlier claim on the guaranteed convergence of the sequence of iterates is not correct as pointed out by the reviewer. Similar approach of restricting the solution set with a proximal operator to ensure strict monotonicity of the objective sequence is highlighted in [Section 4, R1] under restrictive mapping. Therefore, by using the discussions in [R1], we can only claim that the sequence of iterates \eqn{\{\sol{t}\}} converges to a continuum of accumulation points, and every limit point is a stationary point.
	
	We have revised Appendix A-D titled \textit{"Stationarity of Limit Points"} to discuss the convergence of the sequence of iterates. We have shown that every limit point of the sequence of iterates generated by the centralized algorithm is a stationary point.
	
	\vspace{1eM}
	\begin{itemize}		
		\item[R1.] R.R. Meyer, ``Sufficient Conditions for the Convergence of Monotonic Mathematical Programming Algorithms," \emph{Journal of Computer and System Sciences}, vol. 12, no. 1, pp. 108-121, 1976
		\item[R2.] J. Tropp, I. Dhillon, R. Heath, and T. Strohmer, ``Designing structured tight frames via an alternating projection method," \emph{IEEE Trans. Inform. Theory}, vol. 51, no. 1, pp. 188â€“209, Jan. 2005.
	\end{itemize}
	
\end{itemize}