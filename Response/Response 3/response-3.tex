\reviewF{The authors have modified the manuscript taken into account the reviewers' comments. However, the proof of convergence still lacks of convincing argumentation to make this paper suitable for publication. Two main aspects are arguable as follows: }

\vspace{1eM}
\underline{\textbf{Response:}} We thank the reviewer for pointing out the mistake in our previous convergence proof. Based on the reviewer's comments and suggestions, we have rewritten Appendix A-D using subsequence convergence instead of claiming the sequence convergence. We hope that our current changes will be convincing enough to make the convergence proof complete.

%As an observation, we have noted that the feasible set of nonconvex problem (16) need not be bounded, even if the feasible set of the original nonconvex problem in (6) and the relaxed \ac{MSE} reformulated problem in (26) is bounded. It happens in problem (16) because of the newly introduced optimization variables in (16b) and (16c) to relax the \ac{SINR} expression, since \eqn{\beta_{l,k,n}} can assume any value satisfying (16c) without violating any other constraints in (16) when \eqn{\gamma_{l,k,n} = 0}. Therefore, to ensure boundedness of the feasible set of problem (16), we can include an additional bounding constraint on \eqn{\beta_{l,k,n}} as \eqn{\beta_{l,k,n} \leq B_{\max}}, where \eqn{B_{\max}} can be a maximum interference threshold seen by any user in the network for a given maximum transmit power budget \eqn{P_{\max}}. Note that the additional constraint on \eqn{\beta_{l,k,n}} will not alter the solution space of other optimization variables. However, as a consequence of considering the regularized objective in (46b), the additional constraint on \eqn{\beta_{l,k,n}} is not required, since \eqn{\beta_{l,k,n}} is bounded due to the proximal term in (46b). We have highlighted this discussion on page 6, after (28). 

\begin{enumerate}
	
	\item \reviewF{When discussing the convergence of the \ac{SCA} algorithm (Appendix A-C), the authors claim that the \ac{SCA} algorithm converges because the objective function is strictly decreasing and the minimizer in each iteration is unique. However, this is in general not enough to guarantee the convergence of the whole sequence. Instead, one can only claim that every limit point of the whole sequence is a stationary point, or globally optimal solution if the problem is convex. As an example, simply consider the gradient projection algorithm, where each subproblem is strongly convex and the objective function  is strictly decreasing but the whole sequence may not converge. The authors are referred to Bertsekas'  book: Nonlinear Programming.}

	\vspace{1eM}
	\underline{\textbf{Response:}} We thank the reviewer for pointing out that the sequence of iterates generated by the \ac{SCA} algorithm in each \ac{AO} step need not be convergent. Based on the reviewer's reference to the gradient projection algorithm in Nonlinear Programming textbook, we have realized that our earlier convergence proof is not mathematically rigorous. We apologize for our emphasis on the convergence of iterates in our previous manuscript. As suggested by the reviewer, stationarity of limit points of the whole sequence of iterates is now addressed in Appendix A-D by using the following statements. 
	\begin{itemize}
		\item The sequence of iterates generated by the algorithm is bounded, therefore, we can only claim that there exist at least one subsequence that converges to a limit point of the original sequence.
		\item Using a convergent subsequence of the original sequence of iterates, we then discuss that every limit point is a stationary point of nonconvex problem in Appendix A-D.
	\end{itemize}
	
	We have made changes in Appendices A-C and A-D to address the concerns raised by the reviewer.
				
	\vspace{1eM}	
	\item \reviewF{Even under the assumption that the inner loop SCA converges, there are doubts with respect to the convergence of the alternating optimization algorithm described by (54). The authors cite [27] to justify convergence, but it is not straightforward to claim that the algorithmic model considered in this manuscript is the same as in [27]. More specifically, let us consider the variable \eqn{x} (so it is consistent with the authors' notation in Appendix A-D). In the algorithmic model of [27], at iteration \eqn{t+1}, all elements of \eqn{\mbf{x}} are updated simultaneously, based on \eqn{\sol{t}}. But in the authors' model, the update of \eqn{\mbf{x}} is performed in two phases: in the first phase, only the elements of \eqn{\mbf{m}} and \eqn{\mbf{\gamma}} in \eqn{\mbf{x}} are updated based on \eqn{\sol{t}}. In the second phase, the remaining elements of \eqn{\mbf{x}} are updated, i.e. \eqn{\mbf{w}} and \eqn{\mbf{\gamma}}, based on both \eqn{\sol{t}} and the elements that have been updated in the first phase. This is not a trivial difference and thus the direct application of the conclusion from [27] in the current context cannot be taken for granted. Due to the open concerns after several revision rounds, it is recommended to reject the paper.}
	
	\vspace{1eM}
	\underline{\textbf{Response:}} We understand the reviewer's concern on using [R1] to our problem involving both \ac{SCA} and \ac{AO} updates. In each iteration, the proposed method solves only a subset of optimization variables by keeping others fixed, therefore, we cannot use [R1] directly as suggested by the reviewer. Even though we have revised our convergence proof based on a convergent subsequence without referring to [R1], nevertheless, we clarify our usage of [R1] in our previous manuscript for the convergence of iterates. 
	
	\begin{itemize}
		\item We refer to [R2] for a similar usage of [R1] while discussing the convergence of \ac{AO} method used in designing tight frames by solving matrix nearness problem. The reference [R1] used to discuss the convergence of iterates generated by the \ac{AO} method in [R2] by referring \ac{AO} update as a composition of two sub-algorithms, wherein each sub-algorithm updates only a subset of optimization variables. It has been discussed comprehensively in [R2, Appendices I and II]. In particular, the discussion on referring \ac{AO} as two sub-algorithms is presented in [R2, Appendix I-F].
		\item However, we apologize for the incomplete usage of [R1, Theorem 3.1] in our previous manuscript to claim the convergence of beamformer iterates. In [R1, Theorem. 3.1], it has been shown that if the strict monotonicity of the objective sequence is ensured and the iterates are bounded, then \textit{either the sequence of iterates converges or the accumulation points of \eqn{\{\sol{t}\}} is a continuum}.
		\item Therefore, our previous claim on the guaranteed convergence of iterates is not correct, as pointed out by the reviewer. 
		\item Similar approach of restricting the solution set with a proximal operator to ensure strict monotonicity of the objective sequence is presented in [R1, Section 4] under restrictive mapping section. Therefore, even if we use the discussions in [R1], we can only claim that the sequence of iterates \eqn{\{\sol{t}\}} converges to a continuum of accumulation points, and every limit point is a stationary point.
	\end{itemize}
	
	We have revised Appendix A-D titled \textit{"Stationarity of Limit Points"} to discuss the convergence of the sequence of iterates. We have revised our claim as every limit point of the sequence of iterates generated by the centralized algorithm is a stationary point.
	
	\vspace{1eM}
	\begin{itemize}		
		\item[R1.] R.R. Meyer, ``Sufficient Conditions for the Convergence of Monotonic Mathematical Programming Algorithms," \emph{Journal of Computer and System Sciences}, vol. 12, no. 1, pp. 108-121, 1976
		\item[R2.] J. Tropp, I. Dhillon, R. Heath, and T. Strohmer, ``Designing structured tight frames via an alternating projection method," \emph{IEEE Trans. Inform. Theory}, vol. 51, no. 1, pp. 188â€“209, Jan. 2005.
	\end{itemize}
	
\end{enumerate}