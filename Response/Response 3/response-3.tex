\reviewF{The authors have modified the manuscript taken into account the reviewers' comments. However, the proof of convergence still lacks of convincing argumentation to make this paper suitable for publication. Two main aspects are arguable as follows: }

\vspace{1eM}
\underline{\textit{Reply:}} We thank the reviewer for pointing the flaw in our earlier convergence proof. Based on the reviewer's suggestion, we have rewritten Appendix A-D with similar arguments used to discuss the convergence of projected gradient algorithms in [R3]. We hope that our current changes will be convincing enough to make the convergence proof complete.

\begin{itemize}
	
	\item \reviewF{When discussing the convergence of the \ac{SCA} algorithm (Appendix A-C), the authors claim that the \ac{SCA} algorithm converges because the objective function is strictly decreasing and the minimizer in each iteration is unique. However, this is in general not enough to guarantee the convergence of the whole sequence. Instead, one can only claim that every limit point of the whole sequence is a stationary point, or globally optimal solution if the problem is convex. As an example, simply consider the gradient projection algorithm, where each subproblem is strongly convex and the objective function  is strictly decreasing but the whole sequence may not converge. The authors are referred to Bertsekas'  book: Nonlinear Programming.}

	\vspace{1eM}	
	\underline{\textit{Reply:}} We agree with the reviewer's comment. The convergence of iterates cannot be guaranteed based on the strict monotonicity of the objective sequence and the uniqueness of minimizer in each step. We apologize for our emphasis on the convergence of sequence of iterates. In view of reviewer's suggestion, we have revised our claim based on subsequence convergence theory. 
		
	Since the feasible set is bounded, the iterates generated by the iterative algorithm are bounded. Therefore, there exists at least one subsequence that converges to a limit point. Now, by using a convergent subsequence of original sequence of iterates, we argued that every limit point is a stationary point of the original nonconvex problem. In order to do so, we used a unified superscript index \eqn{t} to represent both \ac{AO} index \eqn{i} and \ac{SCA} indexing \eqn{k}. Now, by using this unified index \eqn{t}, we represent iterate \eqn{\mbf{x}^{t}} as the solution variable produced by the iterative algorithm in every \ac{SCA} \eqn{k} and \ac{AO} step {i}. By using this notation and the subsequence argument, we have revised our convergence discussions in Appendices A-C and A-D. 
		
	Even though the feasible sets of the original nonconvex problem in (6) and the relaxed \ac{MSE} reformulated problem in (26) are bounded due to the total power constraint, we noted that the solution space of the relaxed nonconvex problem (16) need not be bounded. It is because of the newly introduced variables in (16b) and (16c) to relax the \ac{SINR} expression, since \eqn{\beta_{l,k,n}} can assume any value satisfying (16c) without violating any other constraints in (16) when \eqn{\gamma_{l,k,n} = 0}. In order to ensure boundedness of the feasible set, we can include an additional bounding constraint on \eqn{\beta_{l,k,n}} as \eqn{\beta_{l,k,n} \leq B_{\max}}, where \eqn{B_{\max}} can be a maximum interference threshold seen by any user in the network with the maximum transmit power constraint of \eqn{P_{\max}}. Note that the additional bounding constraint on \eqn{\beta_{l,k,n}} will not alter the solution space of other optimization variables. However, as a consequence of considering the regularized objective in (46b), the additional constraint on \eqn{\beta_{l,k,n}} is not required, since \eqn{\beta_{l,k,n}} is bounded due to the proximal term in (46b). We have highlighted this information on page 6, after (28). 
	
	\vspace{1eM}	
	\item \reviewF{Even under the assumption that the inner loop SCA converges, there are doubts with respect to the convergence of the alternating optimization algorithm described by (54). The authors cite [27] to justify convergence, but it is not straightforward to claim that the algorithmic model considered in this manuscript is the same as in [27]. More specifically, let us consider the variable \eqn{x} (so it is consistent with the authors' notation in Appendix A-D). In the algorithmic model of [27], at iteration \eqn{t+1}, all elements of \eqn{\mbf{x}} are updated simultaneously, based on \eqn{\sol{t}}. But in the authors' model, the update of \eqn{\mbf{x}} is performed in two phases: in the first phase, only the elements of \eqn{\mbf{m}} and \eqn{\mbf{\gamma}} in \eqn{\mbf{x}} are updated based on \eqn{\sol{t}}. In the second phase, the remaining elements of \eqn{\mbf{x}} are updated, i.e. \eqn{\mbf{w}} and \eqn{\mbf{\gamma}}, based on both \eqn{\sol{t}} and the elements that have been updated in the first phase. This is not a trivial difference and thus the direct application of the conclusion from [27] in the current context cannot be taken for granted. Due to the open concerns after several revision rounds, it is recommended to reject the paper.}
	
	\vspace{1eM}
	\underline{\textit{Reply:}} We understand the reviewer's concern on using [R1] to our problem involving \ac{SCA} and \ac{AO} updates, which involves partial update of optimization variables. Even though our revised convergence proof is not based on [R1], we clarify our usage of [R1] for \ac{AO} update. Therefore, we refer to [R2], where the problem considered was to design tight frames based on solving matrix nearness problem using \ac{AO} approach. In order to prove the convergence of the sequence of iterates generated by the \ac{AO} iterations, [R1] is used with the composition of two sub-algorithms as discussed briefly in [Appendix I and II, R2].
		
	We note that in our previous manuscript, the convergence of the sequence of beamformer iterates claim based on [R1] was incomplete. We apologize for the incomplete usage of the theorem presented in [R1]. In [Theorem. 3.1, R1], it has been shown that if the strict monotonicity of the objective sequence is ensured and the iterates are bounded, then \textit{either the sequence of iterates converges or the accumulation points of \eqn{\{\sol{t}\}} is a continuum}. Therefore, our earlier claim on the guaranteed convergence of the sequence of iterates is not correct as pointed out by the reviewer. Ensuring strict monotonic decrease in the objective sequence by restricting the solution set is discussed briefly in [Section 4, R1]. By using [R1], we can only claim the convergence of subsequence of \eqn{\{\sol{t}\}} and not the sequence itself.
	
	We revised Appendix A-D titled \textit{"Stationarity of Limit Points"} to discuss briefly on the convergence of iterates. We show that every limit point of the sequence of iterates generated by the centralized algorithm is a stationary point.
	
	\vspace{1eM}
	\begin{itemize}
		
		\item[R1.] R.R. Meyer, ``Sufficient Conditions for the Convergence of Monotonic Mathematical Programming Algorithms," \emph{Journal of Computer and System Sciences}, vol. 12, no. 1, pp. 108-121, 1976
		\item[R2.] J. Tropp, I. Dhillon, R. Heath, and T. Strohmer, ``Designing structured tight frames via an alternating projection method," \emph{IEEE Trans. Inform. Theory}, vol. 51, no. 1, pp. 188â€“209, Jan. 2005.
		\item[R3.] D. P. Bertsekas, ``Nonlinear Programming," 2nd ed., \emph{Athena Scientific}, 1999.
	\end{itemize}
	
\end{itemize}