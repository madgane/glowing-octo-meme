Comments:
\review{The response to the reviewer's concerns are generally satisfying, except the convergence proof.}

\vspace{1eM}
\underline{\textit{Reply:}} We thank the reviewer for providing valuable and insightful comments.

\vspace{1eM}
\review{For the resubmitted manuscript, the reviewer still has the following concerns}

\begin{enumerate}
\cmnt{1} \review{Considering the length of the manuscript, it would be better to shorten some parts that are not new in this manuscript, e.g. III.A. More space can be left for convergence proof, which is very important. }

\resp We have removed a couple of paragraphs from Section III-A and shortened the discussions on Section V (\textit{i.e.}, the simulation results) to provide additional details on the convergence proof. We believe the removed parts do not affect the readability of the paper and the added text regarding the convergence analysis certainly improves the quality. 

\cmnt{2} \review{In convergence proof (48), why does the 2nd inequality hold? In fact, to prove the feasibility of  \eqn{{m_{k+1}^{(i)}, w_{*}^{(i-1)};m_k^{(i)} }}, the part between 2nd and 3rd inequality is not necessary, \eqn{\leq 0} directly follows the 2nd inequality since the solution \eqn{m_{k+1}^{(i)}, \gamma_{k+1}^{(i)} } is the optimal solution, and therefore feasible.  }

\resp We have removed the additional inequality relating the previous operating point from (49) in the revised manuscript. 

\cmnt{3} \review{The solutions SCA iterations \eqn{\mbf{m}^{(i)}_k} does not necessarily converge. In fact \eqn{\mbf{m}} has compact feasible region, and thus \eqn{\mbf{m}^{(i)}_k} has limit points for any specific \eqn{i}.  However \eqn{m_{*}^{(i)}} does not necessarily exist (the whole sequence \eqn{\mbf{m}^{(i)}_k} may be not convergent). Similar problem happens to \eqn{\mbf{w}^{(i)}_k}. }

\resp We thank the reviewer for the comment. It is true that the sequence of iterates generated by the iterative \ac{SCA} algorithm, namely, \eqn{\mbf{m}^{(i)}_k} and \eqn{\mbf{w}^{(i)}_k} need not converge even if the objective sequence converges. It follows from the fact that there can be multiple minimizers (solutions) for the convex subproblem in each \ac{SCA} step. However, we have modified the convergence discussion using the strong convexity argument by regularizing the objective of each \ac{SCA} subproblem with a quadratic term as discussed in [30] and [31]. The additional quadratic term ensures the uniqueness of the minimizer in each of the \ac{SCA} subproblem. Due to the uniqueness of the minimizer and the strict monotonicity of the objective sequence in each \ac{SCA} step, the convergence of the iterates is guaranteed. The above discussion is included in Appendix A-B around (46) and in Appendix A-C after (50).

\cmnt{4} \review{Strict monotonicity with respect to the objective function \eqn{f} should be rigorously proved. Note that to guarantee the uniqueness of the beamformer iterates, (52) instead of the objective function is used.} 

\resp As suggested by the reviewer, we have included the argument to show the strict monotonicity of the objective sequence in Appendix A-C following (52). Moreover, we have also mentioned that we in fact use the regularized objective in (46b) to analytically prove the convergence of the iterates, although we have numerically observed that the objective sequence of Algorithm 1 (with the original objective) always converges. Please refer to the paragraph before Section III-C.

\begin{comment}
Note that the objective sequence \eqn{\{f(\mbf{x}_k)\}} generated by an iterative \ac{SCA} algorithm is monotonic, \textit{i.e}, \eqn{f(\mbf{x}_k) \leq f(\mbf{x}_{k-1})} and holds with equality at the limit point of the iterative procedure. However, upon convergence, due to the convexity of the objective, we may have multiple limit points, say, \eqn{\mbf{y}_{\ast}} with the same objective as \eqn{f(\mbf{y}_{\ast}) = f(\mbf{x}_{\ast})}. Therefore, strict monotonicity cannot be guaranteed for the objective sequence. However, when the objective function is strongly convex with a parameter \eqn{m > 0}, as discussed in Appendix A-B, the minimizer for the subproblem in each \ac{SCA} iteration \eqn{k} is unique as
\begin{equation} \label{eqn-aa}
f(\ma_{k}) - f(\ma_{k+1}) \geq  \nabla f(\ma_{k+1})^\tran (\ma_{k} - \ma_{k+1}) + m \|\ma_{k} - \ma_{k+1}\|^2
\end{equation}
where \eqn{\ma_{k+1}} is the optimal solution for the \eqn{\ith{k}} \ac{SCA} subproblem and \eqn{\ma_{k}} is the unique minimizer obtained in the previous \eqn{\ith{k-1}} \ac{SCA} step, which is a feasible point for the current problem. Since \eqn{\ma_{k+1}} is the solution in the \eqn{\ith{k}} \ac{SCA} problem, following conditions hold
\begin{subeqnarray}
\nabla f(\ma_{k+1})^\tran (\ma_{k} - \ma_{k+1}) &\geq& 0 \slabel{eqn-a} \\
f(\ma_{k}) - f(\ma_{k+1}) &\geq& m \|\ma_{k} - \ma_{k+1}\|^2 \slabel{eqn-b}
\end{subeqnarray}
where \eqref{eqn-a} is due to the lack of any descent direction in the feasible set and \eqref{eqn-b} follows from \eqref{eqn-aa} using \eqref{eqn-a}. As \eqn{k \rightarrow \infty, \|\ma_{k+1} - \ma_{k}\| \rightarrow 0}, and therefore the iterative algorithm converges to a unique minimizer, say, \eqn{\ma_{\ast}}, which is the limit point of the algorithm. Now by using the monotonicity of the \ac{SCA} updates and the uniqueness of the minimizer in each step, we can guarantee the strict monotonicity of the objective sequence generated by the iterative problem. 
\end{comment}

\cmnt{5} \review{Note that the conclusions [32, Thm 2] and [26, Thm 10] have lots of assumptions. To invoke these reference, explicit exposition should be provided to show that these conclusions can be applied to our problem. The same questions occur to the proof in Appendix B, where conclusions in [11] [36] and [37] are used. Too many details are omitted to make the proof convincing and clear. }

\resp We have updated the manuscript to include the details regarding the stationary point discussion in Appendix A-E. The convergence analysis for the primal and the \ac{ADMM} algorithms are presented in Appendix B. We have also mentioned the conditions required for showing the stationarity property of the limit point and included more mathematical steps to make the convergence proof of distributed approaches rigorous.

\end{enumerate}
