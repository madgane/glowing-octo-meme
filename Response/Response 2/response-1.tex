Comments:
\review{The response to the reviewer's concerns are generally satisfying, except the convergence proof.}

\vspace{1eM}
\underline{\textit{Reply:}} We thank the reviewer for providing valuable and insightful comments.

\vspace{1eM}
For the resubmitted manuscript, the reviewer still has the following concerns

\begin{enumerate}
\cmnt{1} \review{Considering the length of the manuscript, it would be better to shorten some parts that are not new in this manuscript, e.g. III.A. More space can be left for convergence proof, which is very important. }

\resp After a careful check of the manuscript, we have removed couple of paragraphs from Section III.A and shortened the discussions on the simulation section to provide additional details for the convergence proof. We believe the removed parts don't affect the readability of the paper and the added text regarding convergence proof certainly improves the quality. 

\cmnt{2} \review{In convergence proof (48), why does the 2nd inequality hold? In fact, to prove the feasibility of  \eqn{{m_{k+1}^{(i)}, w_{*}^{(i-1)};m_k^{(i)} }}, the part between 2nd and 3rd inequality is not necessary, \eqn{\leq 0} directly follows the 2nd inequality since the solution \eqn{m_{k+1}^{(i)}, \gamma_{k+1}^{(i)} } is the optimal solution, and therefore feasible.  }

\resp We thank the reviewer for the critical comment. It is not possible to define the inequality with the previous optimal point. Since it is not possible to comment on the inclusion of the previous constraint set in the current update (except the earlier optimal point), the inequality is not guaranteed. Based on the comment from the reviewer, we have removed the inequality specifying the previous operating point from (48), since the newly found optimal point is inside the feasible set. 

\cmnt{3} \review{The solutions SCA iterations \eqn{\mbf{m}^{(i)}_k} does not necessarily converge. In fact \eqn{\mbf{m}} has compact feasible region, and thus \eqn{\mbf{m}^{(i)}_k} has limit points for any specific \eqn{i}.  However \eqn{m_{*}^{(i)}} does not necessarily exist( the whole sequence \eqn{\mbf{m}^{(i)}_k} may be not convergent). Similar problem happens to \eqn{\mbf{w}^{(i)}_k}. }

\resp We understand the reviewer concern. Even though the \ac{SCA} algorithm converges, it is not guaranteed that the iterates involved in the iterative algorithm, namely, \eqn{\mbf{m}^{(i)}_k} and \eqn{\mbf{w}^{(i)}_k} to converge. It is true that the iterates need not converge when the objective function is convex. We have modified the discussion on the strong convexity of the objective function to impose the uniqueness of the iterates in each \ac{SCA} update as well. By regularizing the objective function with a strongly convex term like \eqn{\|\mbf{m} - \mbf{m}^{(i)}_k\|^2}, we can guarantee the uniqueness of the iterates upon the \ac{SCA} convergence. We thank the reviewer for citing this issue on the sequence convergence and the uniqueness of the minimizer. We have included this information in the centralized convergence proof on Appendix A-B after (44), after (49) and on Appendix A-C to describe the strong convexity.

\cmnt{4} \review{Strict monotonicity with respect to the objective function \eqn{f} should be rigorously proved. Note that to guarantee the uniqueness of the beamformer iterates, (52) instead of the objective function is used.} 

\resp We thank the reviewer for the pointing out the issue involving the strict monotonicity. As suggested by the reviewer, we have included Appendix A-D to discuss the strict monotonicity of the objective function in each update point of the algorithm. 

Let \eqn{\mbf{z}} be the stacked vector of optimization variables that can be either \eqn{[\mx,\iter{\my}{\ast}{i-1},\mz]} or \eqn{[\iter{\mx}{\ast}{i},\my,\mz]} depending on the optimization problem that we are trying to solve, \textit{i.e}, either solving for \eqn{\mx} and \eqn{\mz} by fixing \eqn{\iter{\my}{\ast}{i-1}} or solving for \eqn{\my} and \eqn{\mz} by keeping \eqn{\iter{\mx}{\ast}{i}} fixed. 

Due to the strong convexity of the modified objective \eqn{f(\mbf{z})}, there exist an unique minimizer in each \ac{SCA} step \eqn{k}. Moreover, in each \ac{SCA} step \eqn{k}, the feasible set \eqn{\mc{X}_{k}^{(i)}} includes the earlier optimal point \eqn{\mbf{z}^{(i)}_k}, therefore, the objective decreases strictly unless \eqn{\mbf{z}^{(i)}_k} is a fixed point or a limit point of the \ac{SCA} iterations [22]. Following the strong convexity of \eqn{f(\mbf{z})}, we have
\begin{equation} \label{strict_monotonicity}
f(\mbf{z}) \geq f(\mbf{z}_{\ast}^{(i)}) + \nabla f(\mbf{z}_{\ast}^{(i)}) (\mbf{z} - \mbf{z}_{\ast}^{(i)}) + c \|\mbf{z} - \mbf{z}_{\ast}^{(i)}\|^2, \forall \mbf{z}\in \mc{X}^{(i)}_\ast
\end{equation}
where \eqn{\mbf{z}_{\ast}^{(i)}} is a limit point as \eqn{k \rightarrow \infty} for the \eqn{\ith{i}} \ac{AO} iteration. The equality in \eqref{strict_monotonicity} is valid iff \eqn{\mbf{z} = \mbf{z}_{\ast}^{(i)}}. Strict monotonicity of \eqn{f(\mbf{z}_k^{(i)})} is ensured in the \ac{SCA} steps by the additional quadratic term, which holds with equality at the limit point \eqn{\mbf{z}_{\ast}^{(i)} \in \mc{X}_{\ast}^{(i)}}.

To ensure strict monotonicity of the objective while swapping the optimization variables from \eqn{\mx,\mz} to \eqn{\my,\mz} in the \eqn{\ith{i}} \ac{AO} iteration, we rely on the discussions to show the inequality (51). Note that the modified objective \eqn{f} is strongly convex, and therefore has a unique minimizer in each \ac{SCA} step. Additionally, the feasible set \eqn{\mc{Y}_0^{(i)}} for \eqn{k=0} \ac{SCA} step with the variables \eqn{\my,\mz} includes the previous minimizer \eqn{\{\iter{\mx}{\ast}{i},\iter{\my}{\ast}{i-1},\iter{\mz}{\ast|\my}{i}\}}, thereby having strictly decreasing objective in (51). The equality in (51) holds for the limit point \eqn{\{\iter{\mx}{\ast}{i},\iter{\my}{\ast}{i},\iter{\mz}{\ast|\mx}{i}\}}. Using the above arguments, we can ensure the strict monotonicity of the overall objective sequence.

\cmnt{5} \review{Note that the conclusions [32, Thm 2] and [26, Thm 10] have lots of assumptions. To invoke these reference, explicit exposition should be provided to show that these conclusions can be applied to our problem. The same questions occur to the proof in Appendix B, where conclusions in [11] [36] and [37] are used. Too many details are omitted to make the proof convincing and clear. }

\resp We thank the reviewer for raising the concern. We have updated the manuscript to include the details regarding the stationary point discussion in Appendix A-F and the convergence proof analysis for the primal and the \ac{ADMM} algorithms in Appendix B. Additional detail includes the conditions required for showing the stationarity of the limit point and we have included more details to utilize the conclusions from [37] to show the convergence of the ADMM scheme.

\end{enumerate}
