Comments:
\review{The response to the reviewer's concerns are generally satisfying, except the convergence proof.}

\vspace{1eM}
\underline{\textit{Reply:}} We thank the reviewer for providing valuable and insightful comments.

\vspace{1eM}
For the resubmitted manuscript, the reviewer still has the following concerns

\begin{enumerate}
\cmnt{1} \review{Considering the length of the manuscript, it would be better to shorten some parts that are not new in this manuscript, e.g. III.A. More space can be left for convergence proof, which is very important. }

\resp After a careful check of the manuscript, we have removed a couple of paragraphs from Section III-A and shortened the discussions on Section V (\textit{i.e.}, the simulation results) to provide additional details for the convergence proof. We believe the removed parts do not affect the readability of the paper and the added text regarding convergence proof certainly improves the quality. 

\cmnt{2} \review{In convergence proof (48), why does the 2nd inequality hold? In fact, to prove the feasibility of  \eqn{{m_{k+1}^{(i)}, w_{*}^{(i-1)};m_k^{(i)} }}, the part between 2nd and 3rd inequality is not necessary, \eqn{\leq 0} directly follows the 2nd inequality since the solution \eqn{m_{k+1}^{(i)}, \gamma_{k+1}^{(i)} } is the optimal solution, and therefore feasible.  }

\resp We thank the reviewer for the critical comment. Based on the comment by the reviewer, we have removed the additional inequality relating the previous operating point from (49) in the revised manuscript. 

\cmnt{3} \review{The solutions SCA iterations \eqn{\mbf{m}^{(i)}_k} does not necessarily converge. In fact \eqn{\mbf{m}} has compact feasible region, and thus \eqn{\mbf{m}^{(i)}_k} has limit points for any specific \eqn{i}.  However \eqn{m_{*}^{(i)}} does not necessarily exist( the whole sequence \eqn{\mbf{m}^{(i)}_k} may be not convergent). Similar problem happens to \eqn{\mbf{w}^{(i)}_k}. }

\resp  We thank the reviewer for citing the issue regarding the sequence convergence. Even though the objective converges in each \ac{SCA} update, it is not guaranteed that the iterates involved in the iterative algorithm, namely, \eqn{\mbf{m}^{(i)}_k} and \eqn{\mbf{w}^{(i)}_k} to converge. It follows from the convexity of the original function, which can have multiple solutions. However, we have modified the convergence discussion using the strong convexity argument by regularizing the objective function with a quadratic term as discussed in [31] and [32], which ensures the uniqueness of the solution in each \ac{SCA} update. 

Since our problem is (highly) nonconvex and there are possibly many stationary points. If we start from different initial points, we may end up with different stationary points. That is to say, the limit point of \ac{SCA} approach is not unique. However, by regularizing the objective with a small quadratic term, the iterate in each step of \ac{SCA} algorithm is unique. This and the fact that the set is compact ensure the \ac{SCA} converges to a limit point. We have updated the manuscript to include this information in the convergence proof for the centralized algorithm in Appendix A. The modifications are included in Appendix A-B around (46) and in Appendix A-C after (50).

\cmnt{4} \review{Strict monotonicity with respect to the objective function \eqn{f} should be rigorously proved. Note that to guarantee the uniqueness of the beamformer iterates, (52) instead of the objective function is used.} 

\resp We thank the reviewer for the pointing out the issue involving the strict monotonicity. As suggested by the reviewer, we have included the strict monotonicity of the objective sequence in Appendix A-C following (52). Moreover, we have also noted the reader that we in fact use the regularized objective in (46b) to analytically prove the convergence of iterates, although we have numerically observed that the objective sequence of Algorithm 1 (with the original objective) always converge. Please refer to the paragraph before Section III-C.

\begin{comment}
Note that the objective sequence \eqn{\{f(\mbf{x}_k)\}} generated by an iterative \ac{SCA} algorithm is monotonic, \textit{i.e}, \eqn{f(\mbf{x}_k) \leq f(\mbf{x}_{k-1})} and holds with equality at the limit point of the iterative procedure. However, upon convergence, due to the convexity of the objective, we may have multiple limit points, say, \eqn{\mbf{y}_{\ast}} with the same objective as \eqn{f(\mbf{y}_{\ast}) = f(\mbf{x}_{\ast})}. Therefore, strict monotonicity cannot be guaranteed for the objective sequence. However, when the objective function is strongly convex with a parameter \eqn{m > 0}, as discussed in Appendix A-B, the minimizer for the subproblem in each \ac{SCA} iteration \eqn{k} is unique as
\begin{equation} \label{eqn-aa}
f(\ma_{k}) - f(\ma_{k+1}) \geq  \nabla f(\ma_{k+1})^\tran (\ma_{k} - \ma_{k+1}) + m \|\ma_{k} - \ma_{k+1}\|^2
\end{equation}
where \eqn{\ma_{k+1}} is the optimal solution for the \eqn{\ith{k}} \ac{SCA} subproblem and \eqn{\ma_{k}} is the unique minimizer obtained in the previous \eqn{\ith{k-1}} \ac{SCA} step, which is a feasible point for the current problem. Since \eqn{\ma_{k+1}} is the solution in the \eqn{\ith{k}} \ac{SCA} problem, following conditions hold
\begin{subeqnarray}
\nabla f(\ma_{k+1})^\tran (\ma_{k} - \ma_{k+1}) &\geq& 0 \slabel{eqn-a} \\
f(\ma_{k}) - f(\ma_{k+1}) &\geq& m \|\ma_{k} - \ma_{k+1}\|^2 \slabel{eqn-b}
\end{subeqnarray}
where \eqref{eqn-a} is due to the lack of any descent direction in the feasible set and \eqref{eqn-b} follows from \eqref{eqn-aa} using \eqref{eqn-a}. As \eqn{k \rightarrow \infty, \|\ma_{k+1} - \ma_{k}\| \rightarrow 0}, and therefore the iterative algorithm converges to a unique minimizer, say, \eqn{\ma_{\ast}}, which is the limit point of the algorithm. Now by using the monotonicity of the \ac{SCA} updates and the uniqueness of the minimizer in each step, we can guarantee the strict monotonicity of the objective sequence generated by the iterative problem. 
\end{comment}

\cmnt{5} \review{Note that the conclusions [32, Thm 2] and [26, Thm 10] have lots of assumptions. To invoke these reference, explicit exposition should be provided to show that these conclusions can be applied to our problem. The same questions occur to the proof in Appendix B, where conclusions in [11] [36] and [37] are used. Too many details are omitted to make the proof convincing and clear. }

\resp We thank the reviewer for raising the concern. We have updated the manuscript to include the details regarding the stationary point discussion in Appendix A-E and the convergence proof analysis for the primal and the \ac{ADMM} algorithms in Appendix B. We have also mentioned the conditions required for showing the stationarity property of the limit point and included more mathematical materials to make the convergence proof of distributed approaches rigorous.

\end{enumerate}
